[["index.html", "DataViz protocols An introduction to data visualization protocols for wet lab scientists Preface", " DataViz protocols An introduction to data visualization protocols for wet lab scientists Joachim Goedhart 2022-04-12 Preface Experiments rely on step-by-step instructions that are detailed in protocols. These protocols, which are used in a wet lab, are similar to the instructions that are defined in scripts for data visualization. Although scientists are familiar with protocols for experiments, they are usually less familiar with code or scripts for handling experimental data. Given the similarities between experimental methods and computer instructions, it should be within reach for experimental scientists to add automated, reproducible data processing and visualization to their toolkit. This book aims to lower the barrier for wet lab scientists to use R and ggplot2 for data visualization. First, by explaining some basic principles in data processing and visualization. Second, by providing example protocols, which can be applied to your own data, and I hope that the protocols serve as inspiration and a starting point for new and improved protocols. Data visualization is the process of transforming information into a picture. The picture that reflects the information, helps humans to understand and interpret the data. As such, data visualization is an important step in the analysis of experimental data and it is key for interpretation of results. Moreover, proper data visualization is important for the communication about experiments and research in presentations and publications. Data visualization usually requires refinement of the data, e.g. reshaping or processing. Therefore, the translation of data into a visualization is a multistep process. This process can be automated by defining the steps in a script for a software application. A script is a set of instructions in a language that both humans and computers can understand. Using a script can make data analysis and visualization faster, robust against errors and reproducible. As it becomes easier and cheaper to gather data, it becomes more important to use automated analyses. Finally, scripts make the processing transparent when the scripts are shared or published. R is a very popular programming language for all things related to data. It is freely available, open-source and there is a large community of active users. In addition, it fulfills a need for reproducible, automated data analysis. And lastly, with the ggplot2 extension, it is possible to generate state-of-the-art data visualizations. There are many great resources out there (that is also the reason I came this far) and below I list the specifics of this resource. First, there is a strict focus on R. All examples use R for all steps. Second, the datasets that are used are realistic and represent data that you may have. Several datasets that are used come from actual experimental data gathered in a wet lab. By using real data, specific issues that may not be treated elsewhere are encountered, discussed and solved. One of the reasons is that R requires a specific data format (detailed in chapter 2 Reading and Reshaping data) before the data can be visualized. It is key to understand how experimental data should be processed and prepared in a way that it can be analyzed and visualized. As the required format is usually unfamiliar to wet lab scientists, I provide several examples of how to do this. Third, since details determine successful use of R, I will go into detail whenever necessary. Examples of details include the use of spaces in column names, reading files with missing values, or optimizing the position of a label in a data visualization. Finally, modern analysis and visualization methods are treated and since the book is in a digital, online format it will be adjusted when new methods are introduced. An example of a recently introduced data visualization is the Superplot, which is the result of Protocol 2. Part of this work has been published as blogs on The Node and the enthusiastic response encouraged me to create a more structured and complete resource. This does not at all imply that this document needs to be read in a structured manner. If you are totally new to R it makes sense to first read the chapter Getting started with R which treats some of the essential basics. On the other hand, if you are familiar with R, you may be interested in the chapters on Reading and Reshaping data or Visualizing data. Finally, masters in R/ggplot2 may jump right to the Complete protocols. This final part brings all the ingredients of the preceding chapters together. Each protocol starts with raw data and shows all the steps that lead to a publication quality plot. I hope that you’ll find this document useful and that it may provide a solid foundation for anyone that wants to use R for the analysis and visualization of scientific data that comes from a wetlab. I look forward to seeing the results on twitter (feel free to mention me: @joachimgoedhart), in meetings, in preprints or in peer reviewed publications. A toast Cheers to all the kind people that helped me to get started with R, answered my questions, provided feedback on code and data visualizations, and helped me to troubleshoot scripts. Also thanks to all co-workers for sharing data and the helpful discussions. Finally, twitter is a huge source of inspiration, a magnificent playground, and an ideal place to meet people, discuss, get feedback or just hang out and I thank anyone I interact(ed) with! "],["getting-started.html", "Chapter 1 Getting started with R 1.1 Running R 1.2 Using the command line 1.3 ?Help? 1.4 Installing packages 1.5 Multiline code", " Chapter 1 Getting started with R There is a lot of great material out there to get you started with R. I enjoyed swirl, which teaches basic R, in R. Whatever you choose, it is probably a good idea to familiarize yourself to some extent with basic R. Below, I only treat some of the basic stuff that is needed to run the code that is presented in the other chapters. 1.1 Running R R is free and it is available for many different platforms. You can run plain R, run it from RStudio Desktop or even from a browser. I prefer Rstudio and the instructions are based on Rstudio as well. When you start Rstudio, you see a couple of windows, including a ‘Console’. This is the place where you can type your commands for R. 1.2 Using the command line A classic in coding language is to print ‘hello world’. To do this, you need to type this piece of code (followed by the enter key) in the console: print('hello world') The result, printed below the code in the console is: [1] \"hello world\" In this document the input and output is shown as grey boxes. The first grey box represents the input and, if there is any output, the second grey box shows the output, for example: print(&#39;hello world&#39;) [1] &quot;hello world&quot; It is possible to copy the code from the first box. When you move your cursor to the upper left corner of the box a copy icon will appear. If you click on the icon, the code is copied. Not every piece of code results in a visible output, for instance when I assign a value to the variable x: x &lt;- 1 To show the value of a variable as output, type its name: x [1] 1 R comes with datasets. Although these datasets are not so relevant for us, they are often used to demonstrate functions in R. One of these datasets is mtcars and we can use the head() function to check the first lines: head(mtcars) mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 1.3 ?Help? If you need help with a specific function, you may look at the documentation by typing a question mark followed by the function: ?head() However, I usually find myself googling instead of reading the documentation. The result is often a website where the correct use is explained by an example. It also make sense to experiment. Systematically changing variables or options (or outcommenting lines of code, see below) will teach you the logic behind a function or operation. Finally, there may be some answers in the chapter Questions and Answers 1.4 Installing packages R comes with a lot of commands and functions, but we often need to load additional ‘packages’ to add functionality. If the package is not yet available, it needs to be downloaded and installed. This handy piece of code checks whether the {tidyverse} package is available, downloads it if necessary and than activates it: if (!require(tidyverse)) { install.packages(&quot;tidyverse&quot;) require(tidyverse) } The output in the console depends on the packages that are installed and activated. The tidyverse package is very versatile and is a superpackage that hold several packages that equip R with superpowers. The most important one is the ggplot2 package that we use for the data visulizations. Whenever necessary, other packages are required and this will be mentioned when necessary. 1.5 Multiline code The tidyverse package introduces a so-called pipe operator %&gt;% which we will use a lot. This operator is useful for stringing multiple functions together. An example is given below, which reads as ‘take the mtcars dataset and next use the head() function’. mtcars %&gt;% head() mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Also for the plots that are generated with ggplot(), several functions can be added and these reflects different layers in the plot. However, in case of ggplot the layers are combined by a +: ggplot(mtcars, aes(wt, mpg)) + geom_point() + geom_line() It is important to end each line with a +, as it indicates that the code continues on the next line. This will result in a warning message: ggplot(mtcars, aes(wt, mpg)) + geom_point() + geom_line() The last line should not end with a plus. A trick that I use a lot, is using NULL on the last line of the code: ggplot(mtcars, aes(wt, mpg)) + geom_point() + geom_line() + NULL The advantage is that it is easy to deactivate a line by placing a hashtag in front of it (without the need to remove the +). The hashtag tells R to treat that line as a comment and not as code: ggplot(mtcars, aes(wt, mpg)) + geom_point() + # geom_line() + NULL This strategy is called ‘commenting out’ and is very useful to examine the effect of a line of code. For plots, it works very well in combination with NULL on the last line. "],["read-and-reshape.html", "Chapter 2 Reading and reshaping data 2.1 Introduction 2.2 Types of data 2.3 Reading data 2.4 Reshaping data", " Chapter 2 Reading and reshaping data 2.1 Introduction Experimental data can be recorded and stored in different ways. Anything that is not digital (e.g. notes in an physical labbook) has to be converted, before it can be used in a computer. Nowadays, most information is already in a digital format and stored in a file. This can be a text file, an excel file, or a file generated by a piece of equipment. In R, the main structure for storing and processing data is a ‘dataframe’ (a modernized version of the dataframe is a ‘tibble’, which is available when the tidyverse package is used). The dataset mtcars that comes with R is a dataframe. We can check the type of a structure or variable by using the function class(). This is a very helpful function to learn about the class of an object: class(mtcars) [1] &quot;data.frame&quot; The dataframe itself can be printed by just typing its name. To look only at the first rows we can use the function head(): head(mtcars) mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 When we read experimental data from a file, this implies that we transfer the information to a dataframe in R. A dataframe can hold several types of data. Let’s first look at what different types of data you may encounter. 2.2 Types of data All the information that is recorded during an experiment can be considered as data. Details of the experimental approach or setup are called metadata, and the data that is measured is the raw data. Data visualization primarily deals with the raw, measured data. Yet, adding data on the experimental design or other types of metadata can be important for the interpretation of the data. The raw data mostly consist of numbers, but these are often accompanied by labels for experimental conditions or to identify objects. All this information is data, but they are clearly of different types. Let’s have a look at the most common types of data. Measurements usually result in quantitative data consisting of numbers. For instance when an optical density is measured with a spectrophotometer or when temperature is determined with a thermometer. This kind of data is called quantitative and continuous data, since it can have any number ranging from minus to plus infinity. Another type of quantitive data is quantitative and discrete data and it consists only of natural numbers. An example is the number of colonies on an agar plate or the number of replicates. There is also data that cannot be expressed in numbers and we call that qualitative data. For instance different experimental categories, e.g. ‘control’ and ‘treated’. This is also known as nominal data (ordinal data is not treated as it is not used in this book). It is important to make the distinction between different types of data and tell R how to treat the data. In some cases a number stored in a file can be a category and it is important to treat the number as a category and not as quantitative data. We will see that R stores information about the type of data in the example below. The variable x to which we assigned a value of 1 is: x &lt;- 1 class(x) [1] &quot;numeric&quot; We can convert this to a ‘factor’, which means that it is no longer a number but qualitative data: x &lt;- as.factor(x) class(x) [1] &quot;factor&quot; And therefore, this will give an error: x + 1 [1] NA When we convert it back to a number, it works: as.numeric(x) + 1 [1] 2 This simple example illustrates the difference between quantitative and qualitative data and it shows that we can change the data type in R. 2.3 Reading data Reading, or loading data is the transfer of information from a file to the memory of R where it is stored as a dataframe. Rstudio supports ‘point-and-click’ loading of data from its menu (File &gt; Import Dataset &gt; …). This is a convenient way of loading data. Since the aim is to perform all steps in a script, including data loading, I’ll explain how to functions are used to load data from the console. 2.3.1 Loading data from a text or csv file Before we can read the file, we need to make sure that we can locate the file. In RStudio you can select the directory (folder) from the menu: Session &gt; Set Working Directory &gt; Choose Directory… If you are running an R script and the data is in the same directory as the script, you can go to the menu of Rstudio and select: Session &gt; Set Working Directory &gt; To Source File Location When the directory is properly set, you can read the file. A common file format is the ‘comma separated values’ or CSV format. Here we load a CSV file that was obtained from fpbase.org and contains the excitation and emission data of Green Fluorescent Protein. This file and all other example data are available at github: https://github.com/JoachimGoedhart/DataViz-protocols The function read_csv() is used to read the file and the data is assigned to a dataframe called ‘df’: df &lt;- read.csv(&#39;FPbase_Spectra.csv&#39;) To check whether the loading was successful, we can look at the first lines of the dataframe with the function head(): head(df) Wavelength mEGFP.EM mEGFP.EX 1 300 NA 0.0962 2 301 NA 0.0872 3 302 NA 0.0801 4 303 NA 0.0739 5 304 NA 0.0675 6 305 NA 0.0612 This data has several columns, each containing quantitative data. Empty cells that do not have any data will be displayed as ‘NA’. Note that this is different from ‘0’. 2.3.2 Loading data from a URL When the CSV file is available online, e.g. in a data repository or on Github, it can be loaded by providng the URL: df &lt;- read.csv(&#39;https://zenodo.org/record/2545922/files/FRET-efficiency_mTq2.csv&#39;) head(df) EGFP mNeonG Clover mKOkappa mOrange2 mScarlet.I mRuby2 TagRFP.T 1 46.25865 61.27913 52.85122 47.28077 25.70513 36.03078 51.68192 12.39361 2 46.48604 60.97392 54.05560 48.42123 26.43752 34.93961 33.75286 12.59682 3 46.66348 61.27877 48.93475 44.93190 24.47023 25.38569 41.93491 14.03628 4 46.30399 60.80149 48.95284 45.47728 21.72633 28.27172 33.68123 12.59829 5 45.67780 59.89690 52.97067 44.03166 21.52321 34.00304 35.81131 20.86654 6 45.35124 62.76465 52.57957 47.08019 23.44953 34.64273 48.87000 23.78392 mCherry 1 31.31533 2 30.85531 3 30.66732 4 34.28993 5 35.64215 6 28.99762 2.3.3 Retrieving data from Excel Suppose we have an excel file with multiple tabs and we would like to access the data for mNeonGreen. To import the correct data into a dataframe use: df &lt;- readxl::read_excel(&#39;FPbase_Spectra.xlsx&#39;, sheet = &#39;mNeonGreen&#39;) head(df) # A tibble: 6 x 3 Wavelength `mNeonGreen EM` `mNeonGreen EX` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 300 NA NA 2 301 NA NA 3 302 NA NA 4 303 NA NA 5 304 NA NA 6 305 NA NA Again, ‘NA’ indicates that no data is available. Since the read_excel() is a funtion from the ‘tidyverse’ the data is stored in a tibble. This can be converted to an ordinary dataframe: df &lt;- as.data.frame(df) class(df) [1] &quot;data.frame&quot; Now that we have the data loaded, we can generate a plot. For simplicity I use the qplot() function. The more flexible ggplot() function will be introduced later. We need to supply the name of the dataframe, the column for the x-axis data and the column that is used for the y-axis. Note that two of the column names have a space and to properly indicate the name of the column we need to enclose the name with backtics (`). qplot(data=df,x=Wavelength, y=`mNeonGreen EX`) A plot with lines instead of dots can be made by supplying this alternative ‘geometry’: qplot(data=df,x=Wavelength, y=`mNeonGreen EX`, geom=&#39;line&#39;) The plots show the excitation spectrum of mNeonGreen, and it can be inferred that the protein is maximally excited near 500 nm. In its current shape, the dataframe is not suitable for simultaneously plotting the excitation and emission spectrum. To do that, we need to reshape the data and this will be the topic of the Reshaping data section. 2.3.4 Retrieving data from multiple files When the data is spread over multiple files, it is useful to read these files and combine them into one dataframe. In this example we have the data from three different conditions, each is an individual CSV file. First we create a list with the files based on a pattern. In this case the relevant files contain the string S1P.csv: filelist = list.files(pattern=&quot;*S1P.csv&quot;) filelist [1] &quot;Cdc42_S1P.csv&quot; &quot;Rac_S1P.csv&quot; &quot;Rho_S1P.csv&quot; Then we use the function map() to perform the function read.csv() for each of the files and we store the result in a new dataframe ‘df_input_list’. df_input_list &lt;- map(filelist, read.csv) The result is a ‘nested’ dataframe, which is a dataframe with dataframes. Instead of having three separate dataframes, we want a single dataframe but it should have a label that reflects the condition. The labels are based on the filenames. Note that we use gsub here to remove the extension of the filename: names(df_input_list) &lt;- gsub(filelist, pattern=&quot;\\\\..*&quot;, replacement=&quot;&quot;) After this, we merge the dataframes and create a column ‘id’ that has the label with the filename: df &lt;- bind_rows(df_input_list, .id = &quot;id&quot;) head(df) id Time Cell.1 Cell.2 Cell.3 Cell.4 Cell.5 1 Cdc42_S1P 0.0000000 1.0035170 1.0015490 0.9810209 0.9869040 1.0041990 2 Cdc42_S1P 0.1666667 0.9991689 0.9961631 0.9801265 0.9891608 0.9989283 3 Cdc42_S1P 0.3333333 1.0013450 1.0046440 1.0106020 1.0124910 0.9953477 4 Cdc42_S1P 0.5000000 1.0015790 1.0043180 1.0106890 1.0066810 1.0027020 5 Cdc42_S1P 0.6666667 0.9943522 0.9933341 1.0168220 1.0045540 0.9988462 6 Cdc42_S1P 0.8333333 1.0046240 1.0071780 1.0140210 1.0042580 0.9920673 Cell.6 Cell.7 Cell.8 Cell.9 Cell.10 Cell.11 Cell.12 1 1.0049370 1.0042000 1.0014210 1.0001080 1.000748 1.0015060 1.0057850 2 0.9892894 0.9903881 0.9975485 0.9937496 0.995352 0.9962637 0.9972004 3 1.0104820 1.0140110 1.0016740 1.0003500 1.003132 1.0015520 0.9942397 4 1.0031760 1.0016000 1.0000720 1.0035080 1.000595 1.0017760 1.0035220 5 0.9921757 0.9898673 0.9993021 1.0023010 1.000192 0.9989264 0.9993660 6 1.0081460 1.0075700 0.9985681 1.0013800 1.001741 0.9989417 1.0028090 Cell.13 Cell.14 Cell.15 Cell.16 Cell.17 Cell.18 Cell.19 Cell.20 1 1.0111880 1.0147230 1.0089390 1.0032410 1.0068720 1.0019290 1.0008660 NA 2 0.9894263 1.0044960 1.0026040 0.9978889 0.9970717 0.9993551 0.9980562 NA 3 0.9921575 1.0073610 1.0029890 1.0009640 0.9984493 0.9968905 0.9966830 NA 4 1.0102150 0.9910634 0.9943020 0.9942904 0.9932356 1.0059050 1.0043110 NA 5 0.9974161 0.9829283 0.9912844 1.0037130 1.0045560 0.9959676 1.0000940 NA 6 1.0070350 0.9790479 0.9861535 1.0059600 1.0065300 1.0070570 1.0049470 NA Cell.21 Cell.22 Cell.23 Cell.24 Cell.25 Cell.26 Cell.27 Cell.28 Cell.29 1 NA NA NA NA NA NA NA NA NA 2 NA NA NA NA NA NA NA NA NA 3 NA NA NA NA NA NA NA NA NA 4 NA NA NA NA NA NA NA NA NA 5 NA NA NA NA NA NA NA NA NA 6 NA NA NA NA NA NA NA NA NA Cell.30 Cell.31 Cell.32 1 NA NA NA 2 NA NA NA 3 NA NA NA 4 NA NA NA 5 NA NA NA 6 NA NA NA This dataframe contains all the relevant information, but it is not tidy yet. We’ll discuss how to convert this dataframe into a tidy format. We can save this dataframe for later: df %&gt;% write.csv(&#39;df_S1P_combined.csv&#39;, row.names=FALSE) 2.4 Reshaping data Data is often recorded in tables or spreadsheets. Columns are typically used for different conditions (indicated in a header) and each data cell contains a measured value. Although this format makes perfect sense for humans, it is less suitable for analysis and visualization in R. Instead of the tabular, or wide, format, the functions from the tidyverse package work with data in a ‘tidy’ format. The benefit of tidy data is that it is a consistent way to structure datasets, facilitating data manipulation and visualization Wickham, 2014. In other words, this format simplifies downstream processing and visualization. In this section, I will show how data can be converted from spreadsheet format to a long, tidy format. This step is needed to prepare the data for visualization with ggplot() which is also part of the tidyverse package. I will use the nomenclature that is used in the original publication by Hadley Wickham. Before we start, a quick warning that I have been struggling with the concept of tidy data. Probably, because I was very much used to collect, process and summarize data in spreadsheets. In addition, I am used to read and present data in a tabular format. It is important to stress that data in the tidy format contains exactly the same information as non-tidy, spreadsheet data, but it is structured in a different way. In fact we can switch between the two formats with functions that are provided by R. 2.4.1 Quantitative data, discrete conditions Let’s say that you have measured cell sizes under a number of different experimental conditions and this is stored in an excel spreadsheet. Let’s load the data: df &lt;- readxl::read_excel(&#39;Length-wide.xls&#39;) df # A tibble: 10 x 5 `Condition 1` `Condition 2` `Condition 3` `Condition 4` `Condition 5` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 14.4 19.2 16.6 21.9 18.7 2 16.2 13.7 13.9 20.2 13.2 3 11.2 13.2 13.2 22.7 18.1 4 15.2 20.1 13.7 19 18.2 5 11.7 21.3 15.9 19.5 15.9 6 17.5 19.9 13 14.3 28.7 7 17 20.8 10.8 14 29.6 8 18.7 18.4 14.9 17.5 13.3 9 19.1 20.7 14.6 16.5 13.9 10 10.7 19.2 14.2 17.8 13.4 First, we will replace the spaces in the column names. Although this is not strictly necessary, it simplifies handling of the data. We use names() to get the names of the dataframe and gsub() to replace characters in a vector. The \"\\\\s\" is a symbol for a space and the second argument of the function \"\"_\" indicates the new character or string: names(df) &lt;- gsub(&quot;\\\\s&quot;, &quot;_&quot;, names(df)) head(df) # A tibble: 6 x 5 Condition_1 Condition_2 Condition_3 Condition_4 Condition_5 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 14.4 19.2 16.6 21.9 18.7 2 16.2 13.7 13.9 20.2 13.2 3 11.2 13.2 13.2 22.7 18.1 4 15.2 20.1 13.7 19 18.2 5 11.7 21.3 15.9 19.5 15.9 6 17.5 19.9 13 14.3 28.7 Now that the column names are fixed, we can restructure the data. The aim is to create a dataframe with one column that specifies the conditions and one column with all the measured values. There is a long history of packages and functions that can be used for restructuring (gather(), melt()). Here we use the most modern function pivot_longer() from the tidyverse package. We need to specify the dataframe, which columns to restructure (specified by everything() here) and the names of the new columns. The current column names will be transferred to a column that is named “condition” and all values will be transferred to a column named “size”. df_tidy &lt;- pivot_longer(df, cols = everything(), names_to = &quot;Condition&quot;, values_to = &quot;Size&quot;) head(df_tidy) # A tibble: 6 x 2 Condition Size &lt;chr&gt; &lt;dbl&gt; 1 Condition_1 14.4 2 Condition_2 19.2 3 Condition_3 16.6 4 Condition_4 21.9 5 Condition_5 18.7 6 Condition_1 16.2 The benefit of this format is that is is now clear what the numbers are. The most important requirement for tidy data is that each variable occupies only a single column and that each row is an observation. Let’s save the data in a csv file: df_tidy %&gt;% write.csv(&#39;Length-tidy.csv&#39;) 2.4.2 Multiple discrete conditions Here, we deal with a more complex spreadsheet that holds data of multiple replicates and two experimental conditions. Note that the data from multiple conditions can be stored in different ways and here we only treat one way. Especially for these kind of datasets, the tidy format is a better, cleaner structure. We will load the data from a repository: df_multiheader &lt;- read.csv(&#39;https://zenodo.org/record/4056966/files/Data-with-replicates.csv&#39;) head(df_multiheader) Control Control.1 Control.2 Drug Drug.1 Drug.2 1 Replicate1 Replicate2 Replicate3 Replicate1 Replicate2 Replicate3 2 43.69202 35.43517 27.69333 35.3156 20.54166 16.13286 3 41.85664 38.17644 35.61621 34.81943 20.43263 16.47575 4 49.11707 39.86308 27.20247 30.45615 29.8097 16.49928 5 49.79331 37.5157 39.98903 37.46084 25.94712 18.48844 6 41.54301 42.66665 26.92205 30.25243 22.90337 21.93457 The first row lists the experimental condition and the second row identifies biological replicates. Now, when this is loaded as an ordinary CSV, the first row is the header, but the second row is treated as data. Let’s load the data without flagging a header. We also add the stringsAsFactors = FALSE to make sure that the data is loaded as characters &lt;chr&gt; and not as factors &lt;fctr&gt;. The difference is not obvious, but we run into problems later when we want to convert all the values into actual numbers (which we will do this at the very last step): df_multiheader &lt;- read.csv(&quot;https://zenodo.org/record/4056966/files/Data-with-replicates.csv&quot;, header = FALSE, stringsAsFactors = FALSE) head(df_multiheader) V1 V2 V3 V4 V5 V6 1 Control Control Control Drug Drug Drug 2 Replicate1 Replicate2 Replicate3 Replicate1 Replicate2 Replicate3 3 43.69202 35.43517 27.69333 35.3156 20.54166 16.13286 4 41.85664 38.17644 35.61621 34.81943 20.43263 16.47575 5 49.11707 39.86308 27.20247 30.45615 29.8097 16.49928 6 49.79331 37.5157 39.98903 37.46084 25.94712 18.48844 We’ll load the first row as a vector that contains the name of each column. To this end we select the first row of the dataframe with the brackets [1,]. The result is a dataframe and to turn this into a vector with strings we use unlist(): first_row &lt;- df_multiheader[1,] %&gt;% unlist(use.names=FALSE) first_row [1] &quot;Control&quot; &quot;Control&quot; &quot;Control&quot; &quot;Drug&quot; &quot;Drug&quot; &quot;Drug&quot; We repeat this for the second row: second_row &lt;- df_multiheader[2,] %&gt;% unlist(use.names=FALSE) second_row [1] &quot;Replicate1&quot; &quot;Replicate2&quot; &quot;Replicate3&quot; &quot;Replicate1&quot; &quot;Replicate2&quot; [6] &quot;Replicate3&quot; Next, row 1 and row 2 are removed from the dataframe, keeping only the data: df &lt;- df_multiheader[-c(1:2),] head(df) V1 V2 V3 V4 V5 V6 3 43.69202 35.43517 27.69333 35.3156 20.54166 16.13286 4 41.85664 38.17644 35.61621 34.81943 20.43263 16.47575 5 49.11707 39.86308 27.20247 30.45615 29.8097 16.49928 6 49.79331 37.5157 39.98903 37.46084 25.94712 18.48844 7 41.54301 42.66665 26.92205 30.25243 22.90337 21.93457 8 44.04201 37.10115 18.24681 35.93469 20.10045 22.86391 The labels of the conditions and replicates are combined by pasting them together with an underscore to separate the labes. The result is a single vector with unique labels: combined_labels &lt;- paste(first_row, second_row, sep=&quot;_&quot;) combined_labels [1] &quot;Control_Replicate1&quot; &quot;Control_Replicate2&quot; &quot;Control_Replicate3&quot; [4] &quot;Drug_Replicate1&quot; &quot;Drug_Replicate2&quot; &quot;Drug_Replicate3&quot; Now, we can add these labels as column names to the dataframe: colnames(df) &lt;- combined_labels head(df) Control_Replicate1 Control_Replicate2 Control_Replicate3 Drug_Replicate1 3 43.69202 35.43517 27.69333 35.3156 4 41.85664 38.17644 35.61621 34.81943 5 49.11707 39.86308 27.20247 30.45615 6 49.79331 37.5157 39.98903 37.46084 7 41.54301 42.66665 26.92205 30.25243 8 44.04201 37.10115 18.24681 35.93469 Drug_Replicate2 Drug_Replicate3 3 20.54166 16.13286 4 20.43263 16.47575 5 29.8097 16.49928 6 25.94712 18.48844 7 22.90337 21.93457 8 20.10045 22.86391 To convert this dataframe into a tidy format we use the pivot_longer() function, exactly like we did in the previous example: df_tidy &lt;- pivot_longer(df, cols = everything(), names_to = &quot;combined_labels&quot;, values_to = &quot;Size&quot;) head(df_tidy) # A tibble: 6 x 2 combined_labels Size &lt;chr&gt; &lt;chr&gt; 1 Control_Replicate1 43.69202 2 Control_Replicate2 35.43517 3 Control_Replicate3 27.69333 4 Drug_Replicate1 35.3156 5 Drug_Replicate2 20.54166 6 Drug_Replicate3 16.13286 The dataframe is tidy now, but we need to split the conditions from the replicates in the first column with combined labels: df_tidy &lt;- df_tidy %&gt;% separate(combined_labels, c(&#39;Treatment&#39;, &#39;Replicate&#39;)) head(df_tidy) # A tibble: 6 x 3 Treatment Replicate Size &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 Control Replicate1 43.69202 2 Control Replicate2 35.43517 3 Control Replicate3 27.69333 4 Drug Replicate1 35.3156 5 Drug Replicate2 20.54166 6 Drug Replicate3 16.13286 There is still one problem that we need to fix. The values in de the column ‘Size’ are characters &lt;chr&gt;, which means these are strings. Let’s convert the strings to actual numbers: df_tidy %&gt;% mutate(Size = as.numeric(Size)) # A tibble: 300 x 3 Treatment Replicate Size &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 Control Replicate1 43.7 2 Control Replicate2 35.4 3 Control Replicate3 27.7 4 Drug Replicate1 35.3 5 Drug Replicate2 20.5 6 Drug Replicate3 16.1 7 Control Replicate1 41.9 8 Control Replicate2 38.2 9 Control Replicate3 35.6 10 Drug Replicate1 34.8 # … with 290 more rows This kind of data, acquired at different conditions with different replicas is ideally suited for a SuperPlot (Lord et al., 2000). An example of this kind of data visualization is Protocol 2 2.4.3 Double quantitative data An example of quantitative continuous data for two variables is when measurements are performed at different concentrations, times or wavelengths. The latter example we have encountered before when spectral data was loaded. Let’s look again at that data and convert it to tidy format. df &lt;- readxl::read_excel(&#39;FPbase_Spectra.xlsx&#39;, sheet = &#39;mNeonGreen&#39;) head(df) # A tibble: 6 x 3 Wavelength `mNeonGreen EM` `mNeonGreen EX` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 300 NA NA 2 301 NA NA 3 302 NA NA 4 303 NA NA 5 304 NA NA 6 305 NA NA Both the Emission (EM) and excitation (EX) data are acquired as a function of wavelength. To convert this data to tidy format, we need to keep a column with wavelength data and we need another column with the spectral data. To achieve this, we will modify the mNeonGreen data and keep the Wavelength data as a column: df_tidy &lt;- pivot_longer(df, cols = -Wavelength, names_to = &quot;sample&quot;, values_to = &quot;intensity&quot;) head(df_tidy) # A tibble: 6 x 3 Wavelength sample intensity &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 300 mNeonGreen EM NA 2 300 mNeonGreen EX NA 3 301 mNeonGreen EM NA 4 301 mNeonGreen EX NA 5 302 mNeonGreen EM NA 6 302 mNeonGreen EX NA We can now plot the two spectra, which was not possible before the conversion: qplot(data=df_tidy,x=Wavelength, y=intensity, color=sample, geom=&#39;line&#39;) A more elaborate example of plotting spectra is given in Protocol 1 2.4.4 Data from multiple files In an earlier example, we have combined the data from multiple files into a single dataframe. Let’s first load it: df &lt;- read.csv(&#39;df_S1P_combined.csv&#39;) head(df) id Time Cell.1 Cell.2 Cell.3 Cell.4 Cell.5 1 Cdc42_S1P 0.0000000 1.0035170 1.0015490 0.9810209 0.9869040 1.0041990 2 Cdc42_S1P 0.1666667 0.9991689 0.9961631 0.9801265 0.9891608 0.9989283 3 Cdc42_S1P 0.3333333 1.0013450 1.0046440 1.0106020 1.0124910 0.9953477 4 Cdc42_S1P 0.5000000 1.0015790 1.0043180 1.0106890 1.0066810 1.0027020 5 Cdc42_S1P 0.6666667 0.9943522 0.9933341 1.0168220 1.0045540 0.9988462 6 Cdc42_S1P 0.8333333 1.0046240 1.0071780 1.0140210 1.0042580 0.9920673 Cell.6 Cell.7 Cell.8 Cell.9 Cell.10 Cell.11 Cell.12 1 1.0049370 1.0042000 1.0014210 1.0001080 1.000748 1.0015060 1.0057850 2 0.9892894 0.9903881 0.9975485 0.9937496 0.995352 0.9962637 0.9972004 3 1.0104820 1.0140110 1.0016740 1.0003500 1.003132 1.0015520 0.9942397 4 1.0031760 1.0016000 1.0000720 1.0035080 1.000595 1.0017760 1.0035220 5 0.9921757 0.9898673 0.9993021 1.0023010 1.000192 0.9989264 0.9993660 6 1.0081460 1.0075700 0.9985681 1.0013800 1.001741 0.9989417 1.0028090 Cell.13 Cell.14 Cell.15 Cell.16 Cell.17 Cell.18 Cell.19 Cell.20 1 1.0111880 1.0147230 1.0089390 1.0032410 1.0068720 1.0019290 1.0008660 NA 2 0.9894263 1.0044960 1.0026040 0.9978889 0.9970717 0.9993551 0.9980562 NA 3 0.9921575 1.0073610 1.0029890 1.0009640 0.9984493 0.9968905 0.9966830 NA 4 1.0102150 0.9910634 0.9943020 0.9942904 0.9932356 1.0059050 1.0043110 NA 5 0.9974161 0.9829283 0.9912844 1.0037130 1.0045560 0.9959676 1.0000940 NA 6 1.0070350 0.9790479 0.9861535 1.0059600 1.0065300 1.0070570 1.0049470 NA Cell.21 Cell.22 Cell.23 Cell.24 Cell.25 Cell.26 Cell.27 Cell.28 Cell.29 1 NA NA NA NA NA NA NA NA NA 2 NA NA NA NA NA NA NA NA NA 3 NA NA NA NA NA NA NA NA NA 4 NA NA NA NA NA NA NA NA NA 5 NA NA NA NA NA NA NA NA NA 6 NA NA NA NA NA NA NA NA NA Cell.30 Cell.31 Cell.32 1 NA NA NA 2 NA NA NA 3 NA NA NA 4 NA NA NA 5 NA NA NA 6 NA NA NA The data is still in a wide format and all the data that is in a column that starts with ‘Cell’ needs to be combined into a single column. The other columns id and Time need to be excluded from this operation and this is achieved with using the - sign: df_tidy &lt;- pivot_longer(df, cols = -c(id, Time), names_to = &quot;object&quot;, values_to = &quot;activity&quot;) head(df_tidy) # A tibble: 6 x 4 id Time object activity &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 Cdc42_S1P 0 Cell.1 1.00 2 Cdc42_S1P 0 Cell.2 1.00 3 Cdc42_S1P 0 Cell.3 0.981 4 Cdc42_S1P 0 Cell.4 0.987 5 Cdc42_S1P 0 Cell.5 1.00 6 Cdc42_S1P 0 Cell.6 1.00 If desired, the column ‘id’ can be renamed and/or split: df_tidy &lt;- df_tidy %&gt;% separate(id, c(&#39;Condition&#39;, &#39;Treatment&#39;)) head(df_tidy) # A tibble: 6 x 5 Condition Treatment Time object activity &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 Cdc42 S1P 0 Cell.1 1.00 2 Cdc42 S1P 0 Cell.2 1.00 3 Cdc42 S1P 0 Cell.3 0.981 4 Cdc42 S1P 0 Cell.4 0.987 5 Cdc42 S1P 0 Cell.5 1.00 6 Cdc42 S1P 0 Cell.6 1.00 This dataframe is perfectly tidy, but there’s one little improvement that improves the sorting of the objects. The number of cells runs from 1 to 32. When it is sorted, Cell.1 will be followed by Cell.10: sort(unique(df_tidy$object)) [1] &quot;Cell.1&quot; &quot;Cell.10&quot; &quot;Cell.11&quot; &quot;Cell.12&quot; &quot;Cell.13&quot; &quot;Cell.14&quot; &quot;Cell.15&quot; [8] &quot;Cell.16&quot; &quot;Cell.17&quot; &quot;Cell.18&quot; &quot;Cell.19&quot; &quot;Cell.2&quot; &quot;Cell.20&quot; &quot;Cell.21&quot; [15] &quot;Cell.22&quot; &quot;Cell.23&quot; &quot;Cell.24&quot; &quot;Cell.25&quot; &quot;Cell.26&quot; &quot;Cell.27&quot; &quot;Cell.28&quot; [22] &quot;Cell.29&quot; &quot;Cell.3&quot; &quot;Cell.30&quot; &quot;Cell.31&quot; &quot;Cell.32&quot; &quot;Cell.4&quot; &quot;Cell.5&quot; [29] &quot;Cell.6&quot; &quot;Cell.7&quot; &quot;Cell.8&quot; &quot;Cell.9&quot; To correct this, we need a 0 preceding the single digit numbers, e.g. Cell.01. To do that, we first split the object column into two columns, using the dot as a separator: df_tidy &lt;- df_tidy %&gt;% separate(&quot;object&quot;, c(&quot;object&quot;, &quot;number&quot;), sep=&quot;\\\\.&quot;) Then, we fill up all the number to two digits by adding a 0 in front of all the single digit numbers with the function str_pad(): df_tidy &lt;- df_tidy %&gt;% mutate(number=str_pad(number, 2, pad = &quot;0&quot;)) We can merge the two columns back together and I changed the seperator to a space: df_tidy &lt;- df_tidy %&gt;% unite(&quot;object&quot;, c(&quot;object&quot;, &quot;number&quot;), sep=&quot; &quot;) head(df_tidy) # A tibble: 6 x 5 Condition Treatment Time object activity &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 Cdc42 S1P 0 Cell 01 1.00 2 Cdc42 S1P 0 Cell 02 1.00 3 Cdc42 S1P 0 Cell 03 0.981 4 Cdc42 S1P 0 Cell 04 0.987 5 Cdc42 S1P 0 Cell 05 1.00 6 Cdc42 S1P 0 Cell 06 1.00 When the sorting is repeated it looks better: sort(unique(df_tidy$object)) [1] &quot;Cell 01&quot; &quot;Cell 02&quot; &quot;Cell 03&quot; &quot;Cell 04&quot; &quot;Cell 05&quot; &quot;Cell 06&quot; &quot;Cell 07&quot; [8] &quot;Cell 08&quot; &quot;Cell 09&quot; &quot;Cell 10&quot; &quot;Cell 11&quot; &quot;Cell 12&quot; &quot;Cell 13&quot; &quot;Cell 14&quot; [15] &quot;Cell 15&quot; &quot;Cell 16&quot; &quot;Cell 17&quot; &quot;Cell 18&quot; &quot;Cell 19&quot; &quot;Cell 20&quot; &quot;Cell 21&quot; [22] &quot;Cell 22&quot; &quot;Cell 23&quot; &quot;Cell 24&quot; &quot;Cell 25&quot; &quot;Cell 26&quot; &quot;Cell 27&quot; &quot;Cell 28&quot; [29] &quot;Cell 29&quot; &quot;Cell 30&quot; &quot;Cell 31&quot; &quot;Cell 32&quot; We will save this tidy dataframe for later use: df_tidy %&gt;% write.csv(&quot;df_S1P_combined_tidy.csv&quot;, row.names = FALSE) 2.4.5 Data in 96-wells format {data-in-96-wells-format} Data measured on samples in multiwell plates are often stored in a structure that resembles the plate layout. As an example, we use here a dataset from a luciferase experiment, measured with a 96-well plate luminescence reader. The data is stored in a sheet named ‘Results’ in an xlsx file. The cells in which the data is stored are located in F21:Q28. By selecting these cells, only the 96 values that were measured are read: df &lt;- readxl::read_excel(&#39;DualLuc_example_data.xlsx&#39;, sheet = &#39;Results&#39;, range = &quot;F21:Q28&quot;, col_names = F) New names: * `` -&gt; ...1 * `` -&gt; ...2 * `` -&gt; ...3 * `` -&gt; ...4 * `` -&gt; ...5 * ... The table-like layout of the data can be changed into a list of 96 values: data_as_list &lt;- df %&gt;% unlist(use.names = F) It is essential to know how the table is converted and this is done by reading the data from the first column, top to bottom, than the second column, etc. Knowing this, we can define the wells to which the data belongs, which would be A1, B1, … , G12, H12. column &lt;- rep(1:12, each=8) row &lt;- rep(LETTERS[1:8],12) Well &lt;- paste0(row,column) Well [1] &quot;A1&quot; &quot;B1&quot; &quot;C1&quot; &quot;D1&quot; &quot;E1&quot; &quot;F1&quot; &quot;G1&quot; &quot;H1&quot; &quot;A2&quot; &quot;B2&quot; &quot;C2&quot; &quot;D2&quot; [13] &quot;E2&quot; &quot;F2&quot; &quot;G2&quot; &quot;H2&quot; &quot;A3&quot; &quot;B3&quot; &quot;C3&quot; &quot;D3&quot; &quot;E3&quot; &quot;F3&quot; &quot;G3&quot; &quot;H3&quot; [25] &quot;A4&quot; &quot;B4&quot; &quot;C4&quot; &quot;D4&quot; &quot;E4&quot; &quot;F4&quot; &quot;G4&quot; &quot;H4&quot; &quot;A5&quot; &quot;B5&quot; &quot;C5&quot; &quot;D5&quot; [37] &quot;E5&quot; &quot;F5&quot; &quot;G5&quot; &quot;H5&quot; &quot;A6&quot; &quot;B6&quot; &quot;C6&quot; &quot;D6&quot; &quot;E6&quot; &quot;F6&quot; &quot;G6&quot; &quot;H6&quot; [49] &quot;A7&quot; &quot;B7&quot; &quot;C7&quot; &quot;D7&quot; &quot;E7&quot; &quot;F7&quot; &quot;G7&quot; &quot;H7&quot; &quot;A8&quot; &quot;B8&quot; &quot;C8&quot; &quot;D8&quot; [61] &quot;E8&quot; &quot;F8&quot; &quot;G8&quot; &quot;H8&quot; &quot;A9&quot; &quot;B9&quot; &quot;C9&quot; &quot;D9&quot; &quot;E9&quot; &quot;F9&quot; &quot;G9&quot; &quot;H9&quot; [73] &quot;A10&quot; &quot;B10&quot; &quot;C10&quot; &quot;D10&quot; &quot;E10&quot; &quot;F10&quot; &quot;G10&quot; &quot;H10&quot; &quot;A11&quot; &quot;B11&quot; &quot;C11&quot; &quot;D11&quot; [85] &quot;E11&quot; &quot;F11&quot; &quot;G11&quot; &quot;H11&quot; &quot;A12&quot; &quot;B12&quot; &quot;C12&quot; &quot;D12&quot; &quot;E12&quot; &quot;F12&quot; &quot;G12&quot; &quot;H12&quot; We can now generate a dataframe that lists the wells and the values, which is a (luminescence) intensity. We can also add two additional columns that list the row and column information: df_tidy_wells &lt;- data.frame(column, row, Well, Intensity=data_as_list) head(df_tidy_wells) column row Well Intensity 1 1 A A1 2010 2 1 B B1 3210 3 1 C C1 1965 4 1 D D1 2381 5 1 E E1 1292 6 1 F F1 991 df_tidy_wells %&gt;% write.csv(&quot;df_tidy_wells.csv&quot;) This concludes the conversion of data from the plate layout into a tidy format. The instructions explained here are used in Protocol 4 "],["plotting-the-data.html", "Chapter 3 Plotting the Data 3.1 Data over time (continuous vs. continuous) 3.2 Discrete conditions 3.3 Statistics 3.4 Plot-a-lot - discrete data 3.5 Optimizing the data visualization 3.6 Adjusting the layout 3.7 Plot-a-lot - continuous data", " Chapter 3 Plotting the Data When the data is in the right shape, it is ready for plotting. In R there is a dedicated package, ggplot2, for state-of-the-art data visualization. It is part of the ‘tidyverse’ and available when the tidyverse package is loaded. The ggplot2 package is extremely versatile and the plots can be fully customized. This great advantage comes with a disadvantage and that is complexity. Hopefully this chapter will get you started with generating some informative and good-looking plots. In the last chapter with Complete protocols we’ll dive deeper into details. The default ‘theme’ that is used in ggplot uses a grey plotting area with white gridlines, as can be seen in the plots that were presented in the previous sections. Since I prefer a more classic, white plotting area, I use a different theme from now on. This is theme_light() and it can be set in R (when the tidyverse or ggplot2 package is loaded) as follows: theme_set(theme_light()) ggplot(mtcars, aes(wt, mpg)) + geom_point() 3.1 Data over time (continuous vs. continuous) We have previously used a quick plot function qplot() from the ggplot2 package. We will use it here again for showing single cell responses from timelapse imaging. First, we load the (tidy) data and filter the data that reports on Rho and does not have any missing values in the column named activity: df_tidy &lt;- read.csv(&quot;df_S1P_combined_tidy.csv&quot;) df_Rho &lt;- df_tidy %&gt;% filter(Condition == &#39;Rho&#39;) %&gt;% filter(!is.na(activity)) We use qplot() to plot the data, by defining the dataframe, and by selecting what data is used for the x- and y-axis. The default geometry (dots) is overridden to show lines and we need to indicate which column defines which is grouped and connected by the line (in this case it is defined by the column object): qplot(data=df_Rho, x=Time, y=activity, geom = &#39;line&#39;, group=object) Let’s now change to the ggplot() function, as it allows for more flexibility. First we create an identical plot: ggplot(data=df_Rho, aes(x=Time, y=activity, group=object)) + geom_line() The aes() function is used for mapping “aesthetics”. The aesthetics specify how the variables from the dataframe are used to visualise those variables. In this case the data in the column ‘Time’ is used for mapping the data onto the x-axis and data in the column ‘activity’ is used to map the data onto the y-axis. The geom_line() function specifies that the data is shown as a line and it can be used to set the appearance of the line. We can set the linewidth (size) and the transparency (alpha). Moreover, we can map the different objects to different colors with aes(): ggplot(data=df_Rho, aes(x=Time, y=activity)) + geom_line(aes(color=object), size=1, alpha=0.8) Since ggplot supports layers, we can add another layer that shows the data as dots using geom_point(). Note that when the definition of the plot spans multiple lines, each line (when followed by another line should end with a +: ggplot(data=df_Rho, aes(x=Time, y=activity)) + geom_line(aes(color=object), size=1, alpha=0.8) + geom_point(size=2, alpha=0.8) Note that the order is important, the geometry that is defined by the last function (lines in this example) appears on top in the plot: ggplot(data=df_Rho, aes(x=Time, y=activity)) + geom_point(size=2, alpha=0.8) + geom_line(aes(color=object), size=1, alpha=0.8) There is another way to define a plot and add layers. First we define a ggplot object: p &lt;- ggplot(data=df_Rho, aes(x=Time, y=activity)) We can define layers with different geometries and add these to the ggplot object: p &lt;- p + geom_point(size=2, alpha=0.8) p &lt;- p + geom_line(aes(color=object), size=1, alpha=0.8) Just like the content of a dataframe can be displayed by typing its name, the plot can be shown by typing its name: p This is convenient and we can use it to experiment with different visualizations. Here I demonstrate this modify&amp;plot approach to remove the legend: p + theme(legend.position = &quot;none&quot;) Or remove the legend and add a title: p + theme(legend.position = &quot;none&quot;) + ggtitle(&quot;Single cell responses over time&quot;) Since ggplot() is part of the tidyverse, it accepts dataframes that are passed through a pipe: %&gt;% This can be used to select a subset of a dataframe for plotting. Below, the ‘Rac’ condition is filtered from the dataframe df_tidy and passed to ggplot for plotting: df_tidy %&gt;% filter(Condition == &#39;Rac&#39;) %&gt;% ggplot(aes(x=Time, y=activity)) + geom_line(aes(color=object)) 3.2 Discrete conditions First, we load a dataset that has intensity measurements for 5 different conditions. For each conditions there are three measurements. This would be a typical outcome of the quantification of a western blot for N=3: df &lt;- read.csv(&#39;Low_n_tidy.csv&#39;) head(df) Condition Intensity 1 A 1.0 2 B 10.3 3 C 5.5 4 D 4.5 5 E 2.3 6 A 1.1 The basic function to plot the data is ggplot(). We supply the name of the dataframe and we define how to ‘map’ the data onto the x- and y-axis. For instance, we can plot the different conditions on the x-axis and show the size measurements on the y-axis: ggplot(data=df, mapping = aes(x=Condition, y=Intensity)) This defines the canvas, but does not plot any data yet. To plot the data, we need to define how it will be plotted. We can choose to plot it as dots with the function geom_point(): ggplot(data=df, mapping = aes(x=Condition, y=Intensity)) + geom_point() Within geom_point() we can specify the looks of the dot. For instance, we can change its color, shape and size: ggplot(data=df, mapping = aes(x=Condition, y=Intensity)) + geom_point(color=&quot;blue&quot;, shape=18 ,size=8) One of the issues with data that has low N, is that it may not look ‘impressive’, in the sense that there is lots of empty space on the canvas. This may be a reason to resort to bar graphs. However, bar graphs only show averages, which hinders transparent communication of results (https://doi.org/10.1371/journal.pbio.1002128). In situations where a bar graph is added, it has to be defined in the first layer to not overlap with the datapoints: ggplot(data=df, mapping = aes(x=Condition, y=Intensity)) + geom_bar(stat = &quot;summary&quot;, fun = &quot;mean&quot;) + geom_point(size=4) In the default setting, there’s too much emphasis on the bar. This can be changed by formatting the looks of the bars, i.e. by changing the fill color, and the width: ggplot(data=df, mapping = aes(x=Condition, y=Intensity)) + geom_bar(stat = &quot;summary&quot;, fun = &quot;mean&quot;, fill=&quot;grey80&quot;, width=0.7) + geom_point(size=4) The overlap of the dots can be reduced by introducing ‘jitter’ which displays the dots with a random offset. Note that the extent of the offset can be controlled and should not exceed the width of the bar. Another way to improve the visibility of overlapping dots is to make the dots transparant. This is controlled by ‘alpha’, which should be a number between 0 (fully transarent, invisible) and 1 (not transparant). In the graph below, both jitter and transparancy are used. ggplot(data=df, mapping = aes(x=Condition, y=Intensity)) + geom_bar(stat = &quot;summary&quot;, fun = &quot;mean&quot;, fill=&quot;grey80&quot;, width=0.7) + geom_jitter(size=4, width=0.2, alpha=0.7) The jitter is applied randomly. To make a plot with reproducible jitter, one can fix the seed that is used for randomization by providing set.seed() with a number of choice, which fixes the randomness: set.seed(1) ggplot(data=df, mapping = aes(x=Condition, y=Intensity)) + geom_bar(stat = &quot;summary&quot;, fun = &quot;mean&quot;, fill=&quot;grey80&quot;, width=0.7) + geom_jitter(size=4, width=0.2, alpha=0.7) In the plot above, the length of the bar reflects the average value. This is only true when the bar starts from 0. Situations in which the length of the bar does not accurately reflect the number are: - using a linear scale that does not include zero - cutting the axis - using a logarithmic scale, which (per definition) does not include zero. An example is shown below, where the logarithmic scale and limites are defined in by the scale_y_log10() function. Due to the non-linear scale, the length of the bar is not proportional to the value (the average) it reflects. This leads to misinterpretation of the data. ggplot(data=df, mapping = aes(x=Condition, y=Intensity)) + geom_bar(stat = &quot;summary&quot;, fun = &quot;mean&quot;, fill=&quot;grey80&quot;, width=0.7) + geom_jitter(size=4, width=0.2, alpha=0.7) + scale_y_log10(limits=c(.5,12)) 3.2.1 X-axis data: qualitative versus quantitative data Suppose that the data comes from an experiment in which the data are measured at different time points. First we define a vector that defines the timepoints: e.g. 0, 1, 2, 5, 10: t &lt;- c(0,1,2,5,10) We need to repeat these timepoints three times, once for each replicate: t3 &lt;- rep(t,3) Now we can add the vector to the dataframe: df &lt;- df %&gt;% mutate(Time=t3) And plot the activity for the different time points: ggplot(data=df, mapping = aes(x=Time, y=Intensity)) + geom_bar(stat = &quot;summary&quot;, fun = &quot;mean&quot;, fill=&quot;grey80&quot;, width=0.7) + geom_jitter(size=4, width=0.2, alpha=0.7) This graph looks different because we it has numbers on the x-axis. The numbers are treated as ‘continuous quantitative data’ and the data is positioned according to the values. To treat the numbers as conditions or labels we need to convert them to qualitative data. This class of data is called factors in R. We can verify the class of data by selecting the column using the class() function. Here we select the third column of the dataframe to check its class: class(df[,3]) [1] &quot;numeric&quot; Now we convert the column ‘Time’ to the class factor: df &lt;- df %&gt;% mutate(Time=as.factor(Time)) Let’s verify that the class is changed: class(df[,3]) [1] &quot;factor&quot; We use the same line of code to plot the data, and the graph will look similar to the graph that used the conditions indicated with letters. ggplot(data=df, mapping = aes(x=Time, y=Intensity)) + geom_bar(stat = &quot;summary&quot;, fun = &quot;mean&quot;, fill=&quot;grey80&quot;, width=0.7) + geom_jitter(size=4, width=0.2, alpha=0.7) Whether you treat the numbers on the x-axis as labels or values determines on the data and the message that you want to convey. If it is important to know when the highest activity occurs, it may not matter that the points are not equidistant (as in this example). In fact the data in the plot may better align with the lanes of a blot (which are also equidistant) from which the data are quantified . On the other hand, if you are interested in the dynamics of the activity, the timepoints on the x-axis should reflect the actual values to enable proper interpretation. 3.3 Statistics 3.3.1 Introduction Thus far, we were mainly concerned with plotting the data. But plots with scientific data often feature some kind of statistics. Next to the mean or median, error bars are used to summarize variability or to reflect the uncertainty of the measurement. Intermezzo: Descriptive vs Inferential Statistics It is a good idea to reflect on the reason to display statistics and it is essential to understand that you can choose between descriptive and inferential statistics. The descriptive statistics are used to summarize the data. Examples of descriptive statistics are the mean, median and standard deviation. Boxplots also display descriptive statistics. Inferential statistics are used to make ‘inferences’ or, in other words, generalize the data that are measured to the population it was sampled from. It is used to compare experiments and make predictions. Examples of inferential statistics are standard error of the mean and confidence intervals. There are (at least) two ways to overlay statistics in a plot. The first way is demonstrated in the previous section, where a layer with the statistics (bar) was directly added to the plot. Below, we take this strategy a step further to display the standard deviation. 3.3.2 Data summaries directly added as a plot layer In the code below the stat_summary() defines a layer with statistics. The fun=mean statement indicates that the function mean() should be applied to every condition on the x-axis: ggplot(data=df, mapping = aes(x=Time, y=Intensity)) + geom_jitter(size=4, width=0.2, alpha=0.7) + stat_summary(fun = mean, geom=&#39;point&#39;, size=8, color=&#39;blue&#39;) This is pretty ugly and it is more common to indicate the mean (or median) with a horizontal line. This can be done by specifying the shape of the point: ggplot(data=df, mapping = aes(x=Time, y=Intensity)) + geom_jitter(size=4, width=0.2, alpha=0.7) + stat_summary(fun = mean, geom=&#39;point&#39;, shape=95, size=24, color=&#39;black&#39;) This works, but it doesn’t allow us to specify the width and the thickness of the line. To have better control over the line we turn to another ‘geom’, geom_errorbar(). This functions is actually used to display errorbars, but if we only set one value for the min and max, it allows us to display the mean. We can change the looks of the horizontal bar by changing the width and the size. The latter defines the thickness of the line. ggplot(data=df, mapping = aes(x=Time, y=Intensity)) + geom_jitter(size=4, width=0.2, alpha=0.7) + stat_summary(fun.min=mean, fun.max=mean, geom=&#39;errorbar&#39;, width=0.6, size =1) We can also indicate the standard deviation (SD), but we need to define a custom function to calculate the position of the upper and lower limit of the errorbar. That is, we need to display mean+SD and mean-SD for each condition. The code for the function that defines the lower limit is: function(y) {mean(y)-sd(y)} and for the upper limit it is: function(y) {mean(y)+sd(y)} Here we go (note that the width is set to a smaller value): ggplot(data=df, mapping = aes(x=Time, y=Intensity)) + geom_jitter(size=4, width=0.2, alpha=0.7) + stat_summary(fun.min=function(y) {mean(y) - sd(y)}, fun.max=function(y) {mean(y) + sd(y)}, geom=&#39;errorbar&#39;, width=0.3, size =1) By combing the layers that define the mean and de sd, we can show both: ggplot(data=df, mapping = aes(x=Time, y=Intensity)) + geom_jitter(size=4, width=0.2, alpha=0.7) + stat_summary(fun.min=function(y) {mean(y) - sd(y)}, fun.max=function(y) {mean(y) + sd(y)}, geom=&#39;errorbar&#39;, width=0.3, size =1) + stat_summary(fun.min=mean, fun.max=mean, geom=&#39;errorbar&#39;, width=0.6, size =1) Finally, an example that displays the 95% confidence intervals: ggplot(data=df, mapping = aes(x=Time, y=Intensity)) + geom_jitter(size=4, width=0.2, alpha=0.7) + stat_summary(fun.min=function(y) {mean(y) - qt((1-0.95)/2, length(y) - 1) * sd(y) / sqrt(length(y) - 1)}, fun.max=function(y) {mean(y) + qt((1-0.95)/2, length(y) - 1) * sd(y) / sqrt(length(y) - 1)}, geom=&#39;errorbar&#39;, width=0.3, size =1) + stat_summary(fun.min=mean, fun.max=mean, geom=&#39;errorbar&#39;, width=0.6, size =1) This method works, but the code to generate this graph is pretty long and the definition of the function make it difficult to follow and understand what’s going on. In addition, the values for the statistics are not accessible. To solve these issue, I will demonstrate below a more intuitive way to calculate and display the statistics. 3.3.3 Data summaries from a dataframe We start out from the same data and dataframe. First, we calculate the statistics and assign the values to a new dataframe. To this end, we use the summarise() function for each condition (Time in this dataset) which we indicate by the use of group_by(): df_summary &lt;- df %&gt;% group_by(Time) %&gt;% summarise(n=n(), mean=mean(Intensity), sd=sd(Intensity)) head(df_summary) # A tibble: 5 x 4 Time n mean sd &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 3 1 0.1 2 1 3 9.83 0.451 3 2 3 6.1 1.22 4 5 3 4.67 0.208 5 10 3 3.17 0.850 This new dataframe can be used as source for displaying the statistics. Note that we need to indicate the df_summary dataframe for each layer: ggplot(data = df) + geom_jitter(aes(x=Time, y=Intensity),size=4, width=0.2, alpha=0.7) + geom_errorbar(data=df_summary, aes(x=Time,ymin=(mean-sd), ymax=(mean+sd)), width=0.3, size =1) + geom_errorbar(data=df_summary, aes(x=Time,ymin=(mean), ymax=(mean)), width=0.6, size =1) How about other stats? If we calculate other stats like sem, MAD and confidence intervals and store those in a dataframe, we can retrieve those for plotting as well. Below the code for the calculation of the most common statistics is presented. There is no function to calculare sem or the confidence interval and so we calulate those using mutate(). The confidence levels is set to 95%: Confidence_level = 0.95 df_summary &lt;- df %&gt;% group_by(Time) %&gt;% summarise(n=n(), mean=mean(Intensity), median=median(Intensity), sd=sd(Intensity)) %&gt;% mutate(sem=sd/sqrt(n-1), mean_CI_lo = mean + qt((1-Confidence_level)/2, n - 1) * sem, mean_CI_hi = mean - qt((1-Confidence_level)/2, n - 1) * sem ) head(df_summary) # A tibble: 5 x 8 Time n mean median sd sem mean_CI_lo mean_CI_hi &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 3 1 1 0.1 0.0707 0.696 1.30 2 1 3 9.83 9.8 0.451 0.319 8.46 11.2 3 2 3 6.1 5.5 1.22 0.860 2.40 9.80 4 5 3 4.67 4.6 0.208 0.147 4.03 5.3 5 10 3 3.17 3.2 0.850 0.601 0.579 5.75 In principle the code for plotting the error bars that reflect the standard deviations (or sem) can be simplified if the upper and lower limit are calculated, similar to the example shown above for the 95% confidence intervals. 3.3.4 Data summaries for continuous x-axis data Let’s revisit the data from the time-course of Rho GTPase activity that we’ve looked at earlier: df_tidy &lt;- read.csv(&quot;df_S1P_combined_tidy.csv&quot;) df_Rho &lt;- df_tidy %&gt;% filter(Condition == &#39;Rho&#39;) %&gt;% filter(!is.na(activity)) Calculate the statistics for each time point: df_summary &lt;- df_Rho %&gt;% group_by(Time) %&gt;% summarise(n=n(), mean=mean(activity), median=median(activity), sd=sd(activity)) %&gt;% mutate(sem=sd/sqrt(n-1), mean_CI_lo = mean + qt((1-Confidence_level)/2, n - 1) * sem, mean_CI_hi = mean - qt((1-Confidence_level)/2, n - 1) * sem ) head(df_summary) # A tibble: 6 x 8 Time n mean median sd sem mean_CI_lo mean_CI_hi &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 12 0.998 0.997 0.00477 0.00144 0.995 1.00 2 0.167 12 1.00 1.00 0.00615 0.00186 0.999 1.01 3 0.333 12 0.998 0.998 0.00260 0.000783 0.996 1.00 4 0.5 12 0.999 0.998 0.00657 0.00198 0.995 1.00 5 0.667 12 1.00 1.00 0.00703 0.00212 0.998 1.01 6 0.833 12 1.00 1.00 0.00741 0.00223 0.998 1.01 With this data summary it is possible to depict the 95% confidence interval as error bars: ggplot(data = df_Rho) + geom_line(aes(x=Time, y=activity, color=object),size=1, alpha=0.7) + geom_errorbar(data=df_summary, aes(x=Time,ymin=mean_CI_lo, ymax=(mean_CI_hi)), width=0.3, size=1, alpha=0.7) It works, but it is also quite messy. Luckily ggplot has a more elegant solution and that’s geom_ribbon(): ggplot(data = df_Rho) + geom_line(aes(x=Time, y=activity, group=object),size=.5, alpha=0.4) + geom_ribbon(data=df_summary, aes(x=Time,ymin=mean_CI_lo, ymax=(mean_CI_hi)), fill=&#39;blue&#39;, alpha=0.3) Note that I also removed the color of the individual lines, as it is more about the ensemble and it’s average than the individual line. Adding a line to reflect the average is pretty straightforward: ggplot(data = df_Rho) + geom_line(aes(x=Time, y=activity, group=object),size=.5, alpha=0.4) + geom_ribbon(data=df_summary, aes(x=Time,ymin=mean_CI_lo, ymax=(mean_CI_hi)), fill=&#39;blue&#39;, alpha=0.3) + geom_line(data=df_summary, aes(x=Time,y=mean), color=&#39;blue&#39;, size=2, alpha=0.8) 3.4 Plot-a-lot - discrete data Other data summaries that are often depicted in plots are boxplots and violinplots. These are not suited for data with low n, as in the previous example. The reason is that the boxplot is defined by five values, i.e. the median (the central line), the interquartile range, IQR (the two limits of the box) and the endpoints of the two lines, also known as whiskers. It makes no sense to use a boxplot for n=5, since it does not add any new information. There is no hard cut-off, but in my opinion boxplots make sense when you have 10 or more datapoints per condition. Although the boxplot is a good data summary for normally distributed and skewed data distributions, it doesn’t capture the underlying distribution well when it is bi- or multimodal. In these cases, a violin plot is better suited. The box- and violinplot are easily added as a layer as they are defined by specific functions, geom_boxplot() and geom_violin(). First, let’s load a dataset with larger n. The function summary() provides a quick overview of the data: df &lt;- read.csv(&#39;Area_tidy.csv&#39;) summary(df) Condition value LARG: 99 Min. : 477.2 TIAM:110 1st Qu.:1152.1 wt : 74 Median :1490.9 Mean :1567.8 3rd Qu.:1838.1 Max. :3906.7 There are three conditions, with relative large n. Let’s plot the data: ggplot(df, aes(x=Condition, y=value)) + geom_jitter(width=0.2, alpha=0.5) Adding a boxplot: ggplot(df, aes(x=Condition, y=value)) + geom_jitter(width=0.2, alpha=0.5) + geom_boxplot() The geom_boxplot displays outliers (data beyond the whiskers). This is clear when only the boxplot is shown: ggplot(df, aes(x=Condition, y=value)) + geom_boxplot() When the data is displayed together with the boxplot, the outliers need to be removed to avoid duplication. And to make sure that the data is visibel and not hidden by the boxplot, we can either change the order of the layers or remove the white color that is used to fill the box: ggplot(df, aes(x=Condition, y=value)) + geom_jitter(width=0.2, alpha=0.5) + geom_boxplot(fill=NA, outlier.color = NA) In case a boxplot is used as summary, it may be useful to have the values of Q1, Q3 and the interquartile range. The code shown below can be used to calculate all these paramters and also includes the median absolute deviation (MAD) as a robust measure of variability: Confidence_level = 0.95 df_summary &lt;- df %&gt;% group_by(Condition) %&gt;% summarise(n=n(), mean=mean(value), median=median(value), sd=sd(value), MAD=mad(value, constant=1), IQR=IQR(value), Q1=quantile(value, probs=0.25), Q3=quantile(value, probs=0.75)) df_summary # A tibble: 3 x 9 Condition n mean median sd MAD IQR Q1 Q3 &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 LARG 99 1361. 1271. 486. 250. 485. 1053. 1538. 2 TIAM 110 1808. 1705. 704. 416. 818. 1293. 2112. 3 wt 74 1487. 1509. 459. 333. 662. 1158. 1821. Display of a violinplot in addition to the data: ggplot(df, aes(x=Condition, y=value)) + geom_violin() + geom_jitter(width=0.2, alpha=0.5) Personally, I like the combination of data and violinplot, but the jitter can make the plot look messy. There are packages that enable the plotting of the data according to its distribution. There is geom_sina() from ggforce package and geom_quasirandom() from the ggbeeswarm package: library(ggbeeswarm) ggplot(df, aes(x=Condition, y=value)) + geom_violin() + geom_quasirandom(width=0.3, alpha=0.5) Sometimes, you find examples of boxplots overlayed on violin plots, like this (and note that we have filled the violins with a unique color for each condition): ggplot(df, aes(x=Condition, y=value)) + geom_violin(aes(fill=Condition), alpha=0.5) + geom_boxplot(width=0.1, outlier.color = NA) I have a preference for showing the actual data. However, for large n the violinplot may be an accurate representation of the data, that better conveys the message than a cluttered plot that shows a lot of dots. 3.5 Optimizing the data visualization 3.5.1 Rotation Rotating a plot by 90 degrees can be surprisingly effective. Especially when the labels that are used for the x-axis are so long that they need to be rotated, it is better to rotate the plot. This improves the readability of the numbers and labels on both the x- and y-axis and avoids the need of tilting your head. library(ggbeeswarm) ggplot(df, aes(x=Condition, y=value)) + geom_boxplot(outlier.colour = NA) + geom_quasirandom(width=0.3, alpha=0.5) + coord_flip() An example of how to further tweak a 90˚ rotated plots is given in Protocol 2 3.5.2 Ordering conditions The order of the conditions used for the x-axis (for discrete conditions) is numeric and alphabetic. In case of a large number of conditions, it can help to sort these according to the median or mean value. This requires another order of the factors. To check the order of the factors in the dataframe we use: levels(df$Condition) [1] &quot;LARG&quot; &quot;TIAM&quot; &quot;wt&quot; Now, we change the order, sorting the factors according to the median of “value” and we verify the order: df &lt;- df %&gt;% mutate(Condition = fct_reorder(Condition, value, .fun=&#39;median&#39;)) levels(df$Condition) [1] &quot;LARG&quot; &quot;wt&quot; &quot;TIAM&quot; Let’s plot the data: ggplot(df, aes(x=Condition, y=value)) + geom_boxplot(outlier.colour = NA) + geom_quasirandom(width=0.3, alpha=0.5) The factors on the x-axis are now sorted according to the median of value. It is also possible to manually set the sequence, in this example the order is set to wt, LARG, TIAM: df &lt;- df %&gt;% mutate(Condition = fct_relevel(Condition, c(&quot;LARG&quot;, &quot;TIAM&quot;, &quot;wt&quot;))) ggplot(df, aes(x=Condition, y=value)) + geom_boxplot(outlier.colour = NA) + geom_quasirandom(width=0.3, alpha=0.5) In the examples above, we have modified the dataframe, since we used mutate() to change the order. To set the order for plotting without altering the dataframe we can define the reordering within ggplot: ggplot(df, aes(x=fct_reorder(Condition, value, .fun=&#39;median&#39;), y=value)) + geom_boxplot(outlier.colour = NA) + geom_quasirandom(width=0.3, alpha=0.5) Alternatively, we can use the pipe operator to feed the data in the reordering function and then use the reordered dataframe for plotting: df %&gt;% mutate(Condition = fct_reorder(Condition, value, .fun=&#39;median&#39;)) %&gt;% ggplot(aes(x=Condition, y=value)) + geom_boxplot(outlier.colour = NA) + geom_quasirandom(width=0.3, alpha=0.5) We can check that the order of the levels in the dataframe has not changed and differs from the order in the plot: levels(df$Condition) [1] &quot;LARG&quot; &quot;TIAM&quot; &quot;wt&quot; 3.6 Adjusting the layout Details matter, also in data visualization. Editing labels, adding titles or annotating data can make the difference between a poor and a clear data visualization. Although it can be quicker and easier to edit a plot with software that deals with vectors, it is not reproducible. And when you need to change the graph, the editing starts all over again. Luckily, with ggplot2, you have full control over every element. A lot of elements are controlled by the function theme(), and examples are the label size and color, the grid, the legend style and the color of the plot elements. This level of control offers great power, but it can be quite daunting (and non-intuitive) for new users. We discuss a couple of straightforward manipulations of the theme below. More detailed modifications of the layout will be showcased in the chapter with Complete Protocols. 3.6.1 Themes Let’s look at a violinplot and we plot it with the default theme, which has a grey background: p &lt;- ggplot(df, aes(x=Condition, y=value)) + geom_violin(aes(fill=Condition), alpha=0.5) + geom_boxplot(width=0.1, outlier.color = NA) + theme_grey() p The default theme is OK-ish and we can change it to one of the other themes that are available in the ggplot2 package, for instance theme_classic(): p + theme_classic() The ggplot2 package has a default dark theme, but that only generate a dark background in the plot area. I made a customized theme theme_darker() for plots on dark background, e.g. black slides. It needs to be loaded with the source() function and then it can be applied: source(&quot;https://raw.githubusercontent.com/JoachimGoedhart/PlotTwist/master/themes.R&quot;) p + theme_darker() Finally, we can modify the text size of all text elements: p + theme_bw(base_size = 16) To reset the theme to the default that is used throughout the book: p &lt;- p + theme_light() 3.6.2 Legend The legend can be controlled through the theme() function. Legends are automatically created when different colors or shapes are used. One example is the plot below, where different conditions are shown in different colors. To change the style, we define the plot object p: To remove the legend we use: p + theme(legend.position = &quot;none&quot;) Other options are “top”, “left”, “bottom” and (the default) “right”. The items of the legend can also be displayed horizontally, which is a nice fit when the legend is shown on top of the plot: p + theme(legend.position = &quot;top&quot;, legend.direction = &quot;horizontal&quot;) To left align the legend: p + theme(legend.position = &quot;top&quot;, legend.direction = &quot;horizontal&quot;, legend.justification=&#39;left&#39;) 3.6.3 Grids I am not a big fan of grids, so I often remove it: p + theme(panel.grid = element_blank()) To only remove the vertical grid and make the horizontal grid more pronounced: p + theme(panel.grid.major.x = element_blank(), panel.grid.major.y = element_line(size=0.5, color=&#39;grey30&#39;), panel.grid.minor.y = element_line(size=0.2, color=&#39;grey30&#39;) ) 3.6.4 Labels/Titles Clear labeling aids the interpretation of the plot. The actual labels and titles are changed or added with the function labs() and their style is controled with the theme() function. Let’s first look at the labels. The titles of the axes and legend are retrieved from the column names in the dataframe. To adjust the axis labels with labs(): p + labs(x=&#39;Perturbation&#39;, y=&#39;Area [µm]&#39;) Note that the title of the legend is not changed. To change the legend title we need to supply a label for ‘fill’, since the legend in this example indicates the color that was used to ‘fill’ the violin plots: p + labs(x=&#39;Perturbation&#39;, y=&#39;Area [µm]&#39;, fill=&#39;Perturbation:&#39;) A title, subtitle and caption can also be added with labs(): p + labs(x=&#39;Condition&#39;, y=&#39;Area [µm]&#39;, title=&#39;This is the title...&#39;, subtitle=&#39;...and this is the subtitle&#39;, caption=&#39;A caption is text at the bottom of the plot&#39;) The style of the different labels can be set with theme(): p + labs(x=&#39;Condition&#39;, y=&#39;Area [µm]&#39;, title=&#39;This is the title...&#39;, subtitle=&#39;...and this is the subtitle&#39;, caption=&#39;A caption is text at the bottom of the plot&#39;) + theme(axis.title.x = element_text(size=16, color=&#39;black&#39;), axis.title.y = element_text(size=16, color=&#39;black&#39;), axis.text = element_text(size=14, color=&#39;orange&#39;), plot.title = element_text(size=18, color=&#39;slateblue&#39;), plot.subtitle = element_text(size=12, color=&#39;slateblue&#39;, hjust = 1), plot.caption = element_text(size=10, color=&#39;grey60&#39;, face = &quot;italic&quot;) ) This is a demonstration of how the different pieces of text can be modified, not a template for a proper data visualization since it uses too many, unnecessary, colors! 3.7 Plot-a-lot - continuous data It can be a challenge to look at individual data or samples instead of summaries when you have a large amount of data. I will illustrate a number of options based on the timeseries data of Rac activity, that we have seen before. This dataset is still quite simple, but given its heterogeneity it illustrates well how the data can be presented in such a way that all traces can be inspected. df_Rac &lt;- read.csv(&quot;df_S1P_combined_tidy.csv&quot;) %&gt;% filter(Condition == &#39;Rac&#39;) %&gt;% filter(!is.na(activity)) head(df_Rac) Condition Treatment Time object activity 1 Rac S1P 0 Cell 01 1.0012160 2 Rac S1P 0 Cell 02 1.0026460 3 Rac S1P 0 Cell 03 1.0022090 4 Rac S1P 0 Cell 04 0.9917870 5 Rac S1P 0 Cell 05 0.9935569 6 Rac S1P 0 Cell 06 0.9961453 We have seen this data before, but let’s look at this again in an ordinary line plot: ggplot(data=df_Rac, aes(x=Time, y=activity)) + geom_line(aes(color=object)) There is a lot to see here and it is a bit of mess. Clearly, there is variation, but it is difficult to connect a line, by its color to the cell it represents. Below, there are some suggestions how this data can be presented more clearly. 3.7.1 Small multiples Small multiples remind me of a stamp collection, where every stamp is a (small) plot. This works very well to display lots of data. It is also pretty straightforward in ggplot2 with facet_wrap(): ggplot(data=df_Rac, aes(x=Time, y=activity)) + geom_line() + facet_wrap(~object) I got rid of color, since it would be redundant and the improved contrast of the black line helps to focus on the data. The lay-out of the 32 mini plots can be improved by fixing the number of columns: ggplot(data=df_Rac, aes(x=Time, y=activity)) + geom_line() + facet_wrap(~object, ncol = 8) Since the small multiple is at its best, when the data stands out and the text and other elements are minimized. Here is an extreme version of the plot to make that point: ggplot(data=df_Rac, aes(x=Time, y=activity)) + geom_line() + facet_wrap(~object, ncol = 8) + theme_void() Further optimization of a small multiple plot is discussed in Protocol 3. For complex experimental designs, the data can split according to two different factors. This also uses a faceting strategy, with the facet_grid() function. This works well when the data has two discrete variables and the application of this function is demonstrated in Protocol 4. 3.7.2 Heatmaps Heatmaps are well suited for dense data and have traditionally been used for microarray and other -omics data. Heatmaps can also be used for timeseries data. To do this, we (i) keep the x-axis as is, (ii) map the objects on the y-axis and (iii) specify that the color of the tile reflects activity: ggplot(data=df_Rac, aes(x=Time, y=object)) + geom_tile(aes(fill=activity)) The default colorscale is not ideal. I personally like the (colorblind friendly) viridis scale and this colorscale can be used to fill the tile with scale_fill_viridis_c(). To sort the data according to the object labels as they appear in the dataframe: ggplot(data=df_Rac, aes(x=Time, y=object)) + geom_tile(aes(fill=activity)) + scale_fill_viridis_c() To reverse the order of the objects we can use fct_rev(): ggplot(data=df_Rac, aes(x=Time, y=fct_rev(object))) + geom_tile(aes(fill=activity)) + scale_fill_viridis_c() A heatmap does not need a grid and usually has no axes. Plus, when there are many objects, it makes sense to hide their names, ticks and the y-axis label: p &lt;- ggplot(data=df_Rac, aes(x=Time, y=fct_rev(object))) + geom_tile(aes(fill=activity)) + scale_fill_viridis_c() + theme(text = element_text(size=16), # Remove borders panel.border = element_blank(), # Remove grid panel.grid.major = element_blank(), panel.grid.minor = element_blank(), # Remove text of the y-axis axis.text.y = element_blank(), # Remove ticks on y-axis axis.ticks.y = element_blank(), # Remove label of y-axis axis.title.y = element_blank(), # Make x-axis ticks more pronounced axis.ticks = element_line(colour = &quot;black&quot;) ) p 3.7.3 Custom objects The use of titles, captions and legends will add infomration to the data visualization. Tweaking the theme will improve the style of the plot and give it the right layout. There’s another layer of attributes that are not discussed yet and these are custom objects and labels. These additions will help to tell the story, i.e. by explaining the experimental design. Let’s look at the cellular response to stimulation with a ligand. The df_Rho dataframe that we have used earlier is used in this example and we plot the average and standard deviation: p &lt;- ggplot(data=df_Rho, aes(x=Time, y=activity)) + stat_summary(fun = mean, geom=&#39;line&#39;, size=2) + stat_summary(fun.min=function(y) {mean(y) - sd(y)}, fun.max=function(y) {mean(y) + sd(y)}, geom=&#39;ribbon&#39;, color=&#39;black&#39;, size =.1, alpha=0.2) p The ligand is added at t=1.75 and we can add a vertical line with geom_vline() to indicate this: p + geom_vline(xintercept = 1.75, size=1, color=&quot;grey20&quot;) More flexible labeling is provided with the annotate() function, which enables the addition of line segments, rectangles and text. First, let’s reproduce the the previous plot with this function, by defining a line segment. The line segment has two coordinates for the start and two for the end. The x position is 1.75 for both start and end. For the y-coordinate we can use Inf and -Inf to define the endpoints of the line. The use of Inf has two advantages. We do not need to define the exact points for the start and end and we prevent rescaling of the plot to show the segment: p + annotate(&quot;segment&quot;, x = 1.75, xend = 1.75, y = -Inf, yend = Inf, size=1, color=&quot;grey20&quot;) In a similar way we can define a rectangle to highlight the time where the ligand was present (It is possible to use Inf for xmax here, in that case the blue rectangle would fill up the entire plot to the right). Since the rectangle is added as the last layer, it would occlude the data. That’s why a an alpha level of 0.1 is used to make the rectangle transparent: p + annotate(&quot;rect&quot;, xmin = 1.75, xmax = 10, ymin = -Inf, ymax = Inf, fill=&quot;blue&quot;, alpha=0.1) We can also add an rectangle to the top of the graph: p + annotate(&quot;rect&quot;, xmin = 1.75, xmax = 10, ymin = 1.26, ymax = 1.27, fill=&quot;black&quot;) + annotate(&quot;text&quot;, x=5, y=1.29, label=&quot;ligand&quot;, size=6) In the example above, the rectangle and text are added to the plot area and the y-axis is expanded. To move the annotation out of the plot area, we need to increase the white space above the plot with theme() and we can scale the plot with coord_cartesian(): p + annotate(&quot;rect&quot;, xmin = 1.75, xmax = 10, ymin = 1.25, ymax = 1.26, fill=&quot;black&quot;) + annotate(&quot;text&quot;, x=5, y=1.28, label=&quot;ligand&quot;, size=6) + coord_cartesian(ylim = c(0.98,1.23), clip = &#39;off&#39;) + theme(plot.margin = margin(t = 50, r = 0, b = 0, l = 0, unit = &quot;pt&quot;)) The annotation is part of the plot and would normally be invisible since everything outside the plot area is clipped. To show the annotation, clip = 'off' is needed in the code above. However, this may lead to undesired behavior when the scaling is set in such a way that the data falls outside the plot area. Although you could use it to create a dramatic ‘off-the-charts’ effect. p + annotate(&quot;rect&quot;, xmin = 1.75, xmax = 10, ymin = 1.25, ymax = 1.26, fill=&quot;black&quot;) + annotate(&quot;text&quot;, x=5, y=1.28, label=&quot;ligand&quot;, size=6) + coord_cartesian(ylim = c(0.98,1.15), clip = &#39;off&#39;) + theme(plot.margin = margin(t = 130, r = 0, b = 0, l = 0, unit = &quot;pt&quot;)) This concludes a couple examples of the annotate() function. Protocol 5 is a good example how these annotations can assist to build an informative data visualization. "],["complete-protocols.html", "Chapter 4 Complete protocols 4.1 Protocol 1 - Spectra of fluorescent proteins 4.2 Protocol 2 - A superplot of calcium concentrations 4.3 Protocol 3 - small multiples of time courses 4.4 Protocol 4 - Plotting data from a 96-wells experiment 4.5 Protocol 5 - A map of amino acids", " Chapter 4 Complete protocols This chapter showcases a number of complete protocols for different kinds of data visualizations. Each protocol starts with the raw data and ends with a publication quality plot. The data is available from Github, just follow this link. In the same folder you can find the R markdown files for each protocol. To reproduce the data visualizations you can either take the data and follow the instructions in the chapter. Alternatively you can download the R markdown file (and the data) and run it step by step. The R markdown files can be used as a starting point to apply the same visualization to your own data. 4.1 Protocol 1 - Spectra of fluorescent proteins This protocol describes how you can turn a csv with spectral data that is obtained from FPbase.org into a plot of those spectra. First, we load the required package: library(tidyverse) For this data visualization, I selected several spectra from fluorescent proteins at FPbase.org: https://www.fpbase.org/spectra/?s=1746,6551,101,102,123,124,1604,1606&amp;showY=0&amp;showX=1&amp;showGrid=0&amp;areaFill=1&amp;logScale=0&amp;scaleEC=0&amp;scaleQY=0&amp;shareTooltip=1&amp;palette=wavelength The data was downloaded in CSV format (by clicking on the button in the lower right corner of the webpage) and renamed to ‘FPbase_Spectra_4FPs.csv’. We read the data from the CSV by using the read_csv() function. This function is part of the tidy verse and loads the data as a tibble. It also guesses type of data for each column. To hide that information, we use show_col_types = FALSE here. df_raw &lt;- read_csv(&quot;FPbase_Spectra_4FPs.csv&quot;, show_col_types = FALSE) Let’s briefly look at what we have loaded: glimpse(df_raw) Rows: 512 Columns: 9 $ Wavelength &lt;dbl&gt; 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310… $ `mTurquoise2 EM` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ `mTurquoise2 EX` &lt;dbl&gt; 0.2484, 0.2266, 0.2048, 0.1852, 0.1634, 0.1482, 0.132… $ `mNeonGreen EM` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ `mNeonGreen EX` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ `mScarlet-I EM` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ `mScarlet-I EX` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ `miRFP670 EM` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ `miRFP670 EX` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… The data needs conversion to a tidy format before plotting. Since we have a single continuous data column with Wavelength information that is used for the x-axis, it is excluded from the operation: df_1 &lt;- pivot_longer( df_raw, cols = -Wavelength, names_to = &quot;Sample&quot;, values_to = &quot;Intensity&quot; ) There are several rows that have NA values for Intensity and this is how to get rid of that: df_1 &lt;- df_1 %&gt;% drop_na(Intensity) The column ‘Sample’ has labels for the fluorescent protein and the type of spectrum. We can separate that column into two different columns that we name ‘Fluorescent Protein’ and ‘Spectrum’: df_1 &lt;- df_1 %&gt;% separate(Sample, c(&quot;Fluorescent Protein&quot;, &quot;Spectrum&quot;), sep = &quot; &quot;) Let’s do a first attempt and plot the data: ggplot( data = df_1, aes(x = Wavelength, y = Intensity, color = `Fluorescent Protein`) ) + geom_line(aes(linetype = Spectrum), size = 1) This looks pretty good already. Now let’s change the order of the fluorescent proteins to their order in the plot: df_1 &lt;- df_1 %&gt;% mutate(`Fluorescent Protein` = forcats::fct_relevel( `Fluorescent Protein`, c(&quot;mTurquoise2&quot;, &quot;mNeonGreen&quot;, &quot;mScarlet-I&quot;, &quot;miRFP670&quot;) )) The data is in the right shape now, so let’s save it: df_1 %&gt;% write.csv(&quot;protocol_1.csv&quot;, row.names=FALSE) We define the plot object and add an extra geometry, geom_area() to fill the area under the curves: p &lt;- ggplot( data = df_1, aes( x = Wavelength, y = Intensity, fill = `Fluorescent Protein` ) ) + geom_line(aes(linetype = Spectrum), size = 0.5, alpha = 0.5 ) + geom_area( aes(linetype = Spectrum), color = NA, position = &quot;identity&quot;, size = 1, alpha = 0.5 ) Let’s check the result: p Next, we set the limits of the axis and force the y-axis to start at 0 p &lt;- p + scale_y_continuous(expand = c(0, 0), limits = c(0, 1.1)) + scale_x_continuous(expand = c(0,0), limits = c(350, 810)) Add labels: p &lt;- p + labs( title = &quot;Spectra of Fluorescent Proteins&quot;, x = &quot;Wavelength [nm]&quot;, y = &quot;Normalized Intensity [a.u.]&quot;, caption = &quot;(based on data from FPbase.org)&quot;, tag = &quot;Protocol 1&quot; ) Modify the layout by adjusting the theme. Comments are used to explain effect of the individual lines of code: p &lt;- #Set text size p + theme_light(base_size = 14) + theme( plot.caption = element_text( color = &quot;grey80&quot;, hjust = 1 ), #Remove grid panel.grid.major = element_blank(), panel.grid.minor = element_blank(), #Set position of legend legend.position = &quot;top&quot;, legend.justification = &quot;left&quot; #Define the legend layout ) + guides( linetype = &quot;none&quot;, fill = guide_legend(title = NULL, label.position = &quot;right&quot;) ) p We are almost there, except that the colors of the plot do not match with the natural colors of the fluorescent proteins. Let’s fix that by defining a custom color palette. The order of the colors matches with the order of the fluorescent proteins that was defined earlier: custom_colors &lt;- c(&quot;blue&quot;, &quot;green&quot;, &quot;orange&quot;, &quot;red&quot;) To apply the custom colors to the filled area: p &lt;- p + scale_fill_manual(values = custom_colors) This is the result: p To save this plot as a PNG file: png(file=paste0(&quot;Protocol_1.png&quot;), width = 4000, height = 2600, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 4.2 Protocol 2 - A superplot of calcium concentrations This protocol is used to create a superplot which differentiates between technical and biological replicates. The concept of superplots has been reported by Lord and colleagues (Lord et al., 2021). We will use the data from a publication by van der Linden et al. (2021) which was used to create figure 5e. The original figure summarizes the data from all experiments and does not identify the biological replicates. Below, we will differentiate the biological replicates, by treating each batch of neutrophils as a biological replicate. We start by loading the required tidyverse package: library(tidyverse) We define the confidence level as 95%: Confidence_level &lt;- 0.95 The data is stored in an excel sheet and we read it, skipping the first 6 lines which contain comments: df_raw &lt;- readxl::read_excel(&#39;figure5.xlsx&#39;, skip=6) Let’s look at the data: head(df_raw) # A tibble: 6 x 7 `Experimental day` `Replicate no.` `Neutrophil no.` `Batch no. neutroph… Stage &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 1 1 1 1 befo… 2 1 1 1 1 befo… 3 1 1 1 1 befo… 4 1 1 1 1 befo… 5 1 1 1 1 befo… 6 1 1 1 1 befo… # … with 2 more variables: dF/F0 &lt;chr&gt;, Calcium (uM) &lt;dbl&gt; The data is already in a tidy format. The column with ‘Stage’ has the four different conditions for which we will compare the data in the column ‘Calcium (uM)’. We change the name of the column ‘Batch no. neutrophils’ to ‘Replicate’ and make sure the different replicates are treated as factors (qualitative data): df_raw &lt;- df_raw %&gt;% mutate(Replicate = as.factor(`Batch no. neutrophils`)) Let’s look at the data, and identify the biological replicates, as suggested in the original publication on Superplot by (Lord et al., 2021). In this example a color code is used to label the replicates: ggplot(data=df_raw, aes(x=Stage)) + geom_jitter(data=df_raw, aes(x=Stage, y=`Calcium (uM)`, color=Replicate)) To display the statistics for the individual biological replicates, we define a new dataframe. To this end, we group the data for the different stages and biological replicates: df_summary &lt;- df_raw %&gt;% group_by(Stage, Replicate) %&gt;% summarise(n=n(), mean=mean(`Calcium (uM)`)) `summarise()` has grouped output by &#39;Stage&#39;. You can override using the `.groups` argument. Next, we use ‘df_summary’ which holds the averages of each biological replicate, and we calculate the statistics for the different conditions: df_summary_replicas &lt;- df_summary %&gt;% group_by(Stage) %&gt;% mutate(n_rep=n(), mean_rep=mean(mean), sd_rep = sd(mean)) %&gt;% mutate(sem = sd_rep / sqrt(n_rep - 1), `95%CI_lo` = mean_rep + qt((1-Confidence_level)/2, n_rep - 1) * sem, `95%CI_hi` = mean_rep - qt((1-Confidence_level)/2, n_rep - 1) * sem, NULL) The dataframe has the summary of the conditions and note that each condition has a summary of 4 biological replicates: head(df_summary_replicas) # A tibble: 6 x 10 # Groups: Stage [2] Stage Replicate n mean n_rep mean_rep sd_rep sem `95%CI_lo` &lt;chr&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 before 1 38 0.0305 4 0.0301 0.00414 0.00239 0.0225 2 before 2 67 0.0270 4 0.0301 0.00414 0.00239 0.0225 3 before 3 56 0.0358 4 0.0301 0.00414 0.00239 0.0225 4 before 4 55 0.0270 4 0.0301 0.00414 0.00239 0.0225 5 crawling 1 7 0.0296 4 0.0317 0.00339 0.00196 0.0255 6 crawling 2 29 0.0289 4 0.0317 0.00339 0.00196 0.0255 # … with 1 more variable: 95%CI_hi &lt;dbl&gt; We can now add or ‘bind’ the data of ‘df_summary_replicas’ to the original dataframe ‘df’ and store this as a dataframe ‘df_2’: df_2 &lt;- df_raw %&gt;% left_join(df_summary_replicas, by = c(&quot;Stage&quot;,&quot;Replicate&quot;)) Let’s save this data: df_2 %&gt;% write.csv(&quot;protocol_2.csv&quot;, row.names=FALSE) Let’s first define a basic plot with all of the data for each stage shown as a violinplot: p &lt;- ggplot(data=df_2, aes(x=Stage)) + geom_violin(data=df_2, aes(x=Stage, y=`Calcium (uM)`), color=NA, fill=&quot;grey80&quot;) This is what it looks like: p We add the 95% confidence interval from the summary of the biological replicates as a line: p &lt;- p + geom_linerange(data = df_2, aes(ymin=`95%CI_lo`, ymax=`95%CI_hi`), size=1, alpha=0.8) And we add the mean value of each replicate as a dot. Here, the size of the dot is reflecting n: p &lt;- p + geom_point(data=df_2, aes(x=Stage, y=mean, size=n, fill=Replicate), shape=21, stroke = 1) The function scale_size_area() ensures that 0 is represented as an area of 0 and allows to to define that an n of 10,50 and 90 is shown in the legend: p &lt;- p + scale_size_area(breaks = c(10, 50, 90), max_size = 6) This is what that looks like: p Next, one of my favorite tweaks for discrete conditions is to rotate the plot 90 degrees. At the same time, the limits are defined. p &lt;- p + coord_flip(ylim = c(0.02,0.09)) + # This ensures correct order of conditions when plot is rotated 90 degrees scale_x_discrete(limits = rev) Rotation improves readability of the labels for the conditions, even when they are long. It also easier to read the different calcium levels: p To guide the interpretation, a line is added as a threshold of 0.06 µM (=60 nM): p &lt;- p + geom_hline(yintercept = 0.060, linetype=&#39;dotted&#39;) Adjusting the axis labels and adding a title and caption: p &lt;- p + labs( title = &quot;Calcium concentrations are less than 60 nM&quot;, subtitle = &quot;at different stages of transendothelial migration&quot;, x = &quot;Stage&quot;, y = &quot;Calcium [µM]&quot;, caption = &quot;(based on data from van der Linden, DOI: 10.1101/2021.06.21.449214)&quot;, tag = &quot;Protocol 2&quot; ) The layout it further optimized. The most tricky part is positioning of the label for the different conditions. It is placed right above the conditions, which I really like. However, getting this right involves a bit of trial and error and I recommend playing with the parameters to see how it affects the positioning. Something similar applies to the legend which is moved into the lower right corner of the plot, although this is eassier to accomplish. The comments explain the effect of the different lines: p &lt;- #Set text size p + theme_classic(base_size = 16) + theme( plot.caption = element_text( color = &quot;grey80&quot;, hjust = 1 ), #Set position of legend to lower right corner legend.position = c(0.88,0.15), #This line positions the label (&#39;title&#39;) of the conditions axis.title.y = element_text(vjust = 0.98, angle = 0, margin=margin(l=70)), #This line positions the names of the conditions #A negative margin is needed for aligning the y-axis &#39;title&#39; with the &#39;text&#39; axis.text.y = element_text(vjust = 0.5, hjust=1, angle = 0, margin=margin(l=-90, r=5)), #Move &#39;tag&#39;, so its position partially overlaps with the conditions plot.tag.position = c(0.06,0.99) ) + guides(fill = &quot;none&quot;, size = guide_legend(title = &#39;n per replicate&#39;, label.position = &quot;left&quot;) ) p To save the plot as a PNG file: png(file=paste0(&quot;Protocol_2.png&quot;), width = 4000, height = 3000, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 4.3 Protocol 3 - small multiples of time courses This protocol displays a number of different timecourses as ‘small multiples’. Small multiples, as the name suggests, displays many small plot separately as a stamp collection. By stressing the data, rather than the labels and grids, this can be a powerful visualization strategy. The data is taken from a publication by Arts et al. (2021) and we recreate figure panel 1F. The original figure is in small multiple format, but we tweak it a bit more to increase the focus on the data. Let’s first load the necessary package: library(tidyverse) The data comes from an excel file: df_raw &lt;- readxl::read_excel(&quot;Data_Arts_Circularity.xlsx&quot;) head(df_raw) # A tibble: 6 x 13 time `neutro 1` `neutro 2` `neutro 3` `neutro 4` `neutro 5` `neutro 6` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 0.53 0.54 0.55 0.59 0.58 0.34 2 10 0.44 0.6 0.5 0.4 0.54 0.4 3 20 0.33 0.55 0.64 0.3 0.48 0.41 4 30 0.35 0.54 0.69 0.28 0.52 0.32 5 40 0.41 0.53 0.57 0.23 0.44 0.29 6 50 0.32 0.4 0.4 0.26 0.43 0.25 # … with 6 more variables: neutro 7 &lt;dbl&gt;, neutro 8 &lt;dbl&gt;, neutro 9 &lt;dbl&gt;, # neutro 10 &lt;dbl&gt;, neutro 11 &lt;dbl&gt;, neutro 12 &lt;dbl&gt; It is in a wide format, so we need to make it tidy. The parameter that was measured over time is the ‘roundness’ of cells: df_3 &lt;- pivot_longer( df_raw, cols = -time, names_to = &quot;Cell&quot;, values_to = &quot;Roundness&quot; ) The data is in the right shape now, so let’s save it: df_3 %&gt;% write.csv(&quot;protocol_3.csv&quot;, row.names = FALSE) First we create a line plot of all the data: p &lt;- ggplot(df_3, aes(x=time, y=Roundness, group=Cell)) + geom_line() p With the facet_wrap() function, we turn this into a small multiple: p &lt;- p + facet_wrap(~Cell) p Set the limits of the axis and force the y-axis to start at 0 p &lt;- p + scale_y_continuous(expand = c(0, 0), limits = c(0, 1.0)) + scale_x_continuous(expand = c(0,0), limits = c(0, 300)) Use a minimal theme and remove the strips and grid to increase focus on the data: p &lt;- p + theme_minimal(base_size = 14) p &lt;- p + theme(strip.background = element_blank(), strip.text = element_blank(), plot.caption = element_text(color = &quot;grey80&quot;), #Remove grid panel.grid.major = element_blank(), panel.grid.minor = element_blank() ) p I do not like the repeated axis for the different plots. We can remove those: p &lt;- p + theme( #Remove axis labels axis.text = element_blank() ) p This is a very minimal plot, focusing entirely on the data. It may work well, but it is informative to add some information about the scaling of the x- and y-axis. To achieve this, I add lines to the lower left plot, which correspond to the data of ‘neutro 6’ (you can see this in the small multiple plot where each plot was labeled). I define a new dataframe with the x- and y-scale for ‘neutro 6’ to do just that: ann_line&lt;-data.frame(xmin=0,xmax=300,ymin=0,ymax=1, Cell=factor(&quot;neutro 6&quot;,levels=c(&quot;neutro 6&quot;))) ann_line xmin xmax ymin ymax Cell 1 0 300 0 1 neutro 6 This dataframe can now be used to draw two lines with geom_segment(): p &lt;- p + #Vertical line geom_segment(data=ann_line, aes(x=xmin,xend=xmin,y=ymin,yend=ymax), size=2, color=&#39;grey40&#39;) + #Horizontal line geom_segment(data=ann_line, aes(x=xmin,xend=xmax,y=ymin,yend=ymin), size=2, color=&#39;grey40&#39;) + NULL p The plot is now in black and white which gives it a strong contrast. We can make it a bit more soft and pleasant to look at by changing to shades of grey. Also, the labels of the axes are moved next to the lines: p &lt;- p + theme(panel.background = element_rect(fill=&#39;grey98&#39;, color=NA), panel.border = element_rect(color=&#39;grey90&#39;, fill=NA), axis.title.x = element_text(size=14, hjust = 0, color=&#39;grey40&#39;), axis.title.y = element_text(size=14, vjust = 0, hjust=0, angle = 90, color=&#39;grey40&#39;), ) Finally, we add a title, caption, and labels (and a scale in brackets): p &lt;- p + labs( title = &quot;Changes in the shape of migrating cells over time&quot;, x = &quot;Time [300s]&quot;, y = &quot;Circularity [0-1]&quot;, caption = &quot;(based on data from Arts et al., DOI: 10.3389/fimmu.2021.667213)&quot;, tag = &quot;Protocol 3&quot; ) p png(file=paste0(&quot;Protocol_3.png&quot;), width = 4000, height = 3200, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 4.4 Protocol 4 - Plotting data from a 96-wells experiment This protocol showcases some serious data wrangling and tidying. One reasons is that the data is acquired with a 96-wells plate reader and the data is stored according to the layout of the plate. This makes total sense from a human perspective, but it is not well suited for data visualization. In addition, the plate is measured twice. One measurement is the luminescence from a luciferase and the other measurement is the luminescence from Renilla. The latter reading serves as a reference and therefore, the luciferase data is divided by the Renilla intensities. A final step before the data is visualized is a normalization to a control condition. The code that is shown here is also the basis for the plotXpress app that can be used to process and visualize the data. In fact, the data visualization is very close to the standard output of plotXpress and uses the same example data. We start by loading a package that we need: library(tidyverse) The measured data is read from an excel sheet. Note that this is the raw data that is stored by the software that operates the plate reader: df_raw &lt;- readxl::read_excel(&quot;DualLuc_example_data.xlsx&quot;, sheet = &quot;Results&quot;) New names: * `` -&gt; ...1 * `` -&gt; ...2 * `` -&gt; ...3 * `` -&gt; ...5 * `` -&gt; ...6 * ... The experimental conditions for each well are stored in a separate CSV file, generated by the experimentalist that did the experiment: df_design &lt;- read.csv(&quot;Tidy_design.csv&quot;) head(df_design) Wells condition treatment1 treatment2 1 A01 - HEK - 2 B01 - HEK - 3 C01 - HEK - 4 D01 - HEK - 5 E01 - neuron - 6 F01 - neuron - You can see that the design file is tidy. In contrast the excel file with data is far from tidy. In the excel sheet, two ‘rectangles’ of cells define the data for the firefly &amp; renilla reads. The data is subset and converted to a vector firefly &lt;- df_raw[19:26,6:17] %&gt;% unlist(use.names = FALSE) renilla &lt;- df_raw[40:47,6:17] %&gt;% unlist(use.names = FALSE) Define a dataframe with wells column &lt;- rep(1:12, each=8) row &lt;- rep(LETTERS[1:8],12) For convenience, all numbers should consist of 2 digits and so we add a zero that precedes the single digit numbers: x0 &lt;- str_pad(column, 2, pad = &quot;0&quot;) To generate a unique index for each row in the dataframe, we define ‘Wells’, which combines the row and column index: Wells &lt;- paste0(row,x0) Next, we create a dataframe that holds the data of individual columns, rows and wells: df_plate &lt;- data.frame(column,row,Wells) head(df_plate) column row Wells 1 1 A A01 2 1 B B01 3 1 C C01 4 1 D D01 5 1 E E01 6 1 F F01 Add to df_plate the vectors with data from firefly and renilla reads df_4 &lt;- data.frame(df_plate,firefly,renilla) Merge the design with the data, based on the well ID - left_join() is used to add only data for wells that are listed in the design dataframe df_4 &lt;- left_join(df_design, df_4, by=&#39;Wells&#39;) head(df_4) Wells condition treatment1 treatment2 column row firefly renilla 1 A01 - HEK - 1 A 2010 2391540 2 B01 - HEK - 1 B 3210 2391639 3 C01 - HEK - 1 C 1965 2390991 4 D01 - HEK - 1 D 2381 2391774 5 E01 - neuron - 1 E 1292 269021 6 F01 - neuron - 1 F 991 268918 Calculate the relative expression from the firefly/renilla ratio df_4 &lt;- df_4 %&gt;% mutate(expression=firefly/renilla) Take all control conditions and calculate the average value df_norm &lt;- df_4 %&gt;% filter(condition == &quot;-&quot;) %&gt;% group_by(treatment1,treatment2) %&gt;% summarise(mean=mean(expression)) `summarise()` has grouped output by &#39;treatment1&#39;. You can override using the `.groups` argument. Combine the mean values (needed for normalization) values with the df_4 dataframe df_4 &lt;- df_4 %&gt;% full_join(df_norm, by=c(&quot;treatment1&quot;,&quot;treatment2&quot;)) %&gt;% na.omit(condition) Calculate the Fold Change by normalizing all measurements against the control (-) df_4 &lt;- df_4 %&gt;% mutate(`Fold Change` = expression/mean) The result is a dataframe that holds all the necessary data: head(df_4) Wells condition treatment1 treatment2 column row firefly renilla expression 1 A01 - HEK - 1 A 2010 2391540 0.0008404626 2 B01 - HEK - 1 B 3210 2391639 0.0013421758 3 C01 - HEK - 1 C 1965 2390991 0.0008218350 4 D01 - HEK - 1 D 2381 2391774 0.0009954954 5 E01 - neuron - 1 E 1292 269021 0.0048025991 6 F01 - neuron - 1 F 991 268918 0.0036851382 mean Fold Change 1 0.0009999922 0.8404692 2 0.0009999922 1.3421863 3 0.0009999922 0.8218414 4 0.0009999922 0.9955032 5 0.0040858977 1.1754085 6 0.0040858977 0.9019164 The data is in the right shape now, so let’s save it: df_4 %&gt;% write.csv(&quot;protocol_4.csv&quot;, row.names = FALSE) Based on the dataframe, we can create a plot with jittered dots that show the data: p &lt;- ggplot(df_4, aes(x=condition, y=`Fold Change`)) + geom_jitter(width = 0.2, alpha=0.5, size=3) To splot the graphs based on treatment1 (vertical) and treatment2 (horizontal) we use the facet_grid() function: p &lt;- p + facet_grid(treatment1~treatment2) We add a horizontal line for mean value: p &lt;- p + stat_summary(fun.min=mean, fun.max=mean, geom=&#39;errorbar&#39;, width=0.6, size=0.5) Add labels: p &lt;- p + labs( title = &quot;Effect of DNA sequences on reporter levels under different conditions&quot;, subtitle = &quot;The expression level was determined by a dual luciferase assay\\n and the values were normalized to a control with no DNA sequence (-)&quot;, x = &quot;DNA Sequence&quot;, y = &quot;Fold change of the reporter relative to the control (-)&quot;, caption = &quot;(based on data from Brandorff et al., DOI: 10.1101/2021.07.08.451595)&quot;, tag = &quot;Protocol 4&quot; ) Set the theme and font size: p &lt;- p + theme_light(base_size = 14) Format the facet labels (strips) and the caption + subtitle p &lt;- p + theme(strip.background = element_rect(fill=&quot;grey90&quot;, color=&quot;grey50&quot;), strip.text = element_text(color=&quot;grey50&quot;), plot.caption = element_text(color = &quot;grey80&quot;), plot.subtitle = element_text(color = &quot;grey50&quot;, face = &quot;italic&quot;), #Remove the grid panel.grid.major = element_blank(), panel.grid.minor = element_blank() ) Let’s look at the result: p To save the plot as a png file: png(file=paste0(&quot;Protocol_4.png&quot;), width = 4000, height = 3200, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 4.5 Protocol 5 - A map of amino acids This data visualization plots the position of amino acids in a given protein. It is inspired by figure 2A of the paper by Basu et al. (2020) Let’s load the tidyverse package: library(tidyverse) First, we define a vector with the 20 amino acids and the order in which we plot them. The amino acids are grouped as hydrophobic (G,A,V,C,P,I,L,M,F,W), hydrophilic (S,T,Y,N,Q), acidic (D,E) and basic (R,H,K). amino_acid_ordered &lt;- strsplit(&quot;GAVCPILMFWSTYNQDERHK&quot;,&quot;&quot;) %&gt;% unlist() amino_acid_ordered [1] &quot;G&quot; &quot;A&quot; &quot;V&quot; &quot;C&quot; &quot;P&quot; &quot;I&quot; &quot;L&quot; &quot;M&quot; &quot;F&quot; &quot;W&quot; &quot;S&quot; &quot;T&quot; &quot;Y&quot; &quot;N&quot; &quot;Q&quot; &quot;D&quot; &quot;E&quot; &quot;R&quot; &quot;H&quot; [20] &quot;K&quot; The protein sequence that we will use is the Homo sapiens Homeobox protein Hox-D13: protein &lt;- c(&quot;MSRAGSWDMDGLRADGGGAGGAPASSSSSSVAAAAASGQCRGFLSAPVFAGTHSGRAAAA AAAAAAAAAAASGFAYPGTSERTGSSSSSSSSAVVAARPEAPPAKECPAPTPAAAAAAPP SAPALGYGYHFGNGYYSCRMSHGVGLQQNALKSSPHASLGGFPVEKYMDVSGLASSSVPA NEVPARAKEVSFYQGYTSPYQHVPGYIDMVSTFGSGEPRHEAYISMEGYQSWTLANGWNS QVYCTKDQPQGSHFWKSSFPGDVALNQPDMCVYRRGRKKRVPYTKLQLKELENEYAINKF INKDKRRRISAATNLSERQVTIWFQNRRVKDKKIVSKLKDTVS&quot;) The protein sequence may contain end-of-line characters “” after copy pasting and we need to remove these: protein &lt;- gsub(&quot;\\n&quot;, &quot;&quot;, protein) Next, the protein sequence is split into single characters and we assign this vector to aa: aa &lt;- strsplit(protein, &quot;&quot;) %&gt;% unlist() We generate a dataframe with a column with the amino acids and a column that defines their position: df_5 &lt;- data.frame(aa=aa, position=1:length(aa)) Now we reorder the data frame to the order of the amino acids that we defined earlier in the vector amino_acid_ordered: df_5 &lt;- df_5 %&gt;% mutate(aa = fct_relevel(aa, amino_acid_ordered)) The basic plot shows a black tile for each amino acid. Note that the y-axis order is defined by the vector amino_acid_ordered, but it needs to be reverted to order the amino acids from top to bottom along the y-axis (which is naturally starts at the bottom it corresponds to the origin). The data is in the right shape now, so let’s save it: df_5 %&gt;% write.csv(&quot;protocol_5.csv&quot;, row.names = FALSE) p &lt;- ggplot() + geom_tile(data=df_5, aes(x=position, y=aa)) + scale_y_discrete(limits = rev(amino_acid_ordered)) p Set the theme to classic, to get rid off the ‘frame’ around the plot and the grid. p &lt;- p+theme_classic(base_size = 16) For each of the four classes of amino acids we can define a box with a color that indicates the class. For example, there are three basic residues that will have a rectangle filled with blue in the background. The amino acids are factors, but we need numbers to define the coordinates for the rectangle. In a plot with a factors (here on the y-axis) their position is defined by a (non visible) natural number. Therefore we can define a box with the function annotate() for the first residue with y-coordinates ymin=0.5 and ymax=1.5: p + annotate(geom = &quot;rect&quot;, xmin = -Inf, ymin = 0.5, xmax = Inf, ymax=1.5, fill=&#39;blue&#39;, alpha=0.4) In this way, we define four colored rectangles that reflect the different amino acids categories; blue=basic, red=acidic, yellow=hydrophilic, grey=hydrophobic: p &lt;- p + annotate(geom = &quot;rect&quot;, xmin = -Inf, ymin = 0.5, xmax = Inf, ymax=3.5, fill=&#39;blue&#39;, alpha=0.15) p &lt;- p + annotate(geom = &quot;rect&quot;, xmin = -Inf, ymin = 3.5, xmax = Inf, ymax=5.5, fill=&#39;red&#39;, alpha=0.15) p &lt;- p + annotate(geom = &quot;rect&quot;, xmin = -Inf, ymin = 5.5, xmax = Inf, ymax=10.5, fill=&#39;yellow&#39;, alpha=0.15) p &lt;- p + annotate(geom = &quot;rect&quot;, xmin = -Inf, ymin = 10.5, xmax = Inf, ymax=20.5, fill=&#39;black&#39;, alpha=0.15) Let’s look at the result: p Adjusting the axis labels and adding a title and caption: p &lt;- p + labs( title = &quot;Mapping the amino acid positions of HOXD13&quot;, subtitle = &quot;shows a high abundance of alanines in the IDR&quot;, y = &quot;Amino acid&quot;, caption = &quot;(based on data from Basu et al, DOI: 10.1016/j.cell.2020.04.018)&quot;, tag = &quot;Protocol 5&quot; ) And a final tweak of the label style and location: p &lt;- p + theme( plot.caption = element_text( color = &quot;grey80&quot;, hjust = 1 )) In the original paper, a region of the protein is annotated as an ‘intrinsically disordered region’ abbreviated as IDR. Here, we we use the annotate() function to add a rectangle and a label to the top of the plot: p &lt;- p + annotate(&quot;rect&quot;, xmin=0, xmax=118, ymin=21, ymax=22, fill=&#39;darkblue&#39;) + annotate(&quot;text&quot;, x=59, y=23, alpha=1, color=&#39;darkblue&#39;, size=4,label=&#39;IDR&#39;) p To avoid clipping of the new label by the subtitle of the plot: p &lt;- p + coord_cartesian(clip = &#39;off&#39;) p The subtitle is quite close to the IDR label. Let’s give the subtitle a bit more room, by adding a margin at the bottom of the subtitle. This can be done with the theme() function to style the subtitle: p &lt;- p + theme(plot.subtitle = element_text(margin = margin(b=20))) p Finally we can save the plot: png(file=paste0(&quot;Protocol_5.png&quot;), width = 4000, height = 3200, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 In progress This chapter is ‘in progress’ and I plan to add more protocols in the future. "],["questions-and-answers.html", "Chapter 5 Questions and Answers", " Chapter 5 Questions and Answers A list of questions that may arise during the use of R and their answers. When you have questions that remain unanswered, feel free to reach out! What is the difference between a data.frame and a tibble? Both are objects that hold data. The data.frame is the structure supported by basic R and the tibble is an extended variant that is supported by the tidyverse package. Dependig on the function that is used to load data, read.csv() from base R or read_csv() from the tidyverse, the data will be loaded in a data.frame or tibble, respectively. It is possible to convert a classic data.frame to a tibble (which requires the tidyverse pakcage): df &lt;- as_tibble(mtcars) class(df) [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; And to convert a tibble to a data.frame: df &lt;- as.data.frame(df) class(df) [1] &quot;data.frame&quot; What’s up with spaces in variable names and in files names? Spaces are used to separate words (as in this line). When a space would be used between two words, these would be treated as separate entities. Avoiding spaces in filenames has a more technical background. Most filesystems currently accept spaces, but some systems do not. To increase compatibility across systems it is a good habit to use_underscores_instead. What are the rules for naming dataframes or variables? Do not use spaces, stick to characters and numbers. Whenever a variable consists of multiple words, e.g. room temperature there are two options: Add an underscore as separator: room_temperature. Use the ‘camelCase’ notation: roomTemperature. Personally, I prefer an underscore and the abbreviation df for dataframe when multiple dataframes are generated, e.g. df_tidy or df_summary What is the difference between &lt;- and = for assigning variables? R prefers the &lt;- for the assignment of a value to a variable. In RStudio the shortcut is &lt;alt&gt;+&lt;-&gt;. It really is a preference since both can be used: x &lt;- 1 y = 2 x + y [1] 3 What does ‘NA’ mean in a dataframe? When no data is available, this is known as a ‘missing value’. In R this is indicated with NA for ‘Not Available’. Empty cells in a CSV file will be converted to NA when the file is loaded. Other characters that can be present in a file (especially xls files) are: “.” or “NaN” or “#N/A” or “#VALUE!”. To convert these strings to NA use: df &lt;- read.csv(&#39;FPbase_Spectra.csv&#39;, na.strings=c(&quot;.&quot;, &quot;NaN&quot;, &quot;#N/A&quot;, &quot;#VALUE!&quot;)) What is the beste way to type a ‘%&gt;%’ operator? I prefer to literally type it. The shortcut that works in Rstudio is &lt;shift&gt;+&lt;command&gt;+&lt;M&gt; or &lt;shift&gt;+&lt;control&gt;+&lt;M&gt;. Where do I find the example data? The example data that is used in Chapter 2 and 3 is located on this github page: https://github.com/JoachimGoedhart/DataViz-protocols The data that is used for the protocols in Chapter 4 is located in the subdirectory /Protocols. Is there a way to re-use or adjust the protocols? Instead of copy-pasting and running the code line-by-line, you can download the R Markdown file (.Rmd) from the protocols folder on the Github repository: https://github.com/JoachimGoedhart/DataViz-protocols. This file (together with the data) can be used to replicate the protocol and to modify it. For more info on R Markdown, see https://rmarkdown.rstudio.com "]]
