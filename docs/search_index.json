[["index.html", "DataViz protocols An introduction to data visualization protocols for wet lab scientists Preface", " DataViz protocols An introduction to data visualization protocols for wet lab scientists Joachim Goedhart 2025-04-04 Preface Experiments rely on step-by-step instructions that are detailed in protocols. These protocols, which are used in a wet lab, are similar to the instructions that are defined in scripts for data visualization. Although scientists are familiar with protocols for experiments, they are usually less familiar with code or scripts for handling experimental data. Given the similarities between experimental methods and computer instructions, it should be within reach for experimental scientists to add automated, reproducible data processing and visualization to their toolkit. This book aims to lower the barrier for wet lab scientists to use R and ggplot2 for data visualization. First, by explaining some basic principles in data processing and visualization. Second, by providing example protocols, which can be applied to your own data, and I hope that the protocols serve as inspiration and a starting point for new and improved protocols. Data visualization is the process of transforming information into a picture. The picture that reflects the information, helps humans to understand and interpret the data. As such, data visualization is an important step in the analysis of experimental data and it is key for interpretation of results. Moreover, proper data visualization is important for the communication about experiments and research in presentations and publications. Data visualization usually requires refinement of the data, e.g. reshaping or processing. Therefore, the translation of data into a visualization is a multistep process. This process can be automated by defining the steps in a script for a software application. A script is a set of instructions in a language that both humans and computers can understand. Using a script can make data analysis and visualization faster, robust against errors and reproducible. As it becomes easier and cheaper to gather data, it becomes more important to use automated analyses. Finally, scripts make the processing transparent when the scripts are shared or published. R is a very popular programming language for all things related to data. It is freely available, open-source and there is a large community of active users. In addition, it fulfills a need for reproducible, automated data analysis. And lastly, with the ggplot2 extension, it is possible to generate state-of-the-art data visualizations. There are many great resources out there (that is also the reason I came this far) and below I list the specifics of this resource. First, there is a strict focus on R. All examples use R for all steps. Second, the datasets that are used are realistic and represent data that you may have. Several datasets that are used come from actual experimental data gathered in a wet lab. By using real data, specific issues that may not be treated elsewhere are encountered, discussed and solved. One of the reasons is that R requires a specific data format (detailed in chapter 2 Reading and Reshaping data) before the data can be visualized. It is key to understand how experimental data should be processed and prepared in a way that it can be analyzed and visualized. As the required format is usually unfamiliar to wet lab scientists, I provide several examples of how to do this. Third, since details determine successful use of R, I will go into detail whenever necessary. Examples of details include the use of spaces in column names, reading files with missing values, or optimizing the position of a label in a data visualization. Finally, modern analysis and visualization methods are treated and since the book is in a digital, online format it will be adjusted when new methods are introduced. An example of a recently introduced data visualization is the Superplot, which is the result of Protocol 2. Part of this work has been published as blogs on The Node and the enthusiastic response encouraged me to create a more structured and complete resource. This does not at all imply that this document needs to be read in a structured manner. If you are totally new to R it makes sense to first read the chapter Getting started with R which treats some of the essential basics. On the other hand, if you are familiar with R, you may be interested in the chapters on Reading and Reshaping data or Visualizing data. Finally, masters in R/ggplot2 may jump right to the Complete protocols. This final part brings all the ingredients of the preceding chapters together. Each protocol starts with raw data and shows all the steps that lead to a publication quality plot. I hope that you’ll find this book useful and that it may provide a solid foundation for anyone that wants to use R for the analysis and visualization of scientific data that comes from a wetlab. I look forward to seeing the results on twitter Bluesky, Mastodon, in meetings, in preprints or in peer reviewed publications. A toast Cheers to all the kind people that helped me to get started with R, answered my questions, provided feedback on code and data visualizations, and helped me to troubleshoot scripts. Also thanks to all co-workers for sharing data and the helpful discussions. Finally, twitter (not to be confused with “X”) was a huge source of inspiration, a magnificent playground, and an ideal place to meet people, discuss, get feedback or just hang out and I thank anyone I interact(ed) with! I hope to continue this on Bluesky. This work work was improved by specific comments from; Daniel C. de la Fuente (@DanCF93) How to cite Goedhart, J. (2022) DataViz protocols - An introduction to data visualization protocols for wet lab scientists, doi: 10.5281/zenodo.7257808 Sharing To share this resource on Bluesky click here "],["getting-started.html", "Chapter 1 Getting started with R 1.1 Running R 1.2 Using the command line 1.3 ?Help 1.4 Installing packages 1.5 Multiline code", " Chapter 1 Getting started with R There is a lot of great material out there to get you started with R. I enjoyed swirl, which teaches basic R, in R. Whatever you choose, it is probably a good idea to familiarize yourself to some extent with basic R. Below, I only treat some of the basic stuff that is needed to run the code that is presented in the other chapters. 1.1 Running R R is free and it is available for many different platforms. You can run plain R, run it from RStudio Desktop or even from a browser. I prefer Rstudio and the instructions are based on Rstudio as well. When you start Rstudio, you see a couple of windows, including a ‘Console’. This is the place where you can type your commands for R. 1.2 Using the command line A classic in coding language is to print ‘hello world’. To do this, you need to type this piece of code (followed by the enter key) in the console: print('hello world') The result, printed below the code in the console is: [1] \"hello world\" In this document the input and output is shown as grey boxes. The first grey box represents the input and, if there is any output, the second grey box shows the output, for example: print(&#39;hello world&#39;) [1] &quot;hello world&quot; It is possible to copy the code from the first box. When you move your cursor to the upper left corner of the box a copy icon will appear. If you click on the icon, the code is copied. Not every piece of code results in a visible output, for instance when I assign a value to the variable x: x &lt;- 1 To show the value of a variable as output, type its name: x [1] 1 R comes with datasets. Although these datasets are not so relevant for us, they are often used to demonstrate functions in R. One of these datasets is mtcars and we can use the head() function to check the first lines: head(mtcars) mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 1.3 ?Help If you need help with a specific function, you may look at the documentation by typing a question mark followed by the function: ?head() However, I usually find myself googling instead of reading the documentation. The result is often a website where the correct use is explained by an example. It also make sense to experiment. Systematically changing variables or options (or outcommenting lines of code, see below) will teach you the logic behind a function or operation. Finally, there may be some answers in the chapter Questions and Answers 1.4 Installing packages R comes with a lot of commands and functions, but we often need to load additional ‘packages’ to add functionality. The most important one that we use here is the {ggplot2} package that we use for the data visualizations. The package can be activated with the function require() or library(): library(&quot;ggplot2&quot;) The {tidyverse} package is very versatile and is a superpackage that hold several packages (including {ggplot2}). Loading the {tidyverse} package is like equipping R with superpowers. Often it is sufficient to load the {tidyverse} package. The first time, the package has to be downloaded and installed. This handy piece of code checks whether the {tidyverse} package is available, downloads it if necessary and than activates it: if (!require(tidyverse)) { install.packages(&quot;tidyverse&quot;) require(tidyverse) } The output in the console depends on the packages that are installed and activated. For some specific functions, other packages are required, and these will be mentioned whenever they are used. 1.5 Multiline code The tidyverse package introduces a so-called pipe operator %&gt;% which we will use a lot. This operator is useful for stringing multiple functions together. An example is given below, which reads as ‘take the mtcars dataset and next use the head() function’. mtcars %&gt;% head() mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Also for the plots that are generated with ggplot(), several functions can be added and these reflects different layers in the plot. However, in case of ggplot the layers are combined by a +: ggplot(mtcars, aes(wt, mpg)) + geom_point() + geom_line() It is important to end each line with a +, as it indicates that the code continues on the next line. This will result in a warning message: ggplot(mtcars, aes(wt, mpg)) + geom_point() + geom_line() The last line should not end with a plus. A trick that I use a lot, is using NULL on the last line of the code: ggplot(mtcars, aes(wt, mpg)) + geom_point() + geom_line() + NULL The advantage is that it is easy to deactivate a line by placing a hashtag in front of it (without the need to remove the +). The hashtag tells R to treat that line as a comment and not as code: ggplot(mtcars, aes(wt, mpg)) + geom_point() + # geom_line() + NULL This strategy is called ‘commenting out’ and is very useful to examine the effect of a line of code. For plots, it works very well in combination with NULL on the last line. "],["read-and-reshape.html", "Chapter 2 Reading and reshaping data 2.1 Introduction 2.2 Types of data 2.3 Reading data 2.4 Reshaping data", " Chapter 2 Reading and reshaping data 2.1 Introduction Experimental data can be recorded and stored in different ways. Anything that is not digital (e.g. notes in an physical labbook) has to be converted, before it can be used in a computer. Nowadays, most information is already in a digital format and stored in a file. This can be a text file, an excel file, or a file generated by a piece of equipment. In R, the main structure for storing and processing data is a ‘dataframe’ (a modernized version of the dataframe is a ‘tibble’, which is available when the tidyverse package is used). The dataset mtcars that comes with R is a dataframe. We can check the type of a structure or variable by using the function class(). This is a very helpful function to learn about the class of an object: class(mtcars) [1] &quot;data.frame&quot; The dataframe itself can be printed by just typing its name. To look only at the first rows we can use the function head(): head(mtcars) mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 When we read experimental data from a file, this implies that we transfer the information to a dataframe in R. A dataframe can hold several types of data. Let’s first look at what different types of data you may encounter. 2.2 Types of data All the information that is recorded during an experiment can be considered as data. Details of the experimental approach or setup are called metadata, and the data that is measured is the raw data. Data visualization primarily deals with the raw, measured data. Yet, adding data on the experimental design or other types of metadata can be important for the interpretation of the data. The raw data mostly consist of numbers, but these are often accompanied by labels for experimental conditions or to identify objects. All this information is data, but they are clearly of different types. Let’s have a look at the most common types of data. Measurements usually result in quantitative data consisting of numbers. For instance when an optical density is measured with a spectrophotometer or when temperature is determined with a thermometer. This kind of data is called quantitative and continuous data, since it can have any number ranging from minus to plus infinity. Another type of quantitive data is quantitative and discrete data and it consists only of natural numbers. An example is the number of colonies on an agar plate or the number of replicates. There is also data that cannot be expressed in numbers and we call that qualitative data. For instance different experimental categories, e.g. ‘control’ and ‘treated’. This is also known as nominal data (ordinal data is not treated as it is not used in this book). It is important to make the distinction between different types of data and tell R how to treat the data. In some cases a number stored in a file can be a category and it is important to treat the number as a category and not as quantitative data. We will see that R stores information about the type of data in the example below. The variable x to which we assigned a value of 1 is: x &lt;- 1 class(x) [1] &quot;numeric&quot; We can convert this to a ‘factor’, which means that it is no longer a number but qualitative data: x &lt;- as.factor(x) class(x) [1] &quot;factor&quot; And therefore, this will give an error: x + 1 [1] NA When we convert it back to a number, it works: as.numeric(x) + 1 [1] 2 This simple example illustrates the difference between quantitative and qualitative data and it shows that we can change the data type in R. 2.3 Reading data Reading, or loading data is the transfer of information from a file to the memory of R where it is stored as a dataframe. Rstudio supports ‘point-and-click’ loading of data from its menu (File &gt; Import Dataset &gt; …). This is a convenient way of loading data. Since the aim is to perform all steps in a script, including data loading, I’ll explain how functions are used to load data from the console. 2.3.1 Loading data from a text or csv file Before we can read the file, we need to make sure that we can locate the file. In RStudio you can select the directory (folder) from the menu: Session &gt; Set Working Directory &gt; Choose Directory… If you are running an R script and the data is in the same directory as the script, you can go to the menu of Rstudio and select: Session &gt; Set Working Directory &gt; To Source File Location When the directory is properly set, you can read the file. A common file format is the ‘comma separated values’ or CSV format. Here we load a CSV file that was obtained from fpbase.org and contains the excitation and emission data of Green Fluorescent Protein. This file and all other example data are available at github: https://github.com/JoachimGoedhart/DataViz-protocols The function read_csv() is used to read the file and the data is assigned to a dataframe called ‘df’: df &lt;- read.csv(&#39;FPbase_Spectra.csv&#39;) To check whether the loading was successful, we can look at the first lines of the dataframe with the function head(): head(df) Wavelength mEGFP.EM mEGFP.EX 1 300 NA 0.0962 2 301 NA 0.0872 3 302 NA 0.0801 4 303 NA 0.0739 5 304 NA 0.0675 6 305 NA 0.0612 This data has several columns, each containing quantitative data. Empty cells that do not have any data will be displayed as ‘NA’. Note that this is different from ‘0’. The ‘delimiter’ of a file is a character that separates the different fields of data. In a CSV file, as the name implies, this is a comma. However, other characters such as semicolons or tabs are often used as delimiters in text files. The function ‘read.delim()’ can be used to load a text file with a delimiter that can be specified, and therefore this function is more flexible for loading of data. For the CSV file, it would be used like this: df &lt;- read.delim(&#39;FPbase_Spectra.csv&#39;, sep = &quot;,&quot;) 2.3.2 Loading data from a URL When the CSV file is available online, e.g. in a data repository or on Github, it can be loaded by providng the URL: df &lt;- read.csv(&#39;https://zenodo.org/record/2545922/files/FRET-efficiency_mTq2.csv&#39;) head(df) EGFP mNeonG Clover mKOkappa mOrange2 mScarlet.I mRuby2 TagRFP.T 1 46.25865 61.27913 52.85122 47.28077 25.70513 36.03078 51.68192 12.39361 2 46.48604 60.97392 54.05560 48.42123 26.43752 34.93961 33.75286 12.59682 3 46.66348 61.27877 48.93475 44.93190 24.47023 25.38569 41.93491 14.03628 4 46.30399 60.80149 48.95284 45.47728 21.72633 28.27172 33.68123 12.59829 5 45.67780 59.89690 52.97067 44.03166 21.52321 34.00304 35.81131 20.86654 6 45.35124 62.76465 52.57957 47.08019 23.44953 34.64273 48.87000 23.78392 mCherry 1 31.31533 2 30.85531 3 30.66732 4 34.28993 5 35.64215 6 28.99762 2.3.3 Retrieving data from Excel Suppose we have an excel file with multiple tabs and we would like to access the data for mNeonGreen. To import the correct data into a dataframe use: df &lt;- readxl::read_excel(&#39;FPbase_Spectra.xlsx&#39;, sheet = &#39;mNeonGreen&#39;) head(df) # A tibble: 6 × 3 Wavelength `mNeonGreen EM` `mNeonGreen EX` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 300 NA NA 2 301 NA NA 3 302 NA NA 4 303 NA NA 5 304 NA NA 6 305 NA NA Again, ‘NA’ indicates that no data is available. Since the read_excel() is a funtion from the ‘tidyverse’ the data is stored in a tibble. This can be converted to an ordinary dataframe: df &lt;- as.data.frame(df) class(df) [1] &quot;data.frame&quot; Now that we have the data loaded, we can generate a plot. For simplicity I use the qplot() function. The more flexible ggplot() function will be introduced later. We need to supply the name of the dataframe, the column for the x-axis data and the column that is used for the y-axis. Note that two of the column names have a space and to properly indicate the name of the column we need to enclose the name with backtics (`). qplot(data=df,x=Wavelength, y=`mNeonGreen EX`) A plot with lines instead of dots can be made by supplying this alternative ‘geometry’: qplot(data=df,x=Wavelength, y=`mNeonGreen EX`, geom=&#39;line&#39;) The plots show the excitation spectrum of mNeonGreen, and it can be inferred that the protein is maximally excited near 500 nm. In its current shape, the dataframe is not suitable for simultaneously plotting the excitation and emission spectrum. To do that, we need to reshape the data and this will be the topic of the Reshaping data section. 2.3.4 Retrieving data from multiple files When the data is spread over multiple files, it is useful to read these files and combine them into one dataframe. In this example we have the data from three different conditions, each is an individual CSV file. First we create a list with the files based on a pattern. In this case the relevant files contain the string S1P.csv: filelist = list.files(pattern=&quot;*S1P.csv&quot;) filelist [1] &quot;Cdc42_S1P.csv&quot; &quot;Rac_S1P.csv&quot; &quot;Rho_S1P.csv&quot; Then we use the function map() to perform the function read.csv() for each of the files and we store the result in a new dataframe ‘df_input_list’. df_input_list &lt;- map(filelist, read.csv) The result is a ‘nested’ dataframe, which is a dataframe with dataframes. Instead of having three separate dataframes, we want a single dataframe but it should have a label that reflects the condition. The labels are based on the filenames. We can use str_replace() to remove the extension of the filenames: names(df_input_list) &lt;- str_replace(filelist, pattern = &quot;.csv&quot;, replacement = &quot;&quot;) After this, we merge the dataframes and create a column ‘id’ that has the label with the filename: df &lt;- bind_rows(df_input_list, .id = &quot;id&quot;) head(df) id Time Cell.1 Cell.2 Cell.3 Cell.4 Cell.5 1 Cdc42_S1P 0.0000000 1.0035170 1.0015490 0.9810209 0.9869040 1.0041990 2 Cdc42_S1P 0.1666667 0.9991689 0.9961631 0.9801265 0.9891608 0.9989283 3 Cdc42_S1P 0.3333333 1.0013450 1.0046440 1.0106020 1.0124910 0.9953477 4 Cdc42_S1P 0.5000000 1.0015790 1.0043180 1.0106890 1.0066810 1.0027020 5 Cdc42_S1P 0.6666667 0.9943522 0.9933341 1.0168220 1.0045540 0.9988462 6 Cdc42_S1P 0.8333333 1.0046240 1.0071780 1.0140210 1.0042580 0.9920673 Cell.6 Cell.7 Cell.8 Cell.9 Cell.10 Cell.11 Cell.12 1 1.0049370 1.0042000 1.0014210 1.0001080 1.000748 1.0015060 1.0057850 2 0.9892894 0.9903881 0.9975485 0.9937496 0.995352 0.9962637 0.9972004 3 1.0104820 1.0140110 1.0016740 1.0003500 1.003132 1.0015520 0.9942397 4 1.0031760 1.0016000 1.0000720 1.0035080 1.000595 1.0017760 1.0035220 5 0.9921757 0.9898673 0.9993021 1.0023010 1.000192 0.9989264 0.9993660 6 1.0081460 1.0075700 0.9985681 1.0013800 1.001741 0.9989417 1.0028090 Cell.13 Cell.14 Cell.15 Cell.16 Cell.17 Cell.18 Cell.19 Cell.20 1 1.0111880 1.0147230 1.0089390 1.0032410 1.0068720 1.0019290 1.0008660 NA 2 0.9894263 1.0044960 1.0026040 0.9978889 0.9970717 0.9993551 0.9980562 NA 3 0.9921575 1.0073610 1.0029890 1.0009640 0.9984493 0.9968905 0.9966830 NA 4 1.0102150 0.9910634 0.9943020 0.9942904 0.9932356 1.0059050 1.0043110 NA 5 0.9974161 0.9829283 0.9912844 1.0037130 1.0045560 0.9959676 1.0000940 NA 6 1.0070350 0.9790479 0.9861535 1.0059600 1.0065300 1.0070570 1.0049470 NA Cell.21 Cell.22 Cell.23 Cell.24 Cell.25 Cell.26 Cell.27 Cell.28 Cell.29 1 NA NA NA NA NA NA NA NA NA 2 NA NA NA NA NA NA NA NA NA 3 NA NA NA NA NA NA NA NA NA 4 NA NA NA NA NA NA NA NA NA 5 NA NA NA NA NA NA NA NA NA 6 NA NA NA NA NA NA NA NA NA Cell.30 Cell.31 Cell.32 1 NA NA NA 2 NA NA NA 3 NA NA NA 4 NA NA NA 5 NA NA NA 6 NA NA NA This dataframe contains all the relevant information, but it is not tidy yet. We’ll discuss how to convert this dataframe into a tidy format. We can save this dataframe for later: df %&gt;% write.csv(&#39;df_S1P_combined.csv&#39;, row.names=FALSE) 2.4 Reshaping data Data is often recorded in tables or spreadsheets. Columns are typically used for different conditions (indicated in a header) and each data cell contains a measured value. Although this format makes perfect sense for humans, it is less suitable for analysis and visualization in R. Instead of the tabular, or wide, format, the functions from the tidyverse package work with data in a ‘tidy’ format. The benefit of tidy data is that it is a consistent way to structure datasets, facilitating data manipulation and visualization Wickham, 2014. In other words, this format simplifies downstream processing and visualization. In this section, I will show how data can be converted from spreadsheet format to a long, tidy format. This step is needed to prepare the data for visualization with ggplot() which is also part of the tidyverse package. I will use the nomenclature that is used in the original publication by Hadley Wickham. Before we start, a quick warning that I have been struggling with the concept of tidy data. Probably, because I was very much used to collect, process and summarize data in spreadsheets. In addition, I am used to read and present data in a tabular format. It is important to realize that data in the tidy format contains exactly the same information as non-tidy, spreadsheet data, but it is structured in a different way. In fact we can switch back and forth between the two formats with functions that are provided by R. 2.4.1 Quantitative data, discrete conditions Let’s say that you have measured cell sizes under a number of different experimental conditions and this is stored in an excel spreadsheet. Let’s load the data: df &lt;- readxl::read_excel(&#39;Length-wide.xls&#39;) df # A tibble: 10 × 5 `Condition 1` `Condition 2` `Condition 3` `Condition 4` `Condition 5` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 14.4 19.2 16.6 21.9 18.7 2 16.2 13.7 13.9 20.2 13.2 3 11.2 13.2 13.2 22.7 18.1 4 15.2 20.1 13.7 19 18.2 5 11.7 21.3 15.9 19.5 15.9 6 17.5 19.9 13 14.3 28.7 7 17 20.8 10.8 14 29.6 8 18.7 18.4 14.9 17.5 13.3 9 19.1 20.7 14.6 16.5 13.9 10 10.7 19.2 14.2 17.8 13.4 First, we will replace the spaces in the column names. Although this is not strictly necessary, it simplifies handling of the data. We use names() to get the names of the dataframe and str_replace() to replace the space by an underscore: names(df) &lt;- str_replace(names(df), pattern = &quot; &quot;, replacement = &quot;_&quot;) head(df) # A tibble: 6 × 5 Condition_1 Condition_2 Condition_3 Condition_4 Condition_5 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 14.4 19.2 16.6 21.9 18.7 2 16.2 13.7 13.9 20.2 13.2 3 11.2 13.2 13.2 22.7 18.1 4 15.2 20.1 13.7 19 18.2 5 11.7 21.3 15.9 19.5 15.9 6 17.5 19.9 13 14.3 28.7 Now that the column names are fixed, we can restructure the data. The aim is to create a dataframe with one column that specifies the conditions and one column with all the measured values. There is a long history of packages and functions that can be used for restructuring (gather(), melt()). Here we use the most modern function pivot_longer() from the tidyverse package. We need to specify the dataframe, which columns to restructure (specified by everything() here) and the names of the new columns. The current column names will be transferred to a column that is named “condition” and all values will be transferred to a column named “size”. df_tidy &lt;- pivot_longer(df, cols = everything(), names_to = &quot;Condition&quot;, values_to = &quot;Size&quot;) head(df_tidy) # A tibble: 6 × 2 Condition Size &lt;chr&gt; &lt;dbl&gt; 1 Condition_1 14.4 2 Condition_2 19.2 3 Condition_3 16.6 4 Condition_4 21.9 5 Condition_5 18.7 6 Condition_1 16.2 The benefit of this format is that is is now clear what the numbers are. The most important requirement for tidy data is that each variable occupies only a single column and that each row is an observation. Let’s save the data in a csv file: df_tidy %&gt;% write.csv(&#39;Length-tidy.csv&#39;) 2.4.2 Multiple discrete conditions Here, we deal with a more complex spreadsheet that holds data of multiple replicates and two experimental conditions. Note that the data from multiple conditions can be stored in different ways and here we only treat one way. Especially for these kind of datasets, the tidy format is a better, cleaner structure. We will load the data from a repository: df_multiheader &lt;- read.csv(&#39;https://zenodo.org/record/4056966/files/Data-with-replicates.csv&#39;) head(df_multiheader) Control Control.1 Control.2 Drug Drug.1 Drug.2 1 Replicate1 Replicate2 Replicate3 Replicate1 Replicate2 Replicate3 2 43.69202 35.43517 27.69333 35.3156 20.54166 16.13286 3 41.85664 38.17644 35.61621 34.81943 20.43263 16.47575 4 49.11707 39.86308 27.20247 30.45615 29.8097 16.49928 5 49.79331 37.5157 39.98903 37.46084 25.94712 18.48844 6 41.54301 42.66665 26.92205 30.25243 22.90337 21.93457 The first row lists the experimental condition and the second row identifies biological replicates. Now, when this is loaded as an ordinary CSV, the first row is the header, but the second row is treated as data. Let’s load the data without flagging a header. We also add the stringsAsFactors = FALSE to make sure that the data is loaded as characters &lt;chr&gt; and not as factors &lt;fctr&gt;. The difference is not obvious, but we run into problems later when we want to convert all the values into actual numbers (which we will do this at the very last step): df_multiheader &lt;- read.csv(&quot;https://zenodo.org/record/4056966/files/Data-with-replicates.csv&quot;, header = FALSE, stringsAsFactors = FALSE) head(df_multiheader) V1 V2 V3 V4 V5 V6 1 Control Control Control Drug Drug Drug 2 Replicate1 Replicate2 Replicate3 Replicate1 Replicate2 Replicate3 3 43.69202 35.43517 27.69333 35.3156 20.54166 16.13286 4 41.85664 38.17644 35.61621 34.81943 20.43263 16.47575 5 49.11707 39.86308 27.20247 30.45615 29.8097 16.49928 6 49.79331 37.5157 39.98903 37.46084 25.94712 18.48844 We’ll load the first row as a vector that contains the name of each column. To this end we select the first row of the dataframe with the brackets [1,]. The result is a dataframe and to turn this into a vector with strings we use unlist(): first_row &lt;- df_multiheader[1,] %&gt;% unlist(use.names=FALSE) first_row [1] &quot;Control&quot; &quot;Control&quot; &quot;Control&quot; &quot;Drug&quot; &quot;Drug&quot; &quot;Drug&quot; We repeat this for the second row: second_row &lt;- df_multiheader[2,] %&gt;% unlist(use.names=FALSE) second_row [1] &quot;Replicate1&quot; &quot;Replicate2&quot; &quot;Replicate3&quot; &quot;Replicate1&quot; &quot;Replicate2&quot; [6] &quot;Replicate3&quot; Next, row 1 and row 2 are removed from the dataframe, keeping only the data: df &lt;- df_multiheader[-c(1:2),] head(df) V1 V2 V3 V4 V5 V6 3 43.69202 35.43517 27.69333 35.3156 20.54166 16.13286 4 41.85664 38.17644 35.61621 34.81943 20.43263 16.47575 5 49.11707 39.86308 27.20247 30.45615 29.8097 16.49928 6 49.79331 37.5157 39.98903 37.46084 25.94712 18.48844 7 41.54301 42.66665 26.92205 30.25243 22.90337 21.93457 8 44.04201 37.10115 18.24681 35.93469 20.10045 22.86391 The labels of the conditions and replicates are combined by pasting them together with an underscore to separate the labes. The result is a single vector with unique labels: combined_labels &lt;- paste(first_row, second_row, sep=&quot;_&quot;) combined_labels [1] &quot;Control_Replicate1&quot; &quot;Control_Replicate2&quot; &quot;Control_Replicate3&quot; [4] &quot;Drug_Replicate1&quot; &quot;Drug_Replicate2&quot; &quot;Drug_Replicate3&quot; Now, we can add these labels as column names to the dataframe: colnames(df) &lt;- combined_labels head(df) Control_Replicate1 Control_Replicate2 Control_Replicate3 Drug_Replicate1 3 43.69202 35.43517 27.69333 35.3156 4 41.85664 38.17644 35.61621 34.81943 5 49.11707 39.86308 27.20247 30.45615 6 49.79331 37.5157 39.98903 37.46084 7 41.54301 42.66665 26.92205 30.25243 8 44.04201 37.10115 18.24681 35.93469 Drug_Replicate2 Drug_Replicate3 3 20.54166 16.13286 4 20.43263 16.47575 5 29.8097 16.49928 6 25.94712 18.48844 7 22.90337 21.93457 8 20.10045 22.86391 To convert this dataframe into a tidy format we use the pivot_longer() function, exactly like we did in the previous example: df_tidy &lt;- pivot_longer(df, cols = everything(), names_to = &quot;combined_labels&quot;, values_to = &quot;Size&quot;) head(df_tidy) # A tibble: 6 × 2 combined_labels Size &lt;chr&gt; &lt;chr&gt; 1 Control_Replicate1 43.69202 2 Control_Replicate2 35.43517 3 Control_Replicate3 27.69333 4 Drug_Replicate1 35.3156 5 Drug_Replicate2 20.54166 6 Drug_Replicate3 16.13286 The dataframe is tidy now, but we need to split the conditions from the replicates in the first column with combined labels: df_tidy &lt;- df_tidy %&gt;% separate(combined_labels, c(&#39;Treatment&#39;, &#39;Replicate&#39;)) head(df_tidy) # A tibble: 6 × 3 Treatment Replicate Size &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 Control Replicate1 43.69202 2 Control Replicate2 35.43517 3 Control Replicate3 27.69333 4 Drug Replicate1 35.3156 5 Drug Replicate2 20.54166 6 Drug Replicate3 16.13286 There is still one problem that we need to fix. The values in de the column ‘Size’ are characters &lt;chr&gt;, which means these are strings. Let’s convert the strings to actual numbers: df_tidy %&gt;% mutate(Size = as.numeric(Size)) # A tibble: 300 × 3 Treatment Replicate Size &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 Control Replicate1 43.7 2 Control Replicate2 35.4 3 Control Replicate3 27.7 4 Drug Replicate1 35.3 5 Drug Replicate2 20.5 6 Drug Replicate3 16.1 7 Control Replicate1 41.9 8 Control Replicate2 38.2 9 Control Replicate3 35.6 10 Drug Replicate1 34.8 # ℹ 290 more rows This kind of data, acquired at different conditions with different replicas is ideally suited for a SuperPlot (Lord et al., 2000). An example of this kind of data visualization is Protocol 2 2.4.3 Double quantitative data An example of quantitative continuous data for two variables is when measurements are performed at different concentrations, times or wavelengths. The latter example we have encountered before when spectral data was loaded. Let’s look again at that data and convert it to tidy format. df &lt;- readxl::read_excel(&#39;FPbase_Spectra.xlsx&#39;, sheet = &#39;mNeonGreen&#39;) head(df) # A tibble: 6 × 3 Wavelength `mNeonGreen EM` `mNeonGreen EX` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 300 NA NA 2 301 NA NA 3 302 NA NA 4 303 NA NA 5 304 NA NA 6 305 NA NA Both the Emission (EM) and excitation (EX) data are acquired as a function of wavelength. To convert this data to tidy format, we need to keep a column with wavelength data and we need another column with the spectral data. To achieve this, we will modify the mNeonGreen data and keep the Wavelength data as a column: df_tidy &lt;- pivot_longer(df, cols = -Wavelength, names_to = &quot;sample&quot;, values_to = &quot;intensity&quot;) head(df_tidy) # A tibble: 6 × 3 Wavelength sample intensity &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 300 mNeonGreen EM NA 2 300 mNeonGreen EX NA 3 301 mNeonGreen EM NA 4 301 mNeonGreen EX NA 5 302 mNeonGreen EM NA 6 302 mNeonGreen EX NA We can now plot the two spectra, which was not possible before the conversion: qplot(data=df_tidy,x=Wavelength, y=intensity, color=sample, geom=&#39;line&#39;) A more elaborate example of plotting spectra is given in Protocol 1 2.4.4 Data from multiple files In an earlier example, we have combined the data from multiple files into a single dataframe. Let’s first load it: df &lt;- read.csv(&#39;df_S1P_combined.csv&#39;) head(df) id Time Cell.1 Cell.2 Cell.3 Cell.4 Cell.5 1 Cdc42_S1P 0.0000000 1.0035170 1.0015490 0.9810209 0.9869040 1.0041990 2 Cdc42_S1P 0.1666667 0.9991689 0.9961631 0.9801265 0.9891608 0.9989283 3 Cdc42_S1P 0.3333333 1.0013450 1.0046440 1.0106020 1.0124910 0.9953477 4 Cdc42_S1P 0.5000000 1.0015790 1.0043180 1.0106890 1.0066810 1.0027020 5 Cdc42_S1P 0.6666667 0.9943522 0.9933341 1.0168220 1.0045540 0.9988462 6 Cdc42_S1P 0.8333333 1.0046240 1.0071780 1.0140210 1.0042580 0.9920673 Cell.6 Cell.7 Cell.8 Cell.9 Cell.10 Cell.11 Cell.12 1 1.0049370 1.0042000 1.0014210 1.0001080 1.000748 1.0015060 1.0057850 2 0.9892894 0.9903881 0.9975485 0.9937496 0.995352 0.9962637 0.9972004 3 1.0104820 1.0140110 1.0016740 1.0003500 1.003132 1.0015520 0.9942397 4 1.0031760 1.0016000 1.0000720 1.0035080 1.000595 1.0017760 1.0035220 5 0.9921757 0.9898673 0.9993021 1.0023010 1.000192 0.9989264 0.9993660 6 1.0081460 1.0075700 0.9985681 1.0013800 1.001741 0.9989417 1.0028090 Cell.13 Cell.14 Cell.15 Cell.16 Cell.17 Cell.18 Cell.19 Cell.20 1 1.0111880 1.0147230 1.0089390 1.0032410 1.0068720 1.0019290 1.0008660 NA 2 0.9894263 1.0044960 1.0026040 0.9978889 0.9970717 0.9993551 0.9980562 NA 3 0.9921575 1.0073610 1.0029890 1.0009640 0.9984493 0.9968905 0.9966830 NA 4 1.0102150 0.9910634 0.9943020 0.9942904 0.9932356 1.0059050 1.0043110 NA 5 0.9974161 0.9829283 0.9912844 1.0037130 1.0045560 0.9959676 1.0000940 NA 6 1.0070350 0.9790479 0.9861535 1.0059600 1.0065300 1.0070570 1.0049470 NA Cell.21 Cell.22 Cell.23 Cell.24 Cell.25 Cell.26 Cell.27 Cell.28 Cell.29 1 NA NA NA NA NA NA NA NA NA 2 NA NA NA NA NA NA NA NA NA 3 NA NA NA NA NA NA NA NA NA 4 NA NA NA NA NA NA NA NA NA 5 NA NA NA NA NA NA NA NA NA 6 NA NA NA NA NA NA NA NA NA Cell.30 Cell.31 Cell.32 1 NA NA NA 2 NA NA NA 3 NA NA NA 4 NA NA NA 5 NA NA NA 6 NA NA NA The data is still in a wide format and all the data that is in a column that starts with ‘Cell’ needs to be combined into a single column. The other columns id and Time need to be excluded from this operation and this is achieved with using the - sign: df_tidy &lt;- pivot_longer(df, cols = -c(id, Time), names_to = &quot;object&quot;, values_to = &quot;activity&quot;) head(df_tidy) # A tibble: 6 × 4 id Time object activity &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 Cdc42_S1P 0 Cell.1 1.00 2 Cdc42_S1P 0 Cell.2 1.00 3 Cdc42_S1P 0 Cell.3 0.981 4 Cdc42_S1P 0 Cell.4 0.987 5 Cdc42_S1P 0 Cell.5 1.00 6 Cdc42_S1P 0 Cell.6 1.00 If desired, the column ‘id’ can be renamed and/or split: df_tidy &lt;- df_tidy %&gt;% separate(id, c(&#39;Condition&#39;, &#39;Treatment&#39;)) head(df_tidy) # A tibble: 6 × 5 Condition Treatment Time object activity &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 Cdc42 S1P 0 Cell.1 1.00 2 Cdc42 S1P 0 Cell.2 1.00 3 Cdc42 S1P 0 Cell.3 0.981 4 Cdc42 S1P 0 Cell.4 0.987 5 Cdc42 S1P 0 Cell.5 1.00 6 Cdc42 S1P 0 Cell.6 1.00 This dataframe is perfectly tidy, but there’s one little improvement that improves the sorting of the objects. The number of cells runs from 1 to 32. When it is sorted, Cell.1 will be followed by Cell.10: sort(unique(df_tidy$object)) [1] &quot;Cell.1&quot; &quot;Cell.10&quot; &quot;Cell.11&quot; &quot;Cell.12&quot; &quot;Cell.13&quot; &quot;Cell.14&quot; &quot;Cell.15&quot; [8] &quot;Cell.16&quot; &quot;Cell.17&quot; &quot;Cell.18&quot; &quot;Cell.19&quot; &quot;Cell.2&quot; &quot;Cell.20&quot; &quot;Cell.21&quot; [15] &quot;Cell.22&quot; &quot;Cell.23&quot; &quot;Cell.24&quot; &quot;Cell.25&quot; &quot;Cell.26&quot; &quot;Cell.27&quot; &quot;Cell.28&quot; [22] &quot;Cell.29&quot; &quot;Cell.3&quot; &quot;Cell.30&quot; &quot;Cell.31&quot; &quot;Cell.32&quot; &quot;Cell.4&quot; &quot;Cell.5&quot; [29] &quot;Cell.6&quot; &quot;Cell.7&quot; &quot;Cell.8&quot; &quot;Cell.9&quot; To correct this, we need a 0 preceding the single digit numbers, e.g. Cell.01. To do that, we first split the object column into two columns, using the dot as a separator: df_tidy &lt;- df_tidy %&gt;% separate(&quot;object&quot;, c(&quot;object&quot;, &quot;number&quot;), sep=&quot;\\\\.&quot;) Then, we fill up all the number to two digits by adding a 0 in front of all the single digit numbers with the function str_pad(): df_tidy &lt;- df_tidy %&gt;% mutate(number=str_pad(number, 2, pad = &quot;0&quot;)) We can merge the two columns back together and I changed the seperator to a space: df_tidy &lt;- df_tidy %&gt;% unite(&quot;object&quot;, c(&quot;object&quot;, &quot;number&quot;), sep=&quot; &quot;) head(df_tidy) # A tibble: 6 × 5 Condition Treatment Time object activity &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 Cdc42 S1P 0 Cell 01 1.00 2 Cdc42 S1P 0 Cell 02 1.00 3 Cdc42 S1P 0 Cell 03 0.981 4 Cdc42 S1P 0 Cell 04 0.987 5 Cdc42 S1P 0 Cell 05 1.00 6 Cdc42 S1P 0 Cell 06 1.00 When the sorting is repeated it looks better: sort(unique(df_tidy$object)) [1] &quot;Cell 01&quot; &quot;Cell 02&quot; &quot;Cell 03&quot; &quot;Cell 04&quot; &quot;Cell 05&quot; &quot;Cell 06&quot; &quot;Cell 07&quot; [8] &quot;Cell 08&quot; &quot;Cell 09&quot; &quot;Cell 10&quot; &quot;Cell 11&quot; &quot;Cell 12&quot; &quot;Cell 13&quot; &quot;Cell 14&quot; [15] &quot;Cell 15&quot; &quot;Cell 16&quot; &quot;Cell 17&quot; &quot;Cell 18&quot; &quot;Cell 19&quot; &quot;Cell 20&quot; &quot;Cell 21&quot; [22] &quot;Cell 22&quot; &quot;Cell 23&quot; &quot;Cell 24&quot; &quot;Cell 25&quot; &quot;Cell 26&quot; &quot;Cell 27&quot; &quot;Cell 28&quot; [29] &quot;Cell 29&quot; &quot;Cell 30&quot; &quot;Cell 31&quot; &quot;Cell 32&quot; We will save this tidy dataframe for later use: df_tidy %&gt;% write.csv(&quot;df_S1P_combined_tidy.csv&quot;, row.names = FALSE) 2.4.5 Data in 96-wells format {data-in-96-wells-format} Data measured on samples in multiwell plates are often stored in a structure that resembles the plate layout. As an example, we use here a dataset from a luciferase experiment, measured with a 96-well plate luminescence reader. The data is stored in a sheet named ‘Results’ in an xlsx file. The cells in which the data is stored are located in F21:Q28. By selecting these cells, only the 96 values that were measured are read: df &lt;- readxl::read_excel(&#39;DualLuc_example_data.xlsx&#39;, sheet = &#39;Results&#39;, range = &quot;F21:Q28&quot;, col_names = F) New names: • `` -&gt; `...1` • `` -&gt; `...2` • `` -&gt; `...3` • `` -&gt; `...4` • `` -&gt; `...5` • `` -&gt; `...6` • `` -&gt; `...7` • `` -&gt; `...8` • `` -&gt; `...9` • `` -&gt; `...10` • `` -&gt; `...11` • `` -&gt; `...12` The table-like layout of the data can be changed into a list of 96 values: data_as_list &lt;- df %&gt;% unlist(use.names = F) It is essential to know how the table is converted and this is done by reading the data from the first column, top to bottom, than the second column, etc. Knowing this, we can define the wells to which the data belongs, which would be A1, B1, … , G12, H12. column &lt;- rep(1:12, each=8) row &lt;- rep(LETTERS[1:8],12) Well &lt;- paste0(row,column) Well [1] &quot;A1&quot; &quot;B1&quot; &quot;C1&quot; &quot;D1&quot; &quot;E1&quot; &quot;F1&quot; &quot;G1&quot; &quot;H1&quot; &quot;A2&quot; &quot;B2&quot; &quot;C2&quot; &quot;D2&quot; [13] &quot;E2&quot; &quot;F2&quot; &quot;G2&quot; &quot;H2&quot; &quot;A3&quot; &quot;B3&quot; &quot;C3&quot; &quot;D3&quot; &quot;E3&quot; &quot;F3&quot; &quot;G3&quot; &quot;H3&quot; [25] &quot;A4&quot; &quot;B4&quot; &quot;C4&quot; &quot;D4&quot; &quot;E4&quot; &quot;F4&quot; &quot;G4&quot; &quot;H4&quot; &quot;A5&quot; &quot;B5&quot; &quot;C5&quot; &quot;D5&quot; [37] &quot;E5&quot; &quot;F5&quot; &quot;G5&quot; &quot;H5&quot; &quot;A6&quot; &quot;B6&quot; &quot;C6&quot; &quot;D6&quot; &quot;E6&quot; &quot;F6&quot; &quot;G6&quot; &quot;H6&quot; [49] &quot;A7&quot; &quot;B7&quot; &quot;C7&quot; &quot;D7&quot; &quot;E7&quot; &quot;F7&quot; &quot;G7&quot; &quot;H7&quot; &quot;A8&quot; &quot;B8&quot; &quot;C8&quot; &quot;D8&quot; [61] &quot;E8&quot; &quot;F8&quot; &quot;G8&quot; &quot;H8&quot; &quot;A9&quot; &quot;B9&quot; &quot;C9&quot; &quot;D9&quot; &quot;E9&quot; &quot;F9&quot; &quot;G9&quot; &quot;H9&quot; [73] &quot;A10&quot; &quot;B10&quot; &quot;C10&quot; &quot;D10&quot; &quot;E10&quot; &quot;F10&quot; &quot;G10&quot; &quot;H10&quot; &quot;A11&quot; &quot;B11&quot; &quot;C11&quot; &quot;D11&quot; [85] &quot;E11&quot; &quot;F11&quot; &quot;G11&quot; &quot;H11&quot; &quot;A12&quot; &quot;B12&quot; &quot;C12&quot; &quot;D12&quot; &quot;E12&quot; &quot;F12&quot; &quot;G12&quot; &quot;H12&quot; We can now generate a dataframe that lists the wells and the values, which is a (luminescence) intensity. We can also add two additional columns that list the row and column information: df_tidy_wells &lt;- data.frame(column, row, Well, Intensity=data_as_list) head(df_tidy_wells) column row Well Intensity 1 1 A A1 2010 2 1 B B1 3210 3 1 C C1 1965 4 1 D D1 2381 5 1 E E1 1292 6 1 F F1 991 df_tidy_wells %&gt;% write.csv(&quot;df_tidy_wells.csv&quot;) This concludes the conversion of data from the plate layout into a tidy format. The instructions explained here are used in Protocol 4 "],["plotting-the-data.html", "Chapter 3 Plotting the Data 3.1 Data over time (continuous vs. continuous) 3.2 Discrete conditions 3.3 Statistics 3.4 Plot-a-lot - discrete data 3.5 Optimizing the data visualization 3.6 Adjusting the layout 3.7 Plot-a-lot - continuous data", " Chapter 3 Plotting the Data When the data is in the right shape, it is ready for plotting. In R there is a dedicated package, ggplot2, for state-of-the-art data visualization. It is part of the ‘tidyverse’ and available when the tidyverse package is loaded. The ggplot2 package is extremely versatile and the plots can be fully customized. This great advantage comes with a disadvantage and that is complexity. Hopefully this chapter will get you started with generating some informative and good-looking plots. In the last chapter with Complete protocols we’ll dive deeper into details. The default ‘theme’ that is used in ggplot uses a grey plotting area with white gridlines, as can be seen in the plots that were presented in the previous sections. Since I prefer a more classic, white plotting area, I use a different theme from now on. This is theme_light() and it can be set in R (when the tidyverse or ggplot2 package is loaded) as follows: theme_set(theme_light()) ggplot(mtcars, aes(wt, mpg)) + geom_point() 3.1 Data over time (continuous vs. continuous) We have previously used a quick plot function qplot() from the ggplot2 package. We will use it here again for showing single cell responses from timelapse imaging. First, we load the (tidy) data and filter the data that reports on Rho and does not have any missing values in the column named activity: df_tidy &lt;- read.csv(&quot;df_S1P_combined_tidy.csv&quot;) df_Rho &lt;- df_tidy %&gt;% filter(Condition == &#39;Rho&#39;) %&gt;% filter(!is.na(activity)) We use qplot() to plot the data, by defining the dataframe, and by selecting what data is used for the x- and y-axis. The default geometry (dots) is overridden to show lines and we need to indicate which column defines which is grouped and connected by the line (in this case it is defined by the column object): qplot(data=df_Rho, x=Time, y=activity, geom = &#39;line&#39;, group=object) Let’s now change to the ggplot() function, as it allows for more flexibility. First we create an identical plot: ggplot(data=df_Rho, aes(x=Time, y=activity, group=object)) + geom_line() The aes() function is used for mapping “aesthetics”. The aesthetics specify how the variables from the dataframe are used to visualise those variables. In this case the data in the column ‘Time’ is used for mapping the data onto the x-axis and data in the column ‘activity’ is used to map the data onto the y-axis. The geom_line() function specifies that the data is shown as a line and it can be used to set the appearance of the line. We can set the linewidth (size) and the transparency (alpha). Moreover, we can map the different objects to different colors with aes(): ggplot(data=df_Rho, aes(x=Time, y=activity)) + geom_line(aes(color=object), size=1, alpha=0.8) Since ggplot supports layers, we can add another layer that shows the data as dots using geom_point(). Note that when the definition of the plot spans multiple lines, each line (when followed by another line should end with a +: ggplot(data=df_Rho, aes(x=Time, y=activity)) + geom_line(aes(color=object), size=1, alpha=0.8) + geom_point(size=2, alpha=0.8) Note that the order is important, the geometry that is defined by the last function (lines in this example) appears on top in the plot: ggplot(data=df_Rho, aes(x=Time, y=activity)) + geom_point(size=2, alpha=0.8) + geom_line(aes(color=object), size=1, alpha=0.8) There is another way to define a plot and add layers. First we define a ggplot object: p &lt;- ggplot(data=df_Rho, aes(x=Time, y=activity)) We can define layers with different geometries and add these to the ggplot object: p &lt;- p + geom_point(size=2, alpha=0.8) p &lt;- p + geom_line(aes(color=object), size=1, alpha=0.8) Just like the content of a dataframe can be displayed by typing its name, the plot can be shown by typing its name: p This is convenient and we can use it to experiment with different visualizations. Here I demonstrate this modify&amp;plot approach to remove the legend: p + theme(legend.position = &quot;none&quot;) Or remove the legend and add a title: p + theme(legend.position = &quot;none&quot;) + ggtitle(&quot;Single cell responses over time&quot;) Since ggplot() is part of the tidyverse, it accepts dataframes that are passed through a pipe: %&gt;% This can be used to select a subset of a dataframe for plotting. Below, the ‘Rac’ condition is filtered from the dataframe df_tidy and passed to ggplot for plotting: df_tidy %&gt;% filter(Condition == &#39;Rac&#39;) %&gt;% ggplot(aes(x=Time, y=activity)) + geom_line(aes(color=object)) 3.2 Discrete conditions First, we load a dataset that has intensity measurements for 5 different conditions. For each conditions there are three measurements. This would be a typical outcome of the quantification of a western blot for N=3: df &lt;- read.csv(&#39;Low_n_tidy.csv&#39;) head(df) Condition Intensity 1 A 1.0 2 B 10.3 3 C 5.5 4 D 4.5 5 E 2.3 6 A 1.1 The basic function to plot the data is ggplot(). We supply the name of the dataframe and we define how to ‘map’ the data onto the x- and y-axis. For instance, we can plot the different conditions on the x-axis and show the size measurements on the y-axis: ggplot(data=df, mapping = aes(x=Condition, y=Intensity)) This defines the canvas, but does not plot any data yet. To plot the data, we need to define how it will be plotted. We can choose to plot it as dots with the function geom_point(): ggplot(data=df, mapping = aes(x=Condition, y=Intensity)) + geom_point() Within geom_point() we can specify the looks of the dot. For instance, we can change its color, shape and size: ggplot(data=df, mapping = aes(x=Condition, y=Intensity)) + geom_point(color=&quot;blue&quot;, shape=18 ,size=8) One of the issues with data that has low N, is that it may not look ‘impressive’, in the sense that there is lots of empty space on the canvas. This may be a reason to resort to bar graphs. However, bar graphs only show averages, which hinders transparent communication of results (https://doi.org/10.1371/journal.pbio.1002128). In situations where a bar graph is added, it has to be defined in the first layer to not overlap with the datapoints: ggplot(data=df, mapping = aes(x=Condition, y=Intensity)) + geom_bar(stat = &quot;summary&quot;, fun = &quot;mean&quot;) + geom_point(size=4) In the default setting, there’s too much emphasis on the bar. This can be changed by formatting the looks of the bars, i.e. by changing the fill color, and the width: ggplot(data=df, mapping = aes(x=Condition, y=Intensity)) + geom_bar(stat = &quot;summary&quot;, fun = &quot;mean&quot;, fill=&quot;grey80&quot;, width=0.7) + geom_point(size=4) The overlap of the dots can be reduced by introducing ‘jitter’ which displays the dots with a random offset. Note that the extent of the offset can be controlled and should not exceed the width of the bar. Another way to improve the visibility of overlapping dots is to make the dots transparant. This is controlled by ‘alpha’, which should be a number between 0 (fully transarent, invisible) and 1 (not transparant). In the graph below, both jitter and transparancy are used. ggplot(data=df, mapping = aes(x=Condition, y=Intensity)) + geom_bar(stat = &quot;summary&quot;, fun = &quot;mean&quot;, fill=&quot;grey80&quot;, width=0.7) + geom_jitter(size=4, width=0.2, alpha=0.7) The jitter is applied randomly. To make a plot with reproducible jitter, one can fix the seed that is used for randomization by providing set.seed() with a number of choice, which fixes the randomness: set.seed(1) ggplot(data=df, mapping = aes(x=Condition, y=Intensity)) + geom_bar(stat = &quot;summary&quot;, fun = &quot;mean&quot;, fill=&quot;grey80&quot;, width=0.7) + geom_jitter(size=4, width=0.2, alpha=0.7) In the plot above, the length of the bar reflects the average value. This is only true when the bar starts from 0. Situations in which the length of the bar does not accurately reflect the number are: - using a linear scale that does not include zero - cutting the axis - using a logarithmic scale, which (per definition) does not include zero. An example is shown below, where the logarithmic scale and limites are defined in by the scale_y_log10() function. Due to the non-linear scale, the length of the bar is not proportional to the value (the average) it reflects. This leads to misinterpretation of the data. ggplot(data=df, mapping = aes(x=Condition, y=Intensity)) + geom_bar(stat = &quot;summary&quot;, fun = &quot;mean&quot;, fill=&quot;grey80&quot;, width=0.7) + geom_jitter(size=4, width=0.2, alpha=0.7) + scale_y_log10(limits=c(.5,12)) 3.2.1 X-axis data: qualitative versus quantitative data Suppose that the data comes from an experiment in which the data are measured at different time points. First we define a vector that defines the timepoints: e.g. 0, 1, 2, 5, 10: t &lt;- c(0,1,2,5,10) We need to repeat these timepoints three times, once for each replicate: t3 &lt;- rep(t,3) Now we can add the vector to the dataframe: df &lt;- df %&gt;% mutate(Time=t3) And plot the activity for the different time points: ggplot(data=df, mapping = aes(x=Time, y=Intensity)) + geom_bar(stat = &quot;summary&quot;, fun = &quot;mean&quot;, fill=&quot;grey80&quot;, width=0.7) + geom_jitter(size=4, width=0.2, alpha=0.7) This graph looks different because we it has numbers on the x-axis. The numbers are treated as ‘continuous quantitative data’ and the data is positioned according to the values. To treat the numbers as conditions or labels we need to convert them to qualitative data. This class of data is called factors in R. We can verify the class of data by selecting the column using the class() function. Here we select the third column of the dataframe to check its class: class(df[,3]) [1] &quot;numeric&quot; Now we convert the column ‘Time’ to the class factor: df &lt;- df %&gt;% mutate(Time=as.factor(Time)) Let’s verify that the class is changed: class(df[,3]) [1] &quot;factor&quot; We use the same line of code to plot the data, and the graph will look similar to the graph that used the conditions indicated with letters. ggplot(data=df, mapping = aes(x=Time, y=Intensity)) + geom_bar(stat = &quot;summary&quot;, fun = &quot;mean&quot;, fill=&quot;grey80&quot;, width=0.7) + geom_jitter(size=4, width=0.2, alpha=0.7) Whether you treat the numbers on the x-axis as labels or values determines on the data and the message that you want to convey. If it is important to know when the highest activity occurs, it may not matter that the points are not equidistant (as in this example). In fact the data in the plot may better align with the lanes of a blot (which are also equidistant) from which the data are quantified . On the other hand, if you are interested in the dynamics of the activity, the timepoints on the x-axis should reflect the actual values to enable proper interpretation. 3.3 Statistics 3.3.1 Introduction Thus far, we were mainly concerned with plotting the data. But plots with scientific data often feature some kind of statistics. Next to the mean or median, error bars are used to summarize variability or to reflect the uncertainty of the measurement. Intermezzo: Descriptive vs Inferential Statistics It is a good idea to reflect on the reason to display statistics and it is essential to understand that you can choose between descriptive and inferential statistics. The descriptive statistics are used to summarize the data. Examples of descriptive statistics are the mean, median and standard deviation. Boxplots also display descriptive statistics. Inferential statistics are used to make ‘inferences’ or, in other words, generalize the data that are measured to the population it was sampled from. It is used to compare experiments and make predictions. Examples of inferential statistics are standard error of the mean and confidence intervals. There are (at least) two ways to overlay statistics in a plot. The first way is demonstrated in the previous section, where a layer with the statistics (bar) was directly added to the plot. Below, we take this strategy a step further to display the standard deviation. 3.3.2 Data summaries directly added as a plot layer In the code below the stat_summary() defines a layer with statistics. The fun=mean statement indicates that the function mean() should be applied to every condition on the x-axis: ggplot(data=df, mapping = aes(x=Time, y=Intensity)) + geom_jitter(size=4, width=0.2, alpha=0.7) + stat_summary(fun = mean, geom=&#39;point&#39;, size=8, color=&#39;blue&#39;) This is pretty ugly and it is more common to indicate the mean (or median) with a horizontal line. This can be done by specifying the shape of the point: ggplot(data=df, mapping = aes(x=Time, y=Intensity)) + geom_jitter(size=4, width=0.2, alpha=0.7) + stat_summary(fun = mean, geom=&#39;point&#39;, shape=95, size=24, color=&#39;black&#39;) This works, but it doesn’t allow us to specify the width and the thickness of the line. To have better control over the line we turn to another ‘geom’, geom_errorbar(). This function is actually used to display errorbars, but if we only set one value for the min and max, it allows us to display the mean. We can change the looks of the horizontal bar by changing the width and the size. The latter defines the thickness of the line. ggplot(data=df, mapping = aes(x=Time, y=Intensity)) + geom_jitter(size=4, width=0.2, alpha=0.7) + stat_summary(fun.min=mean, fun.max=mean, geom=&#39;errorbar&#39;, width=0.6, size =1) We can also indicate the standard deviation (SD), but we need to define a custom function to calculate the position of the upper and lower limit of the errorbar. That is, we need to display mean+SD and mean-SD for each condition. The code for the function that defines the lower limit is: function(y) {mean(y)-sd(y)} and for the upper limit it is: function(y) {mean(y)+sd(y)} Here we go (note that the width is set to a smaller value): ggplot(data=df, mapping = aes(x=Time, y=Intensity)) + geom_jitter(size=4, width=0.2, alpha=0.7) + stat_summary(fun.min=function(y) {mean(y) - sd(y)}, fun.max=function(y) {mean(y) + sd(y)}, geom=&#39;errorbar&#39;, width=0.3, size =1) By combing the layers that define the mean and de sd, we can show both: ggplot(data=df, mapping = aes(x=Time, y=Intensity)) + geom_jitter(size=4, width=0.2, alpha=0.7) + stat_summary(fun.min=function(y) {mean(y) - sd(y)}, fun.max=function(y) {mean(y) + sd(y)}, geom=&#39;errorbar&#39;, width=0.3, size =1) + stat_summary(fun.min=mean, fun.max=mean, geom=&#39;errorbar&#39;, width=0.6, size =1) Finally, an example that displays the 95% confidence intervals: ggplot(data=df, mapping = aes(x=Time, y=Intensity)) + geom_jitter(size=4, width=0.2, alpha=0.7) + stat_summary(fun.min=function(y) {mean(y) - qt((1-0.95)/2, length(y) - 1) * sd(y) / sqrt(length(y) - 1)}, fun.max=function(y) {mean(y) + qt((1-0.95)/2, length(y) - 1) * sd(y) / sqrt(length(y) - 1)}, geom=&#39;errorbar&#39;, width=0.3, size =1) + stat_summary(fun.min=mean, fun.max=mean, geom=&#39;errorbar&#39;, width=0.6, size =1) This method works, but the code to generate this graph is pretty long and the definition of the function make it difficult to follow and understand what’s going on. In addition, the values for the statistics are not accessible. To solve these issue, I will demonstrate below a more intuitive way to calculate and display the statistics. 3.3.3 Data summaries from a dataframe We start out from the same data and dataframe. First, we calculate the statistics and assign the values to a new dataframe. To this end, we use the summarise() function for each condition (Time in this dataset) which we indicate by the use of group_by(): df_summary &lt;- df %&gt;% group_by(Time) %&gt;% summarise(n=n(), mean=mean(Intensity), sd=sd(Intensity)) head(df_summary) # A tibble: 5 × 4 Time n mean sd &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 3 1 0.1 2 1 3 9.83 0.451 3 2 3 6.1 1.22 4 5 3 4.67 0.208 5 10 3 3.17 0.850 This new dataframe can be used as source for displaying the statistics. Note that we need to indicate the df_summary dataframe for each layer: ggplot(data = df) + geom_jitter(aes(x=Time, y=Intensity),size=4, width=0.2, alpha=0.7) + geom_errorbar(data=df_summary, aes(x=Time,ymin=(mean-sd), ymax=(mean+sd)), width=0.3, size =1) + geom_errorbar(data=df_summary, aes(x=Time,ymin=(mean), ymax=(mean)), width=0.6, size =1) How about other stats? If we calculate other stats like sem, MAD and confidence intervals and store those in a dataframe, we can retrieve those for plotting as well. Below the code for the calculation of the most common statistics is presented. There is no function to calculare sem or the confidence interval and so we calulate those using mutate(). The confidence levels is set to 95%: Confidence_level = 0.95 df_summary &lt;- df %&gt;% group_by(Time) %&gt;% summarise(n=n(), mean=mean(Intensity), median=median(Intensity), sd=sd(Intensity)) %&gt;% mutate(sem=sd/sqrt(n-1), mean_CI_lo = mean + qt((1-Confidence_level)/2, n - 1) * sem, mean_CI_hi = mean - qt((1-Confidence_level)/2, n - 1) * sem ) head(df_summary) # A tibble: 5 × 8 Time n mean median sd sem mean_CI_lo mean_CI_hi &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 3 1 1 0.1 0.0707 0.696 1.30 2 1 3 9.83 9.8 0.451 0.319 8.46 11.2 3 2 3 6.1 5.5 1.22 0.860 2.40 9.80 4 5 3 4.67 4.6 0.208 0.147 4.03 5.3 5 10 3 3.17 3.2 0.850 0.601 0.579 5.75 In principle the code for plotting the error bars that reflect the standard deviations (or sem) can be simplified if the upper and lower limit are calculated, similar to the example shown above for the 95% confidence intervals. 3.3.4 Data summaries for continuous x-axis data Let’s revisit the data from the time-course of Rho GTPase activity that we’ve looked at earlier: df_tidy &lt;- read.csv(&quot;df_S1P_combined_tidy.csv&quot;) df_Rho &lt;- df_tidy %&gt;% filter(Condition == &#39;Rho&#39;) %&gt;% filter(!is.na(activity)) Calculate the statistics for each time point: df_summary &lt;- df_Rho %&gt;% group_by(Time) %&gt;% summarise(n=n(), mean=mean(activity), median=median(activity), sd=sd(activity)) %&gt;% mutate(sem=sd/sqrt(n-1), mean_CI_lo = mean + qt((1-Confidence_level)/2, n - 1) * sem, mean_CI_hi = mean - qt((1-Confidence_level)/2, n - 1) * sem ) head(df_summary) # A tibble: 6 × 8 Time n mean median sd sem mean_CI_lo mean_CI_hi &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 12 0.998 0.997 0.00477 0.00144 0.995 1.00 2 0.167 12 1.00 1.00 0.00615 0.00186 0.999 1.01 3 0.333 12 0.998 0.998 0.00260 0.000783 0.996 1.00 4 0.5 12 0.999 0.998 0.00657 0.00198 0.995 1.00 5 0.667 12 1.00 1.00 0.00703 0.00212 0.998 1.01 6 0.833 12 1.00 1.00 0.00741 0.00223 0.998 1.01 With this data summary it is possible to depict the 95% confidence interval as error bars: ggplot(data = df_Rho) + geom_line(aes(x=Time, y=activity, color=object),size=1, alpha=0.7) + geom_errorbar(data=df_summary, aes(x=Time,ymin=mean_CI_lo, ymax=(mean_CI_hi)), width=0.3, size=1, alpha=0.7) It works, but it is also quite messy. Luckily ggplot has a more elegant solution and that’s geom_ribbon(): ggplot(data = df_Rho) + geom_line(aes(x=Time, y=activity, group=object),size=.5, alpha=0.4) + geom_ribbon(data=df_summary, aes(x=Time,ymin=mean_CI_lo, ymax=(mean_CI_hi)), fill=&#39;blue&#39;, alpha=0.3) Note that I also removed the color of the individual lines, as it is more about the ensemble and it’s average than the individual line. Adding a line to reflect the average is pretty straightforward: ggplot(data = df_Rho) + geom_line(aes(x=Time, y=activity, group=object),size=.5, alpha=0.4) + geom_ribbon(data=df_summary, aes(x=Time,ymin=mean_CI_lo, ymax=(mean_CI_hi)), fill=&#39;blue&#39;, alpha=0.3) + geom_line(data=df_summary, aes(x=Time,y=mean), color=&#39;blue&#39;, size=2, alpha=0.8) 3.4 Plot-a-lot - discrete data Other data summaries that are often depicted in plots are boxplots and violinplots. These are not suited for data with low n, as in the previous example. The reason is that the boxplot is defined by five values, i.e. the median (the central line), the interquartile range, IQR (the two limits of the box) and the endpoints of the two lines, also known as whiskers. It makes no sense to use a boxplot for n=5, since it does not add any new information. There is no hard cut-off, but in my opinion boxplots make sense when you have 10 or more datapoints per condition. Although the boxplot is a good data summary for normally distributed and skewed data distributions, it doesn’t capture the underlying distribution well when it is bi- or multimodal. In these cases, a violin plot is better suited. The box- and violinplot are easily added as a layer as they are defined by specific functions, geom_boxplot() and geom_violin(). First, let’s load a dataset with larger n. The function summary() provides a quick overview of the data: df &lt;- read.csv(&#39;Area_tidy.csv&#39;) summary(df) Condition value Length:283 Min. : 477.2 Class :character 1st Qu.:1152.1 Mode :character Median :1490.9 Mean :1567.8 3rd Qu.:1838.1 Max. :3906.7 There are three conditions, with relative large n. Let’s plot the data: ggplot(df, aes(x=Condition, y=value)) + geom_jitter(width=0.2, alpha=0.5) Adding a boxplot: ggplot(df, aes(x=Condition, y=value)) + geom_jitter(width=0.2, alpha=0.5) + geom_boxplot() The geom_boxplot displays outliers (data beyond the whiskers). This is clear when only the boxplot is shown: ggplot(df, aes(x=Condition, y=value)) + geom_boxplot() When the data is displayed together with the boxplot, the outliers need to be removed to avoid duplication. And to make sure that the data is visibel and not hidden by the boxplot, we can either change the order of the layers or remove the white color that is used to fill the box: ggplot(df, aes(x=Condition, y=value)) + geom_jitter(width=0.2, alpha=0.5) + geom_boxplot(fill=NA, outlier.color = NA) In case a boxplot is used as summary, it may be useful to have the values of Q1, Q3 and the interquartile range. The code shown below can be used to calculate all these paramters and also includes the median absolute deviation (MAD) as a robust measure of variability: Confidence_level = 0.95 df_summary &lt;- df %&gt;% group_by(Condition) %&gt;% summarise(n=n(), mean=mean(value), median=median(value), sd=sd(value), MAD=mad(value, constant=1), IQR=IQR(value), Q1=quantile(value, probs=0.25), Q3=quantile(value, probs=0.75)) df_summary # A tibble: 3 × 9 Condition n mean median sd MAD IQR Q1 Q3 &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 LARG 99 1361. 1271. 486. 250. 485. 1053. 1538. 2 TIAM 110 1808. 1705. 704. 416. 818. 1293. 2112. 3 wt 74 1487. 1509. 459. 333. 662. 1158. 1821. Display of a violinplot in addition to the data: ggplot(df, aes(x=Condition, y=value)) + geom_violin() + geom_jitter(width=0.2, alpha=0.5) Personally, I like the combination of data and violinplot, but the jitter can make the plot look messy. There are packages that enable the plotting of the data according to its distribution. There is geom_sina() from the {ggforce} package and geom_quasirandom() from the {ggbeeswarm} package: library(ggbeeswarm) ggplot(df, aes(x=Condition, y=value)) + geom_violin() + geom_quasirandom(width=0.3, alpha=0.5) Sometimes, you find examples of boxplots overlayed on violin plots, like this (and note that we have filled the violins with a unique color for each condition): ggplot(df, aes(x=Condition, y=value)) + geom_violin(aes(fill=Condition), alpha=0.5) + geom_boxplot(width=0.1, outlier.color = NA) Note that geom_quasirandom() can generate plots with excessive overlap between data points when there’s large amounts of data. In some cases this can be solved by increasing the transparency (by decreasing alpha). Another option is to use geom_beeswarm(), which does not allow overlap of the points. I have a preference for showing the actual data, but for large n the violinplot may better convey the message of the data than a cluttered plot that shows a lot of dots. 3.5 Optimizing the data visualization 3.5.1 Rotation Rotating a plot by 90 degrees can be surprisingly effective. Especially when the labels that are used for the x-axis are so long that they need to be rotated, it is better to rotate the plot. This improves the readability of the numbers and labels on both the x- and y-axis and avoids the need of tilting your head. library(ggbeeswarm) ggplot(df, aes(x=Condition, y=value)) + geom_boxplot(outlier.colour = NA) + geom_quasirandom(width=0.3, alpha=0.5) + coord_flip() An example of how to further tweak a 90˚ rotated plots is given in Protocol 2 3.5.2 Ordering conditions The order of the conditions used for the x-axis (for discrete conditions) is numeric and alphabetic. In case of a large number of conditions, it can help to sort these according to the median or mean value. This requires another order of the factors. To check the order of the factors in the dataframe we use: levels(df$Condition) NULL Now, we change the order, sorting the factors according to the median of “value” and we verify the order: df &lt;- df %&gt;% mutate(Condition = fct_reorder(Condition, value, .fun=&#39;median&#39;)) levels(df$Condition) [1] &quot;LARG&quot; &quot;wt&quot; &quot;TIAM&quot; Let’s plot the data: ggplot(df, aes(x=Condition, y=value)) + geom_boxplot(outlier.colour = NA) + geom_quasirandom(width=0.3, alpha=0.5) The factors on the x-axis are now sorted according to the median of value. It is also possible to manually set the sequence, in this example the order is set to wt, LARG, TIAM: df &lt;- df %&gt;% mutate(Condition = fct_relevel(Condition, c(&quot;LARG&quot;, &quot;TIAM&quot;, &quot;wt&quot;))) ggplot(df, aes(x=Condition, y=value)) + geom_boxplot(outlier.colour = NA) + geom_quasirandom(width=0.3, alpha=0.5) In the examples above, we have modified the dataframe, since we used mutate() to change the order. To set the order for plotting without altering the dataframe we can define the reordering within ggplot: ggplot(df, aes(x=fct_reorder(Condition, value, .fun=&#39;median&#39;), y=value)) + geom_boxplot(outlier.colour = NA) + geom_quasirandom(width=0.3, alpha=0.5) Alternatively, we can use the pipe operator to feed the data in the reordering function and then use the reordered dataframe for plotting: df %&gt;% mutate(Condition = fct_reorder(Condition, value, .fun=&#39;median&#39;)) %&gt;% ggplot(aes(x=Condition, y=value)) + geom_boxplot(outlier.colour = NA) + geom_quasirandom(width=0.3, alpha=0.5) We can check that the order of the levels in the dataframe has not changed and differs from the order in the plot: levels(df$Condition) [1] &quot;LARG&quot; &quot;TIAM&quot; &quot;wt&quot; 3.6 Adjusting the layout Details matter, also in data visualization. Editing labels, adding titles or annotating data can make the difference between a poor and a clear data visualization. Although it can be quicker and easier to edit a plot with software that deals with vectors, it is not reproducible. And when you need to change the graph, the editing starts all over again. Luckily, with ggplot2, you have full control over every element. A lot of elements are controlled by the function theme(), and examples are the label size and color, the grid, the legend style and the color of the plot elements. This level of control offers great power, but it can be quite daunting (and non-intuitive) for new users. We discuss a couple of straightforward manipulations of the theme below. More detailed modifications of the layout will be showcased in the chapter with Complete Protocols. 3.6.1 Themes Let’s look at a violinplot and we plot it with the default theme, which has a grey background: p &lt;- ggplot(df, aes(x=Condition, y=value)) + geom_violin(aes(fill=Condition), alpha=0.5) + geom_boxplot(width=0.1, outlier.color = NA) + theme_grey() p The default theme is OK-ish and we can change it to one of the other themes that are available in the ggplot2 package, for instance theme_classic(): p + theme_classic() The ggplot2 package has a default dark theme, but that only generate a dark background in the plot area. I made a customized theme theme_darker() for plots on dark background, e.g. black slides. It needs to be loaded with the source() function and then it can be applied: source(&quot;https://raw.githubusercontent.com/JoachimGoedhart/PlotTwist/master/themes.R&quot;) p + theme_darker() Finally, we can modify the text size of all text elements: p + theme_bw(base_size = 16) To reset the theme to the default that is used throughout the book: p &lt;- p + theme_light() 3.6.2 Legend The legend can be controlled through the theme() function. Legends are automatically created when different colors or shapes are used. One example is the plot below, where different conditions are shown in different colors. To change the style, we define the plot object p: To remove the legend we use: p + theme(legend.position = &quot;none&quot;) Other options are “top”, “left”, “bottom” and (the default) “right”. The items of the legend can also be displayed horizontally, which is a nice fit when the legend is shown on top of the plot: p + theme(legend.position = &quot;top&quot;, legend.direction = &quot;horizontal&quot;) To left align the legend: p + theme(legend.position = &quot;top&quot;, legend.direction = &quot;horizontal&quot;, legend.justification=&#39;left&#39;) 3.6.3 Grids I am not a big fan of grids, so I often remove it: p + theme(panel.grid = element_blank()) To only remove the vertical grid and make the horizontal grid more pronounced: p + theme(panel.grid.major.x = element_blank(), panel.grid.major.y = element_line(size=0.5, color=&#39;grey30&#39;), panel.grid.minor.y = element_line(size=0.2, color=&#39;grey30&#39;) ) 3.6.4 Labels/Titles Clear labeling aids the interpretation of the plot. The actual labels and titles are changed or added with the function labs() and their style is controled with the theme() function. Let’s first look at the labels. The titles of the axes and legend are retrieved from the column names in the dataframe. To adjust the axis labels with labs(): p + labs(x=&#39;Perturbation&#39;, y=&#39;Area [µm]&#39;) Note that the title of the legend is not changed. To change the legend title we need to supply a label for ‘fill’, since the legend in this example indicates the color that was used to ‘fill’ the violin plots: p + labs(x=&#39;Perturbation&#39;, y=&#39;Area [µm]&#39;, fill=&#39;Perturbation:&#39;) A title, subtitle and caption can also be added with labs(): p + labs(x=&#39;Condition&#39;, y=&#39;Area [µm]&#39;, title=&#39;This is the title...&#39;, subtitle=&#39;...and this is the subtitle&#39;, caption=&#39;A caption is text at the bottom of the plot&#39;) The style of the different labels can be set with theme(): p + labs(x=&#39;Condition&#39;, y=&#39;Area [µm]&#39;, title=&#39;This is the title...&#39;, subtitle=&#39;...and this is the subtitle&#39;, caption=&#39;A caption is text at the bottom of the plot&#39;) + theme(axis.title.x = element_text(size=16, color=&#39;black&#39;), axis.title.y = element_text(size=16, color=&#39;black&#39;), axis.text = element_text(size=14, color=&#39;orange&#39;), plot.title = element_text(size=18, color=&#39;slateblue&#39;), plot.subtitle = element_text(size=12, color=&#39;slateblue&#39;, hjust = 1), plot.caption = element_text(size=10, color=&#39;grey60&#39;, face = &quot;italic&quot;) ) This is a demonstration of how the different pieces of text can be modified, not a template for a proper data visualization since it uses too many, unnecessary, colors! 3.7 Plot-a-lot - continuous data It can be a challenge to look at individual data or samples instead of summaries when you have a large amount of data. I will illustrate a number of options based on the timeseries data of Rac activity, that we have seen before. This dataset is still quite simple, but given its heterogeneity it illustrates well how the data can be presented in such a way that all traces can be inspected. df_Rac &lt;- read.csv(&quot;df_S1P_combined_tidy.csv&quot;) %&gt;% filter(Condition == &#39;Rac&#39;) %&gt;% filter(!is.na(activity)) head(df_Rac) Condition Treatment Time object activity 1 Rac S1P 0 Cell 01 1.0012160 2 Rac S1P 0 Cell 02 1.0026460 3 Rac S1P 0 Cell 03 1.0022090 4 Rac S1P 0 Cell 04 0.9917870 5 Rac S1P 0 Cell 05 0.9935569 6 Rac S1P 0 Cell 06 0.9961453 We have seen this data before, but let’s look at this again in an ordinary line plot: ggplot(data=df_Rac, aes(x=Time, y=activity)) + geom_line(aes(color=object)) There is a lot to see here and it is a bit of mess. Clearly, there is variation, but it is difficult to connect a line, by its color to the cell it represents. Below, there are some suggestions how this data can be presented more clearly. 3.7.1 Small multiples Small multiples remind me of a stamp collection, where every stamp is a (small) plot. This works very well to display lots of data. It is also pretty straightforward in ggplot2 with facet_wrap(): ggplot(data=df_Rac, aes(x=Time, y=activity)) + geom_line() + facet_wrap(~object) I got rid of color, since it would be redundant and the improved contrast of the black line helps to focus on the data. The lay-out of the 32 mini plots can be improved by fixing the number of columns: ggplot(data=df_Rac, aes(x=Time, y=activity)) + geom_line() + facet_wrap(~object, ncol = 8) Since the small multiple is at its best, when the data stands out and the text and other elements are minimized. Here is an extreme version of the plot to make that point: ggplot(data=df_Rac, aes(x=Time, y=activity)) + geom_line() + facet_wrap(~object, ncol = 8) + theme_void() Further optimization of a small multiple plot is discussed in Protocol 3. For complex experimental designs, the data can split according to two different factors. This also uses a faceting strategy, with the facet_grid() function. This works well when the data has two discrete variables and the application of this function is demonstrated in Protocol 4. 3.7.2 Heatmaps Heatmaps are well suited for dense data and have traditionally been used for microarray and other -omics data. Heatmaps can also be used for timeseries data. To do this, we (i) keep the x-axis as is, (ii) map the objects on the y-axis and (iii) specify that the color of the tile reflects activity: ggplot(data=df_Rac, aes(x=Time, y=object)) + geom_tile(aes(fill=activity)) The default colorscale is not ideal. I personally like the (colorblind friendly) viridis scale and this colorscale can be used to fill the tile with scale_fill_viridis_c(). To sort the data according to the object labels as they appear in the dataframe: ggplot(data=df_Rac, aes(x=Time, y=object)) + geom_tile(aes(fill=activity)) + scale_fill_viridis_c() To reverse the order of the objects we can use fct_rev(): ggplot(data=df_Rac, aes(x=Time, y=fct_rev(object))) + geom_tile(aes(fill=activity)) + scale_fill_viridis_c() A heatmap does not need a grid and usually has no axes. Plus, when there are many objects, it makes sense to hide their names, ticks and the y-axis label: p &lt;- ggplot(data=df_Rac, aes(x=Time, y=fct_rev(object))) + geom_tile(aes(fill=activity)) + scale_fill_viridis_c() + theme(text = element_text(size=16), # Remove borders panel.border = element_blank(), # Remove grid panel.grid.major = element_blank(), panel.grid.minor = element_blank(), # Remove text of the y-axis axis.text.y = element_blank(), # Remove ticks on y-axis axis.ticks.y = element_blank(), # Remove label of y-axis axis.title.y = element_blank(), # Make x-axis ticks more pronounced axis.ticks = element_line(colour = &quot;black&quot;) ) p 3.7.3 Custom objects The use of titles, captions and legends will add infomration to the data visualization. Tweaking the theme will improve the style of the plot and give it the right layout. There’s another layer of attributes that are not discussed yet and these are custom objects and labels. These additions will help to tell the story, i.e. by explaining the experimental design. Let’s look at the cellular response to stimulation with a ligand. The df_Rho dataframe that we have used earlier is used in this example and we plot the average and standard deviation: p &lt;- ggplot(data=df_Rho, aes(x=Time, y=activity)) + stat_summary(fun = mean, geom=&#39;line&#39;, size=2) + stat_summary(fun.min=function(y) {mean(y) - sd(y)}, fun.max=function(y) {mean(y) + sd(y)}, geom=&#39;ribbon&#39;, color=&#39;black&#39;, size =.1, alpha=0.2) p The ligand is added at t=1.75 and we can add a vertical line with geom_vline() to indicate this: p + geom_vline(xintercept = 1.75, size=1, color=&quot;grey20&quot;) More flexible labeling is provided with the annotate() function, which enables the addition of line segments, rectangles and text. First, let’s reproduce the the previous plot with this function, by defining a line segment. The line segment has two coordinates for the start and two for the end. The x position is 1.75 for both start and end. For the y-coordinate we can use Inf and -Inf to define the endpoints of the line. The use of Inf has two advantages. We do not need to define the exact points for the start and end and we prevent rescaling of the plot to show the segment: p + annotate(&quot;segment&quot;, x = 1.75, xend = 1.75, y = -Inf, yend = Inf, size=1, color=&quot;grey20&quot;) In a similar way we can define a rectangle to highlight the time where the ligand was present (It is possible to use Inf for xmax here, in that case the blue rectangle would fill up the entire plot to the right). Since the rectangle is added as the last layer, it would occlude the data. That’s why a an alpha level of 0.1 is used to make the rectangle transparent: p + annotate(&quot;rect&quot;, xmin = 1.75, xmax = 10, ymin = -Inf, ymax = Inf, fill=&quot;blue&quot;, alpha=0.1) We can also add an rectangle to the top of the graph: p + annotate(&quot;rect&quot;, xmin = 1.75, xmax = 10, ymin = 1.26, ymax = 1.27, fill=&quot;black&quot;) + annotate(&quot;text&quot;, x=5, y=1.29, label=&quot;ligand&quot;, size=6) In the example above, the rectangle and text are added to the plot area and the y-axis is expanded. To move the annotation out of the plot area, we need to increase the white space above the plot with theme() and we can scale the plot with coord_cartesian(): p + annotate(&quot;rect&quot;, xmin = 1.75, xmax = 10, ymin = 1.25, ymax = 1.26, fill=&quot;black&quot;) + annotate(&quot;text&quot;, x=5, y=1.28, label=&quot;ligand&quot;, size=6) + coord_cartesian(ylim = c(0.98,1.23), clip = &#39;off&#39;) + theme(plot.margin = margin(t = 50, r = 0, b = 0, l = 0, unit = &quot;pt&quot;)) The annotation is part of the plot and would normally be invisible since everything outside the plot area is clipped. To show the annotation, clip = 'off' is needed in the code above. However, this may lead to undesired behavior when the scaling is set in such a way that the data falls outside the plot area. Although you could use it to create a dramatic ‘off-the-charts’ effect. p + annotate(&quot;rect&quot;, xmin = 1.75, xmax = 10, ymin = 1.25, ymax = 1.26, fill=&quot;black&quot;) + annotate(&quot;text&quot;, x=5, y=1.28, label=&quot;ligand&quot;, size=6) + coord_cartesian(ylim = c(0.98,1.15), clip = &#39;off&#39;) + theme(plot.margin = margin(t = 130, r = 0, b = 0, l = 0, unit = &quot;pt&quot;)) This concludes a couple examples of the annotate() function. Protocol 5 is a good example how these annotations can assist to build an informative data visualization. "],["complete-protocols.html", "Chapter 4 Complete protocols 4.1 Protocol 1 - Spectra of fluorescent proteins 4.2 Protocol 2 - A superplot of calcium concentrations 4.3 Protocol 3 - small multiples of time courses 4.4 Protocol 4 - Plotting data from a 96-wells experiment 4.5 Protocol 5 - A map of amino acids 4.6 Protocol 6 - Heatmap style visualization of timelapse data 4.7 Protocol 7 - Ridgeline plot 4.8 Protocol 8 - Plotting data in a 96-wells layout 4.9 Protocol 9 - A dose response curve with fit 4.10 Protocol 10 - Plotting data that was harvested with a Google form 4.11 Protocol 11 - Plotting time data from a Google form 4.12 Protocol 12 - Plotting grouped data 4.13 Protocol 13 - Plotting multiple plots side-by-side 4.14 Protocol 14 - Volcano plot 4.15 Protocol 15 - Timeline 4.16 Protocol 16 - Scatterplots and correlation 4.17 Protocol 17 - Animated plots of data from optogenetics experiments 4.18 Protocol 18 - Combining animated plots with a movie 4.19 Protocol 19 - Plotting ratiometric FRET data 4.20 Protocol 20 - On- and off-kinetics 4.21 Protocol 21 - A spiral plot 4.22 Protocol 22 - Frequencies of discrete data 4.23 Protocol 23 - Plotting multiple conditions side-by-side 4.24 Protocol 24 - Colored slopes plot 4.25 Protocol 25 - Colorblind friendly colors on a dark theme 4.26 Protocol 26 - Donut charts 4.27 Protocol 27 - Raincloud plot", " Chapter 4 Complete protocols This chapter showcases several complete protocols for different kinds of data visualizations. Each protocol starts with the raw data and ends with a publication quality plot. The data is available from Github in the ‘/data’ folder on this page. The github page holds a separate Rmarkdown file for each protocol. To reproduce the data visualizations you can either take the data and follow the instructions in the chapter. Alternatively you can download the Rmarkdown file (and the data) and run it step by step. The Rmarkdown files can also be used as a starting point to apply the same visualization to your own data. An overview of the data visualizations that are generated by the protocols: 4.1 Protocol 1 - Spectra of fluorescent proteins This protocol describes how you can turn a csv with spectral data that is obtained from FPbase.org into a plot of those spectra. First, we load the required package: library(tidyverse) For this data visualization, I selected several spectra from fluorescent proteins at FPbase.org: https://www.fpbase.org/spectra/?s=1746,6551,101,102,123,124,1604,1606&amp;showY=0&amp;showX=1&amp;showGrid=0&amp;areaFill=1&amp;logScale=0&amp;scaleEC=0&amp;scaleQY=0&amp;shareTooltip=1&amp;palette=wavelength The data was downloaded in CSV format (by clicking on the button in the lower right corner of the webpage) and renamed to ‘FPbase_Spectra_4FPs.csv’. We read the data from the CSV by using the read_csv() function. This function is part of the tidy verse and loads the data as a tibble. It also guesses type of data for each column. To hide that information, we use show_col_types = FALSE here. df_raw &lt;- read_csv(&quot;data/FPbase_Spectra_4FPs.csv&quot;, show_col_types = FALSE) Let’s briefly look at what we have loaded: glimpse(df_raw) Rows: 512 Columns: 9 $ Wavelength &lt;dbl&gt; 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310… $ `mTurquoise2 EM` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ `mTurquoise2 EX` &lt;dbl&gt; 0.2484, 0.2266, 0.2048, 0.1852, 0.1634, 0.1482, 0.132… $ `mNeonGreen EM` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ `mNeonGreen EX` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ `mScarlet-I EM` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ `mScarlet-I EX` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ `miRFP670 EM` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ `miRFP670 EX` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… The data needs conversion to a tidy format before plotting. Since we have a single continuous data column with Wavelength information that is used for the x-axis, it is excluded from the operation: df_1 &lt;- pivot_longer( df_raw, cols = -Wavelength, names_to = &quot;Sample&quot;, values_to = &quot;Intensity&quot; ) There are several rows that have NA values for Intensity and this is how to get rid of that: df_1 &lt;- df_1 %&gt;% drop_na(Intensity) The column ‘Sample’ has labels for the fluorescent protein and the type of spectrum. We can separate that column into two different columns that we name ‘Fluorescent Protein’ and ‘Spectrum’: df_1 &lt;- df_1 %&gt;% separate(Sample, c(&quot;Fluorescent Protein&quot;, &quot;Spectrum&quot;), sep = &quot; &quot;) Let’s do a first attempt and plot the data: ggplot( data = df_1, aes(x = Wavelength, y = Intensity, color = `Fluorescent Protein`) ) + geom_line(aes(linetype = Spectrum), size = 1) This looks pretty good already. Now let’s change the order of the fluorescent proteins to their order in the plot: df_1 &lt;- df_1 %&gt;% mutate(`Fluorescent Protein` = forcats::fct_relevel( `Fluorescent Protein`, c(&quot;mTurquoise2&quot;, &quot;mNeonGreen&quot;, &quot;mScarlet-I&quot;, &quot;miRFP670&quot;) )) The data is in the right shape now, so let’s save it: df_1 %&gt;% write.csv(&quot;protocol_1.csv&quot;, row.names=FALSE) We define the plot object and add an extra geometry, geom_area() to fill the area under the curves: p &lt;- ggplot( data = df_1, aes( x = Wavelength, y = Intensity, fill = `Fluorescent Protein` ) ) + geom_line(aes(linetype = Spectrum), size = 0.5, alpha = 0.5 ) + geom_area( aes(linetype = Spectrum), color = NA, position = &quot;identity&quot;, size = 1, alpha = 0.5 ) Let’s check the result: p Next, we set the limits of the axis and force the y-axis to start at 0 p &lt;- p + scale_y_continuous(expand = c(0, 0), limits = c(0, 1.1)) + scale_x_continuous(expand = c(0,0), limits = c(350, 810)) Add labels: p &lt;- p + labs( title = &quot;Spectra of Fluorescent Proteins&quot;, x = &quot;Wavelength [nm]&quot;, y = &quot;Normalized Intensity [a.u.]&quot;, caption = &quot;@joachimgoedhart\\n(based on data from FPbase.org)&quot;, tag = &quot;Protocol 1&quot; ) Modify the layout by adjusting the theme. Comments are used to explain effect of the individual lines of code: p &lt;- #Set text size p + theme_light(base_size = 14) + theme( plot.caption = element_text( color = &quot;grey80&quot;, hjust = 1 ), #Remove grid panel.grid.major = element_blank(), panel.grid.minor = element_blank(), #Set position of legend legend.position = &quot;top&quot;, legend.justification = &quot;left&quot; #Define the legend layout ) + guides( linetype = &quot;none&quot;, fill = guide_legend(title = NULL, label.position = &quot;right&quot;) ) p We are almost there, except that the colors of the plot do not match with the natural colors of the fluorescent proteins. Let’s fix that by defining a custom color palette. The order of the colors matches with the order of the fluorescent proteins that was defined earlier: custom_colors &lt;- c(&quot;blue&quot;, &quot;green&quot;, &quot;orange&quot;, &quot;red&quot;) To apply the custom colors to the filled area: p &lt;- p + scale_fill_manual(values = custom_colors) This is the result: p To save this plot as a PNG file: png(file=paste0(&quot;Protocol_01.png&quot;), width = 4000, height = 3000, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 4.2 Protocol 2 - A superplot of calcium concentrations This protocol is used to create a superplot which differentiates between technical and biological replicates. The concept of superplots has been reported by Lord and colleagues (Lord et al., 2021). We will use the data that was used to create figure 5e in a publication by van der Linden et al. (2021). The figure in the publication summarizes the data from all experiments and does not identify the biological replicates. Below, we will differentiate the biological replicates, by treating each batch of neutrophils as a biological replicate. We start by loading the required tidyverse package: library(tidyverse) We define the confidence level as 95%: Confidence_level &lt;- 0.95 The data is stored in an excel sheet and we read it, skipping the first 6 lines which contain comments: df_raw &lt;- readxl::read_excel(&#39;data/figure5.xlsx&#39;, skip=6) Let’s look at the data: head(df_raw) # A tibble: 6 × 7 `Experimental day` `Replicate no.` `Neutrophil no.` `Batch no. neutrophils` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 1 1 1 2 1 1 1 1 3 1 1 1 1 4 1 1 1 1 5 1 1 1 1 6 1 1 1 1 # ℹ 3 more variables: Stage &lt;chr&gt;, `dF/F0` &lt;chr&gt;, `Calcium (uM)` &lt;dbl&gt; The data is already in a tidy format. The column with ‘Stage’ has the four different conditions for which we will compare the data in the column ‘Calcium (uM)’. We change the name of the column ‘Batch no. neutrophils’ to ‘Replicate’ and make sure the different replicates are treated as factors (qualitative data): df_raw &lt;- df_raw %&gt;% mutate(Replicate = as.factor(`Batch no. neutrophils`)) Let’s look at the data, and identify the biological replicates, as suggested in the original publication on Superplot by (Lord et al., 2021). In this example a color code is used to label the replicates: ggplot(data=df_raw, aes(x=Stage)) + geom_jitter(data=df_raw, aes(x=Stage, y=`Calcium (uM)`, color=Replicate)) To display the statistics for the individual biological replicates, we define a new dataframe. To this end, we group the data for the different stages and biological replicates: df_summary &lt;- df_raw %&gt;% group_by(Stage, Replicate) %&gt;% summarise(n=n(), mean=mean(`Calcium (uM)`)) `summarise()` has grouped output by &#39;Stage&#39;. You can override using the `.groups` argument. Next, we use ‘df_summary’ which holds the averages of each biological replicate, and we calculate the statistics for the different conditions: df_summary_replicas &lt;- df_summary %&gt;% group_by(Stage) %&gt;% mutate(n_rep=n(), mean_rep=mean(mean), sd_rep = sd(mean)) %&gt;% mutate(sem = sd_rep / sqrt(n_rep - 1), `95%CI_lo` = mean_rep + qt((1-Confidence_level)/2, n_rep - 1) * sem, `95%CI_hi` = mean_rep - qt((1-Confidence_level)/2, n_rep - 1) * sem, NULL) The dataframe has the summary of the conditions and note that each condition has a summary of 4 biological replicates: head(df_summary_replicas) # A tibble: 6 × 10 # Groups: Stage [2] Stage Replicate n mean n_rep mean_rep sd_rep sem `95%CI_lo` &lt;chr&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 before 1 38 0.0305 4 0.0301 0.00414 0.00239 0.0225 2 before 2 67 0.0270 4 0.0301 0.00414 0.00239 0.0225 3 before 3 56 0.0358 4 0.0301 0.00414 0.00239 0.0225 4 before 4 55 0.0270 4 0.0301 0.00414 0.00239 0.0225 5 crawling 1 7 0.0296 4 0.0317 0.00339 0.00196 0.0255 6 crawling 2 29 0.0289 4 0.0317 0.00339 0.00196 0.0255 # ℹ 1 more variable: `95%CI_hi` &lt;dbl&gt; We can now add or ‘bind’ the data of ‘df_summary_replicas’ to the original dataframe ‘df’ and store this as a dataframe ‘df_2’: df_2 &lt;- df_raw %&gt;% left_join(df_summary_replicas, by = c(&quot;Stage&quot;,&quot;Replicate&quot;)) Let’s save this data: df_2 %&gt;% write.csv(&quot;protocol_2.csv&quot;, row.names=FALSE) Let’s first define a basic plot with all of the data for each stage shown as a violinplot: p &lt;- ggplot(data=df_2, aes(x=Stage)) + geom_violin(data=df_2, aes(x=Stage, y=`Calcium (uM)`), color=NA, fill=&quot;grey80&quot;) This is what it looks like: p We add the 95% confidence interval from the summary of the biological replicates as a line: p &lt;- p + geom_linerange(data = df_2, aes(ymin=`95%CI_lo`, ymax=`95%CI_hi`), size=1, alpha=0.8) And we add the mean value of each replicate as a dot. Here, the size of the dot is reflecting n: p &lt;- p + geom_point(data=df_2, aes(x=Stage, y=mean, size=n, fill=Replicate), shape=21, stroke = 1) The function scale_size_area() ensures that 0 is represented as an area of 0 and allows to to define that an n of 10,50 and 90 is shown in the legend: p &lt;- p + scale_size_area(breaks = c(10, 50, 90), max_size = 6) This is what that looks like: p Next, one of my favorite tweaks for discrete conditions is to rotate the plot 90 degrees. At the same time, the limits are defined. p &lt;- p + coord_flip(ylim = c(0.02,0.09)) + # This ensures correct order of conditions when plot is rotated 90 degrees scale_x_discrete(limits = rev) Rotation improves readability of the labels for the conditions, even when they are long. It also easier to read the different calcium levels: p To guide the interpretation, a line is added as a threshold of 0.06 µM (=60 nM): p &lt;- p + geom_hline(yintercept = 0.060, linetype=&#39;dotted&#39;) Adjusting the axis labels and adding a title and caption: p &lt;- p + labs( title = &quot;Calcium concentrations are less than 60 nM&quot;, subtitle = &quot;at different stages of transendothelial migration&quot;, x = &quot;Stage&quot;, y = &quot;Calcium [µM]&quot;, caption = &quot;@joachimgoedhart\\n(based on data from van der Linden, DOI: 10.1101/2021.06.21.449214)&quot;, tag = &quot;Protocol 2&quot; ) The layout it further optimized. The most tricky part is positioning of the label for the different conditions. It is placed right above the conditions, which I really like. However, getting this right involves a bit of trial and error and I recommend playing with the parameters to see how it affects the positioning. Something similar applies to the legend which is moved into the lower right corner of the plot, although this is eassier to accomplish. The comments explain the effect of the different lines: p &lt;- #Set text size p + theme_classic(base_size = 16) + theme( plot.caption = element_text( color = &quot;grey80&quot;, hjust = 1 ), #Set position of legend to lower right corner legend.position = c(0.88,0.15), #This line positions the label (&#39;title&#39;) of the conditions axis.title.y = element_text(vjust = 0.98, angle = 0, margin=margin(l=70)), #This line positions the names of the conditions #A negative margin is needed for aligning the y-axis &#39;title&#39; with the &#39;text&#39; axis.text.y = element_text(vjust = 0.5, hjust=1, angle = 0, margin=margin(l=-90, r=5)), #Move &#39;tag&#39;, so its position partially overlaps with the conditions plot.tag.position = c(0.06,0.99) ) + guides(fill = &quot;none&quot;, size = guide_legend(title = &#39;n per replicate&#39;, label.position = &quot;left&quot;) ) p To save the plot as a PNG file: png(file=paste0(&quot;Protocol_02.png&quot;), width = 4000, height = 3000, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 4.3 Protocol 3 - small multiples of time courses This protocol displays a number of different timecourses as ‘small multiples’. Small multiples, as the name suggests, displays many small plot separately as a stamp collection. By stressing the data, rather than the labels and grids, this can be a powerful visualization strategy. The data is taken from a publication by Arts et al. (2021) and we recreate figure panel 1F. The original figure is in small multiple format, but we tweak it a bit more to increase the focus on the data. Let’s first load the necessary package: library(tidyverse) The data comes from an excel file: df_raw &lt;- readxl::read_excel(&quot;data/Data_Arts_Circularity.xlsx&quot;) head(df_raw) # A tibble: 6 × 13 time `neutro 1` `neutro 2` `neutro 3` `neutro 4` `neutro 5` `neutro 6` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 0.53 0.54 0.55 0.59 0.58 0.34 2 10 0.44 0.6 0.5 0.4 0.54 0.4 3 20 0.33 0.55 0.64 0.3 0.48 0.41 4 30 0.35 0.54 0.69 0.28 0.52 0.32 5 40 0.41 0.53 0.57 0.23 0.44 0.29 6 50 0.32 0.4 0.4 0.26 0.43 0.25 # ℹ 6 more variables: `neutro 7` &lt;dbl&gt;, `neutro 8` &lt;dbl&gt;, `neutro 9` &lt;dbl&gt;, # `neutro 10` &lt;dbl&gt;, `neutro 11` &lt;dbl&gt;, `neutro 12` &lt;dbl&gt; It is in a wide format, so we need to make it tidy. The parameter that was measured over time is the ‘roundness’ of cells: df_3 &lt;- pivot_longer( df_raw, cols = -time, names_to = &quot;Cell&quot;, values_to = &quot;Roundness&quot; ) The data is in the right shape now, so let’s save it: df_3 %&gt;% write.csv(&quot;protocol_3.csv&quot;, row.names = FALSE) First we create a line plot of all the data: p &lt;- ggplot(df_3, aes(x=time, y=Roundness, group=Cell)) + geom_line() p With the facet_wrap() function, we turn this into a small multiple: p &lt;- p + facet_wrap(~Cell) p Set the limits of the axis and force the y-axis to start at 0 p &lt;- p + scale_y_continuous(expand = c(0, 0), limits = c(0, 1.0)) + scale_x_continuous(expand = c(0,0), limits = c(0, 300)) Use a minimal theme and remove the strips and grid to increase focus on the data: p &lt;- p + theme_minimal(base_size = 14) p &lt;- p + theme(strip.background = element_blank(), strip.text = element_blank(), plot.caption = element_text(color = &quot;grey80&quot;), #Remove grid panel.grid.major = element_blank(), panel.grid.minor = element_blank() ) p I do not like the repeated axis for the different plots. We can remove those: p &lt;- p + theme( #Remove axis labels axis.text = element_blank() ) p This is a very minimal plot, focusing entirely on the data. It may work well, but it is informative to add some information about the scaling of the x- and y-axis. To achieve this, I add lines to the lower left plot, which correspond to the data of ‘neutro 6’ (you can see this in the small multiple plot where each plot was labeled). I define a new dataframe with the x- and y-scale for ‘neutro 6’ to do just that: ann_line&lt;-data.frame(xmin=0,xmax=300,ymin=0,ymax=1, Cell=factor(&quot;neutro 6&quot;,levels=c(&quot;neutro 6&quot;))) ann_line xmin xmax ymin ymax Cell 1 0 300 0 1 neutro 6 This dataframe can now be used to draw two lines with geom_segment(): p &lt;- p + #Vertical line geom_segment(data=ann_line, aes(x=xmin,xend=xmin,y=ymin,yend=ymax), size=2, color=&#39;grey40&#39;) + #Horizontal line geom_segment(data=ann_line, aes(x=xmin,xend=xmax,y=ymin,yend=ymin), size=2, color=&#39;grey40&#39;) + NULL p The plot is now in black and white which gives it a strong contrast. We can make it a bit more soft and pleasant to look at by changing to shades of grey. Also, the labels of the axes are moved next to the lines: p &lt;- p + theme(panel.background = element_rect(fill=&#39;grey98&#39;, color=NA), panel.border = element_rect(color=&#39;grey90&#39;, fill=NA), axis.title.x = element_text(size=14, hjust = 0, color=&#39;grey40&#39;), axis.title.y = element_text(size=14, vjust = 0, hjust=0, angle = 90, color=&#39;grey40&#39;), ) Finally, we add a title, caption, and labels (and a scale in brackets): p &lt;- p + labs( title = &quot;Changes in the shape of migrating cells over time&quot;, x = &quot;Time [300s]&quot;, y = &quot;Circularity [0-1]&quot;, caption = &quot;@joachimgoedhart\\n(based on data from Arts et al., DOI: 10.3389/fimmu.2021.667213)&quot;, tag = &quot;Protocol 3&quot; ) p png(file=paste0(&quot;Protocol_03.png&quot;), width = 4000, height = 3000, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 4.4 Protocol 4 - Plotting data from a 96-wells experiment This protocol showcases some serious data wrangling and tidying. One reason is that the data is acquired with a 96-wells plate reader and the data is stored according to the layout of the plate. This makes total sense from a human perspective, but it is not well suited for data visualization. In addition, the plate is measured twice. One measurement is the luminescence from a luciferase and the other measurement is the luminescence from Renilla. The latter reading serves as a reference and therefore, the luciferase data is divided by the Renilla intensities. A final step before the data is visualized is a normalization to a control condition. The code that is shown here is also the basis for the plotXpress app that can be used to process and visualize the data. In fact, the data visualization is very close to the standard output of plotXpress and uses the same example data. We start by loading a package that we need: library(tidyverse) The measured data is read from an excel sheet. Note that this is the raw data that is stored by the software that operates the plate reader: df_raw &lt;- readxl::read_excel(&quot;data/DualLuc_example_data.xlsx&quot;, sheet = &quot;Results&quot;) New names: • `` -&gt; `...1` • `` -&gt; `...2` • `` -&gt; `...3` • `` -&gt; `...5` • `` -&gt; `...6` • `` -&gt; `...7` • `` -&gt; `...8` • `` -&gt; `...9` • `` -&gt; `...10` • `` -&gt; `...11` • `` -&gt; `...12` • `` -&gt; `...13` • `` -&gt; `...14` • `` -&gt; `...15` • `` -&gt; `...16` • `` -&gt; `...17` The experimental conditions for each well are stored in a separate CSV file, generated by the experimentalist that did the experiment: df_design &lt;- read.csv(&quot;data/Tidy_design.csv&quot;) head(df_design) Wells condition treatment1 treatment2 1 A01 - HEK - 2 B01 - HEK - 3 C01 - HEK - 4 D01 - HEK - 5 E01 - neuron - 6 F01 - neuron - You can see that the design file is tidy. In contrast the excel file with data is far from tidy. In the excel sheet, two ‘rectangles’ of cells define the data for the firefly &amp; renilla reads. The data is subset and converted to a vector firefly &lt;- df_raw[19:26,6:17] %&gt;% unlist(use.names = FALSE) renilla &lt;- df_raw[40:47,6:17] %&gt;% unlist(use.names = FALSE) Define a dataframe with wells column &lt;- rep(1:12, each=8) row &lt;- rep(LETTERS[1:8],12) For convenience, all numbers should consist of 2 digits and so we add a zero that precedes the single digit numbers: x0 &lt;- str_pad(column, 2, pad = &quot;0&quot;) To generate a unique index for each row in the dataframe, we define ‘Wells’, which combines the row and column index: Wells &lt;- paste0(row,x0) Next, we create a dataframe that holds the data of individual columns, rows and wells: df_plate &lt;- data.frame(column,row,Wells) head(df_plate) column row Wells 1 1 A A01 2 1 B B01 3 1 C C01 4 1 D D01 5 1 E E01 6 1 F F01 Add to df_plate the vectors with data from firefly and renilla reads df_4 &lt;- data.frame(df_plate,firefly,renilla) Merge the design with the data, based on the well ID - left_join() is used to add only data for wells that are listed in the design dataframe df_4 &lt;- left_join(df_design, df_4, by=&#39;Wells&#39;) head(df_4) Wells condition treatment1 treatment2 column row firefly renilla 1 A01 - HEK - 1 A 2010 2391540 2 B01 - HEK - 1 B 3210 2391639 3 C01 - HEK - 1 C 1965 2390991 4 D01 - HEK - 1 D 2381 2391774 5 E01 - neuron - 1 E 1292 269021 6 F01 - neuron - 1 F 991 268918 Calculate the relative expression from the firefly/renilla ratio df_4 &lt;- df_4 %&gt;% mutate(expression=firefly/renilla) Take all control conditions and calculate the average value df_norm &lt;- df_4 %&gt;% filter(condition == &quot;-&quot;) %&gt;% group_by(treatment1,treatment2) %&gt;% summarise(mean=mean(expression)) `summarise()` has grouped output by &#39;treatment1&#39;. You can override using the `.groups` argument. Combine the mean values (needed for normalization) values with the df_4 dataframe df_4 &lt;- df_4 %&gt;% full_join(df_norm, by=c(&quot;treatment1&quot;,&quot;treatment2&quot;)) %&gt;% na.omit(condition) Calculate the Fold Change by normalizing all measurements against the control (-) df_4 &lt;- df_4 %&gt;% mutate(`Fold Change` = expression/mean) The result is a dataframe that holds all the necessary data: head(df_4) Wells condition treatment1 treatment2 column row firefly renilla expression 1 A01 - HEK - 1 A 2010 2391540 0.0008404626 2 B01 - HEK - 1 B 3210 2391639 0.0013421758 3 C01 - HEK - 1 C 1965 2390991 0.0008218350 4 D01 - HEK - 1 D 2381 2391774 0.0009954954 5 E01 - neuron - 1 E 1292 269021 0.0048025991 6 F01 - neuron - 1 F 991 268918 0.0036851382 mean Fold Change 1 0.0009999922 0.8404692 2 0.0009999922 1.3421863 3 0.0009999922 0.8218414 4 0.0009999922 0.9955032 5 0.0040858977 1.1754085 6 0.0040858977 0.9019164 The data is in the right shape now, so let’s save it: df_4 %&gt;% write.csv(&quot;protocol_4.csv&quot;, row.names = FALSE) Based on the dataframe, we can create a plot with jittered dots that show the data: p &lt;- ggplot(df_4, aes(x=condition, y=`Fold Change`)) + geom_jitter(width = 0.2, alpha=0.5, size=3) To splot the graphs based on treatment1 (vertical) and treatment2 (horizontal) we use the facet_grid() function: p &lt;- p + facet_grid(treatment1~treatment2) We add a horizontal line for mean value: p &lt;- p + stat_summary(fun.min=mean, fun.max=mean, geom=&#39;errorbar&#39;, width=0.6, size=0.5) Add labels: p &lt;- p + labs( title = &quot;Effect of DNA sequences on reporter levels under different conditions&quot;, subtitle = &quot;The expression level was determined by a dual luciferase assay\\n and the values were normalized to a control with no DNA sequence (-)&quot;, x = &quot;DNA Sequence&quot;, y = &quot;Fold change of the reporter relative to the control (-)&quot;, caption = &quot;@joachimgoedhart\\n(based on data from Brandorff et al., DOI: 10.1101/2021.07.08.451595)&quot;, tag = &quot;Protocol 4&quot; ) Set the theme and font size: p &lt;- p + theme_light(base_size = 14) Format the facet labels (strips) and the caption + subtitle p &lt;- p + theme(strip.background = element_rect(fill=&quot;grey90&quot;, color=&quot;grey50&quot;), strip.text = element_text(color=&quot;grey50&quot;), plot.caption = element_text(color = &quot;grey80&quot;), plot.subtitle = element_text(color = &quot;grey50&quot;, face = &quot;italic&quot;), #Remove the grid panel.grid.major = element_blank(), panel.grid.minor = element_blank() ) Let’s look at the result: p To save the plot as a png file: png(file=paste0(&quot;Protocol_04.png&quot;), width = 4000, height = 3000, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 4.5 Protocol 5 - A map of amino acids This data visualization plots the position of amino acids in a given protein. It is inspired by figure 2A of the paper by Basu et al. (2020) Let’s load the tidyverse package: library(tidyverse) First, we define a vector with the 20 amino acids and the order in which we plot them. The amino acids are grouped as hydrophobic (G,A,V,C,P,I,L,M,F,W), hydrophilic (S,T,Y,N,Q), acidic (D,E) and basic (R,H,K). amino_acid_ordered &lt;- strsplit(&quot;GAVCPILMFWSTYNQDERHK&quot;,&quot;&quot;) %&gt;% unlist() amino_acid_ordered [1] &quot;G&quot; &quot;A&quot; &quot;V&quot; &quot;C&quot; &quot;P&quot; &quot;I&quot; &quot;L&quot; &quot;M&quot; &quot;F&quot; &quot;W&quot; &quot;S&quot; &quot;T&quot; &quot;Y&quot; &quot;N&quot; &quot;Q&quot; &quot;D&quot; &quot;E&quot; &quot;R&quot; &quot;H&quot; [20] &quot;K&quot; The protein sequence that we will use is the Homo sapiens Homeobox protein Hox-D13: protein &lt;- c(&quot;MSRAGSWDMDGLRADGGGAGGAPASSSSSSVAAAAASGQCRGFLSAPVFAGTHSGRAAAA AAAAAAAAAAASGFAYPGTSERTGSSSSSSSSAVVAARPEAPPAKECPAPTPAAAAAAPP SAPALGYGYHFGNGYYSCRMSHGVGLQQNALKSSPHASLGGFPVEKYMDVSGLASSSVPA NEVPARAKEVSFYQGYTSPYQHVPGYIDMVSTFGSGEPRHEAYISMEGYQSWTLANGWNS QVYCTKDQPQGSHFWKSSFPGDVALNQPDMCVYRRGRKKRVPYTKLQLKELENEYAINKF INKDKRRRISAATNLSERQVTIWFQNRRVKDKKIVSKLKDTVS&quot;) The protein sequence may contain end-of-line characters “” after copy pasting and we need to remove these. The gsub() function can be used: gsub(&quot;\\n&quot;, &quot;&quot;, protein) [1] &quot;MSRAGSWDMDGLRADGGGAGGAPASSSSSSVAAAAASGQCRGFLSAPVFAGTHSGRAAAAAAAAAAAAAAASGFAYPGTSERTGSSSSSSSSAVVAARPEAPPAKECPAPTPAAAAAAPPSAPALGYGYHFGNGYYSCRMSHGVGLQQNALKSSPHASLGGFPVEKYMDVSGLASSSVPANEVPARAKEVSFYQGYTSPYQHVPGYIDMVSTFGSGEPRHEAYISMEGYQSWTLANGWNSQVYCTKDQPQGSHFWKSSFPGDVALNQPDMCVYRRGRKKRVPYTKLQLKELENEYAINKFINKDKRRRISAATNLSERQVTIWFQNRRVKDKKIVSKLKDTVS&quot; But we can also use str_replace_all() from the {tidyverse} package: protein &lt;- protein %&gt;% str_replace_all(&quot;\\n&quot;, &quot;&quot;) Next, the protein sequence is split into single characters and we assign this vector to aa: aa &lt;- strsplit(protein, &quot;&quot;) %&gt;% unlist() We generate a dataframe with a column with the amino acids and a column that defines their position: df_5 &lt;- data.frame(aa=aa, position=1:length(aa)) Now we reorder the data frame to the order of the amino acids that we defined earlier in the vector amino_acid_ordered: df_5 &lt;- df_5 %&gt;% mutate(aa = fct_relevel(aa, amino_acid_ordered)) The basic plot shows a black tile for each amino acid. Note that the y-axis order is defined by the vector amino_acid_ordered, but it needs to be reverted to order the amino acids from top to bottom along the y-axis (which is naturally starts at the bottom it corresponds to the origin). The data is in the right shape now, so let’s save it: df_5 %&gt;% write.csv(&quot;protocol_5.csv&quot;, row.names = FALSE) p &lt;- ggplot() + geom_tile(data=df_5, aes(x=position, y=aa)) + scale_y_discrete(limits = rev(amino_acid_ordered)) p Set the theme to classic, to get rid off the ‘frame’ around the plot and the grid. p &lt;- p+theme_classic(base_size = 16) For each of the four classes of amino acids we can define a box with a color that indicates the class. For example, there are three basic residues that will have a rectangle filled with blue in the background. The amino acids are factors, but we need numbers to define the coordinates for the rectangle. In a plot with a factors (here on the y-axis) their position is defined by a (non visible) natural number. Therefore we can define a box with the function annotate() for the first residue with y-coordinates ymin=0.5 and ymax=1.5: p + annotate(geom = &quot;rect&quot;, xmin = -Inf, ymin = 0.5, xmax = Inf, ymax=1.5, fill=&#39;blue&#39;, alpha=0.4) In this way, we define four colored rectangles that reflect the different amino acids categories; blue=basic, red=acidic, yellow=hydrophilic, grey=hydrophobic: p &lt;- p + annotate(geom = &quot;rect&quot;, xmin = -Inf, ymin = 0.5, xmax = Inf, ymax=3.5, fill=&#39;blue&#39;, alpha=0.15) p &lt;- p + annotate(geom = &quot;rect&quot;, xmin = -Inf, ymin = 3.5, xmax = Inf, ymax=5.5, fill=&#39;red&#39;, alpha=0.15) p &lt;- p + annotate(geom = &quot;rect&quot;, xmin = -Inf, ymin = 5.5, xmax = Inf, ymax=10.5, fill=&#39;yellow&#39;, alpha=0.15) p &lt;- p + annotate(geom = &quot;rect&quot;, xmin = -Inf, ymin = 10.5, xmax = Inf, ymax=20.5, fill=&#39;black&#39;, alpha=0.15) Let’s look at the result: p Adjusting the axis labels and adding a title and caption: p &lt;- p + labs( title = &quot;Mapping the amino acid positions of HOXD13&quot;, subtitle = &quot;shows a high abundance of alanines in the IDR&quot;, y = &quot;Amino acid&quot;, caption = &quot;@joachimgoedhart | based on data from Basu et al, DOI: 10.1016/j.cell.2020.04.018&quot;, tag = &quot;Protocol 5&quot; ) And a final tweak of the label style and location: p &lt;- p + theme( plot.caption = element_text( color = &quot;grey80&quot;, hjust = 1 )) In the original paper, a region of the protein is annotated as an ‘intrinsically disordered region’ abbreviated as IDR. Here, we we use the annotate() function to add a rectangle and a label to the top of the plot: p &lt;- p + annotate(&quot;rect&quot;, xmin=0, xmax=118, ymin=21, ymax=22, fill=&#39;darkblue&#39;) + annotate(&quot;text&quot;, x=59, y=23, alpha=1, color=&#39;darkblue&#39;, size=4,label=&#39;IDR&#39;) p To avoid clipping of the new label by the subtitle of the plot: p &lt;- p + coord_cartesian(clip = &#39;off&#39;) p The subtitle is quite close to the IDR label. Let’s give the subtitle a bit more room, by adding a margin at the bottom of the subtitle. This can be done with the theme() function to style the subtitle: p &lt;- p + theme(plot.subtitle = element_text(margin = margin(b=20))) p Finally we can save the plot: png(file=paste0(&quot;Protocol_05.png&quot;), width = 4000, height = 3000, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 4.6 Protocol 6 - Heatmap style visualization of timelapse data Lineplots are typically used to plot data from timeseries (also known as longitudinal data). However, in case of many samples/objects, this may result in a cluttered data visualization. To solve this, the same data can be presented as a heatmap, where every row is an object and the response is coded as a color. A downside is that it is less quantitative as it is difficult to ‘read’ the numbers from this kind of visualization. Still, it is a powerful visualization to plot a lot of data and show its dynamics and heterogeneity. The data and visualization is originally published by Chavez-Abiega et al. (2021) The multipurpose {tidyverse} package is used for data wrangling and plotting: library(tidyverse) A CSV file with the data is loaded. Since the file is a couple of Megabytes, we use the faster fread() function from the package data.table: library(data.table) Attaching package: &#39;data.table&#39; The following objects are masked from &#39;package:lubridate&#39;: hour, isoweek, mday, minute, month, quarter, second, wday, week, yday, year The following objects are masked from &#39;package:dplyr&#39;: between, first, last The following object is masked from &#39;package:purrr&#39;: transpose df_S1P &lt;- data.table::fread(&quot;data/ERK_repsonse_to_S1P.csv&quot;) head(df_S1P) Time_in_min Ligand Unique_Object CN_ERK Slide &lt;num&gt; &lt;char&gt; &lt;int&gt; &lt;num&gt; &lt;char&gt; 1: 2.5 S1P 100 0.3941 P1 2: 3.0 S1P 100 0.3921 P1 3: 3.5 S1P 100 0.3900 P1 4: 4.0 S1P 100 0.3880 P1 5: 4.5 S1P 100 0.3859 P1 6: 5.0 S1P 100 0.3839 P1 The column ‘CN_ERK’ has the data on the activity that we will plot over time. Each number in the ‘Unique_Object’ column reflects an individual cell measurement and so we can use that to group the measurements using group_by(Unique_Object). We subtract the average baseline activity from each trace by subtracting the data acquired at the first 5 timepoints: CN_ERK[1:5]. The data is stored in a new column with normalized ERK activity data ‘ERKn’: df_sub &lt;- df_S1P %&gt;% group_by(Unique_Object) %&gt;% arrange(Time_in_min) %&gt;% mutate(ERKn=CN_ERK-mean(CN_ERK[1:5])) %&gt;% ungroup() Around Timepoint 23 (minutes), the ligand was added. To set this time point to zero, we subtract a value of 23 (minutes) from each point: df_sub$Time_in_min &lt;- df_sub$Time_in_min-23 The column ‘Unique_Object’ that identifies the individual cells contains natural numbers, but these need to be treated as qualitative data. Therefore, we change the data type in this column to a factor with as.factor(): df_sub &lt;- df_sub %&gt;% mutate(Unique_Object=as.factor(Unique_Object)) To order objects, we need to order ‘Unique_Object’ according to something. That something can be the maximum value of the Erk activity: df_sub &lt;- df_sub %&gt;% mutate(Unique_Object = fct_reorder(Unique_Object, ERKn, max)) Plot the data in heatmap style. We use theme_void here to focus only on the data and we will deal with styling of the axes and labels later: ggplot(df_sub, aes(x=Time_in_min, y=Unique_Object,fill=ERKn)) + geom_tile() + theme_void()+ scale_fill_viridis_c() Can we also sort the data based on something else? Definitely, but it requires a bit of understanding of functions. Previously, we used the maximum value. This is defined by the function max, which takes the maxium value from a vector of numbers. Let’s look at an artificial example: x &lt;- c(1,3,4,5,6,7,9) max(x) [1] 9 Other functions that take a vector as input and return a single value as output can be used. Other existing examples are mean(), sum() and min(). We can also define a function: second &lt;- function(x) {x[2]} second(x) [1] 3 one_but_last &lt;- function(x) {x[length(x)-1]} one_but_last(x) [1] 7 We can use the new function to sort the dataframe: df_sub &lt;- df_sub %&gt;% mutate(Unique_Object = fct_reorder(Unique_Object, ERKn, one_but_last)) ggplot(df_sub, aes(x=Time_in_min, y=Unique_Object,fill=ERKn)) + geom_tile() + theme_void()+ scale_fill_viridis_c() If we want to sort on the sum of the top five values we can define a function: top_five &lt;- function(x) {sum(tail(sort(x),5))} But we can also directly implement the function in the fct_reorder() function: df_6 &lt;- df_sub %&gt;% mutate(Unique_Object = fct_reorder(Unique_Object, ERKn, function(x) {sum(tail(sort(x),5))})) Let’s save this data: df_6 %&gt;% write.csv(&quot;protocol_6.csv&quot;, row.names=FALSE) p &lt;- ggplot(df_6, aes(x=Time_in_min, y=Unique_Object,fill=ERKn)) + geom_tile() + # scale_x_continuous(breaks=seq(0,60, by=15), labels=seq(0,60, by=15), limits=c(-8,60)) + scale_fill_viridis_c(na.value=&quot;black&quot;, limits = range(-0.1,1.5)) Let’s look at the plot p Add labels: p &lt;- p + labs( title = &quot;ERK activity is increased by S1P&quot;, subtitle = &quot;and varies strongly between cells&quot;, x = &quot;Time after addition of S1P (min)&quot;, y = &quot;Cells sorted according to response&quot;, caption = &quot;@joachimgoedhart | data from Chavez-Abiega, DOI: 10.1101/2021.07.27.453948&quot;, tag = &quot;Protocol 6&quot;, fill= &quot;ERK C/N ratio&quot; ) The theme_void() would be close to what we want as a theme, but I prefer to start from theme_light and remove the redundant features (grids and y-axis labels): p &lt;- p + theme_light(base_size = 14) + theme(plot.caption = element_text(color = &quot;grey80&quot;, hjust = 2.0), plot.title = element_text(hjust = 0.1, margin = margin(t=10)), plot.subtitle = element_text(hjust = 0.1, margin = margin(t=2, b=5)), # Remove background panel.background = element_blank(), # Remove borders panel.border = element_blank(), # Remove grid panel.grid.major = element_blank(), panel.grid.minor = element_blank(), # Remove text of the y-axis axis.text.y = element_blank(), # Remove ticks on y-axis axis.ticks.y = element_blank(), # Remove label of y-axis axis.title.y = element_blank(), # Make x-axis ticks more pronounced axis.ticks = element_line(colour = &quot;black&quot;) ) p The caption does not look good in this plot, but it has been optimized to look good in the saved PNG. To get a proper aligned caption in the Rmd you may need to optimize the hjust value in theme((plot.caption = element_text())) To save the plot as PNG: png(file=paste0(&quot;Protocol_06.png&quot;), width = 4000, height = 3000, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 4.7 Protocol 7 - Ridgeline plot In protocol 6 we have seen how time traces can be visualized with a heatmap style data visualization. The advantage is that huge amounts of data can be simultaneously visualized. However, it is difficult to relate the color to numbers. As such the color coded values in a heatmap give a qualitative view of heterogeneity. In this protocol, we use a method that allows more data than an ordinary lineplot, but keeps the advantage of this plot, i.e. allowing a quantitative comparison. To do this, an offset between curves is introduced. A package to this directly in ggplot2 is available ({ggridges}) but it may not work well when the values show a negative and positive deviation from baseline values (usually the baseline is zero). In addition, we have more flexibility (and insight) when we do this ourselves. Load the {tidyverse} package and the data from a CSV file: require(tidyverse) df1 &lt;- read.csv(&quot;data/Rac_S1P.csv&quot;) head(df1) Time Cell.1 Cell.2 Cell.3 Cell.4 Cell.5 Cell.6 1 0.0000000 1.0012160 1.0026460 1.0022090 0.9917870 0.9935569 0.9961453 2 0.1666667 0.9994997 0.9928106 0.9997658 0.9975348 1.0018910 1.0039790 3 0.3333333 0.9908362 0.9964057 0.9905094 0.9946743 0.9961497 0.9953369 4 0.5000000 0.9991967 0.9972504 0.9972806 1.0074250 1.0060510 1.0062390 5 0.6666667 1.0093450 1.0109910 1.0103590 1.0084080 1.0022130 0.9982496 6 0.8333333 0.9941078 0.9940830 0.9990720 1.0181230 1.0110220 1.0139400 Cell.7 Cell.8 Cell.9 Cell.10 Cell.11 Cell.12 Cell.13 1 0.9964277 1.0006770 0.9999106 1.0043270 1.0086590 1.0074670 0.9875549 2 1.0098780 1.0015050 1.0014380 0.9938114 0.9791933 0.9925425 1.0106440 3 0.9917276 1.0000020 0.9997350 1.0049370 1.0250660 1.0079290 1.0062720 4 1.0039710 1.0046840 0.9976190 0.9971925 1.0008340 0.9975954 1.0050230 5 0.9979655 0.9930890 1.0012990 0.9997435 0.9865567 0.9944269 0.9907017 6 1.0087770 0.9916067 1.0025310 0.9919835 0.9930975 1.0012420 1.0025800 Cell.14 Cell.15 Cell.16 Cell.17 Cell.18 Cell.19 Cell.20 1 0.9941667 0.9931722 0.9918481 0.9973316 0.9976466 0.9982940 0.9954924 2 1.0024420 0.9980425 0.9974822 0.9976164 0.9881339 1.0122250 1.0132220 3 1.0096800 1.0074450 1.0073690 0.9980090 0.9980346 0.9923310 0.9891400 4 1.0007800 0.9995486 1.0018860 1.0064760 1.0073410 0.9959588 0.9984370 5 0.9930816 1.0018040 1.0014300 1.0006490 1.0088410 1.0013600 1.0040700 6 1.0117190 1.0082190 1.0092750 1.0018410 0.9976196 1.0008680 1.0013100 Cell.21 Cell.22 Cell.23 Cell.24 Cell.25 Cell.26 Cell.27 1 1.0024980 0.9986917 0.9987728 0.9988987 1.0019450 0.9954534 1.0025060 2 1.0022340 1.0023420 0.9963444 0.9969288 0.9990684 0.9973828 0.9944580 3 1.0003150 1.0014660 0.9985501 0.9983435 0.9944611 0.9987581 0.9935282 4 0.9969226 0.9956030 1.0020300 1.0063250 1.0036700 1.0056470 1.0065540 5 0.9980916 1.0018990 1.0042990 0.9995825 1.0009020 1.0028340 1.0031070 6 0.9993777 1.0041500 0.9984087 1.0016190 1.0007090 1.0022360 1.0007950 Cell.28 Cell.29 Cell.30 Cell.31 Cell.32 1 0.9957569 0.9852318 1.0007450 0.9927866 0.9871355 2 0.9952635 0.9931840 1.0030060 0.9977890 1.0028110 3 0.9976221 1.0010230 0.9998598 1.0029210 0.9985340 4 1.0068090 1.0139990 0.9968880 1.0026060 1.0039670 5 1.0046560 1.0070850 0.9995161 1.0039100 1.0076550 6 1.0073290 1.0128910 1.0000250 1.0025260 0.9998251 The data is not tidy, so it needs to be re-arranged: df_tidy &lt;- pivot_longer(df1, cols = -c(Time), names_to = &quot;Cell&quot;, values_to = &quot;Activity&quot;) head(df_tidy) # A tibble: 6 × 3 Time Cell Activity &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 0 Cell.1 1.00 2 0 Cell.2 1.00 3 0 Cell.3 1.00 4 0 Cell.4 0.992 5 0 Cell.5 0.994 6 0 Cell.6 0.996 In the next step, we create a new dataframe ‘df_rank’ to order the traces. We group the data by ‘Cell’ and extract the data from a specified time window with filter(). The filtered data is used to integrate the activity by using the function sum(). This summed value is used to generate a rank, ranging from 0 to 1: df_rank &lt;- df_tidy %&gt;% group_by(Cell) %&gt;% filter(Time&gt;=2 &amp; Time &lt;=10) %&gt;% summarise(amplitude=sum(Activity)) %&gt;% mutate(rank=percent_rank(amplitude)) head(df_rank) # A tibble: 6 × 3 Cell amplitude rank &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Cell.1 50.3 0.742 2 Cell.10 50.9 0.806 3 Cell.11 49.8 0.645 4 Cell.12 50.3 0.774 5 Cell.13 49.6 0.548 6 Cell.14 49.4 0.484 We can add the rank information from ‘df_rank’ to the ‘df_tidy’ dataframe: df_7 &lt;- as.data.frame(full_join(df_tidy, df_rank,by=&quot;Cell&quot;)) This data is saved: df_7 %&gt;% write.csv(&quot;protocol_7.csv&quot;, row.names=FALSE) Let’s make a lineplot of this data and use the rank to shift the data plotted on the y-axis: ggplot(df_7, aes(x=Time, y=Activity+(rank*1), group=Cell, height = rank)) + geom_line(alpha=0.5, size=0.5) We can use the rank to shift the plot also in the horizontal direction: p &lt;- ggplot(df_7, aes(x=Time+(rank*10), y=(Activity+(rank*1)), group=Cell)) + geom_line(alpha=0.5, size=0.5) p Add labels: p &lt;- p + labs( title = &quot;Activities measured in single cells over time&quot;, subtitle = &quot;Sorting of the cells is based on integrated activity&quot;, x = &quot;Time [min]&quot;, y = &quot;Activity [arbitrary units]&quot;, caption = &quot;@joachimgoedhart | data from Reinhard et al; doi: 10.1091/mbc.E17-03-0136&quot;, tag = &quot;Protocol 7&quot; ) If we would like to use color, this would be a way to do that: p + geom_line(aes(color=as.factor(rank))) + theme(legend.position = &quot;none&quot;) Although it looks flashy, we do not really need color here. So we stick to black and white and make some adjustments to the layout by tweaking the theme settings. To remove the grid and show the axis: p &lt;- p + theme_classic(base_size = 16) p &lt;- p + theme(panel.grid.major = element_blank(), plot.caption = element_text(color = &quot;grey80&quot;), panel.grid.minor = element_blank(), NULL) p To give it more of a 3D feel we can add a third axis by defining a line: p &lt;- p + annotate(geom = &quot;segment&quot;, x=0,y=1,xend=10,yend=2, size=1) The next step is to remove the original x-axis, which is a bit too long and also replace that with a line that runs until 20 (minutes): p &lt;- p + theme(axis.line.x = element_blank(), axis.title.x = element_text(hjust = 0.3) ) + annotate(geom = &quot;segment&quot;, x=0,y=1,xend=20,yend=1, size=1) p Finally, we can use scale_x_continuous() to improve two aspects of the x-axis. First, the labels run up to 30, but we can set the scale to 0-20 with breaks = c(0,20). Second, the default in ggplot is add a bit of margin to the plot, that’s why the x-axis does not touch the y-axis. This can be solved by using expand = c(0, 0): We can manually add two p &lt;- p + scale_x_continuous(breaks = c(0,10,20), expand = c(0, 0)) p To save the plot as PNG: png(file=paste0(&quot;Protocol_07.png&quot;), width = 4000, height = 3000, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 4.8 Protocol 8 - Plotting data in a 96-wells layout This protocol displays data from a 96-wells plate in the same format as the plate. This data visualization of 96-well plate data is also used in the plotXpress app. We start by loading a package that we need: library(tidyverse) A good example of data wrangling related to 96-wells data is given in protocol 4. Here we start from the tidy data that was the result of data tiyding in the section Data in 96-wells format: df_8 &lt;- read.csv(&quot;data/df_tidy_wells.csv&quot;) head(df_8) X column row Well Intensity 1 1 1 A A1 2010 2 2 1 B B1 3210 3 3 1 C C1 1965 4 4 1 D D1 2381 5 5 1 E E1 1292 6 6 1 F F1 991 We can construct a basic plot where each well is represented by a large dot and the color used to fill the dot represents the intensity. This is much like a heatmap, but it uses a dot instead of a tile: ggplot(data=df_8, aes(x=column, y=row)) + geom_point(aes(color=Intensity), size=10) There is a couple of things that need to be fixed. First, we invert the row names, to put the row ‘A’ at the top. In addition, the column names are depicted as factors (this can also be achieved by mutating the dataframe): p &lt;- ggplot(data=df_8, aes(x=as.factor(column), y=fct_rev(row))) + geom_point(aes(color=Intensity), size=10) p Let’s change the color palette that displays intensity to viridis. The numbers in the legend are high and that is why they are difficult to read quickly. So we use the scales package to use the scientific notation: p &lt;- p + scale_x_discrete(position = &quot;top&quot;) + scale_color_viridis_c(label = scales::scientific) p In my opinion, the scientific notation does not work well, so let’s try to use commas as a thousands separator: p &lt;- p + scale_x_discrete(position = &quot;top&quot;) + scale_color_viridis_c(label = scales::comma) Scale for x is already present. Adding another scale for x, which will replace the existing scale. Scale for colour is already present. Adding another scale for colour, which will replace the existing scale. p This is a bit better, but the numbers need to be right aligned: p &lt;- p + theme(legend.text.align = 1) p This looks better, now add the labels (and we skip the labels for the x- and y-axis): p &lt;- p + labs( title = &quot;Readings from a 96-wells plate&quot;, subtitle = NULL, x = NULL, y = NULL, caption = &quot;\\n@joachimgoedhart\\nbased on data from Brandorff et al., DOI: 10.1101/2021.07.08.451595&quot;, color= &#39;Intensity [a.u.]&#39;, tag = &quot;Protocol 8&quot; ) p Set the theme and font size: p &lt;- p + theme_light(base_size = 14) Note that this overrides any previously defined modifications of the theme() function, such as the alignment of the legend labels. So we need to define this again. p &lt;- p + theme(legend.text.align = 1) We also adjust the theme settings for the other elements: p &lt;- p + theme(plot.caption = element_text(color = &quot;grey80&quot;), plot.subtitle = element_text(color = &quot;grey50&quot;, face = &quot;italic&quot;), #Remove the grid panel.grid.major = element_blank(), panel.grid.minor = element_blank() ) Let’s look at the result: p To save the plot as a png file: png(file=paste0(&quot;Protocol_08.png&quot;), width = 3000, height = 2000, units = &quot;px&quot;, res = 400) p + coord_fixed() dev.off() quartz_off_screen 2 4.9 Protocol 9 - A dose response curve with fit In this protocol we visualize the data that was used to make a ‘dose reponse curve’. The data consists of some response that is measured at different concentrations of a compound that induces the effect (agonist). This is a typical experiment in pharmacology to understand the relation between the dose and the response. One of the relevant parameters is the ‘Half maximal effective concentration’ abbreviated as EC50. This is the concentration at which 50% of the maximal response is measured and we obtain this value by curve fitting. In this protocol we use data acquired from single cells from different biological replicates. We generate a ‘superplot’ (Lord et al., 2021) to distuinguish technical and sample replicates. More details about the experimental approach are published by Chavez-Abiega et al. (2022). We start by loading the {tidyverse} package: library(tidyverse) Read the data: df_DRC &lt;- read.csv(&quot;data/DRC_Histamine.csv&quot;, stringsAsFactors = TRUE) head(df_DRC) X Unique_Unique_Object Experiment Aktnac ERKnac Condition Concen 1 1 12 20200122 P1 0.0558 -0.2106 PTx 0 2 2 15 20200122 P1 0.2458 -0.1424 PTx 0 3 3 16 20200122 P1 0.0412 -0.2493 PTx 0 4 4 22 20200122 P1 -0.1194 -0.1980 PTx 0 5 5 23 20200122 P1 0.1838 -0.1540 PTx 0 6 6 25 20200122 P1 0.3291 -0.7078 PTx 0 The concentration is listed in the column ‘Concen’, the measured response is in the column ‘ERKnac’ and the column ‘Experiment’ identifies the replicates. Let’s rename the columns for easier identification of what they represent: df_DRC &lt;- df_DRC %&gt;% rename(Concentration=Concen, Response=ERKnac) head(df_DRC) X Unique_Unique_Object Experiment Aktnac Response Condition Concentration 1 1 12 20200122 P1 0.0558 -0.2106 PTx 0 2 2 15 20200122 P1 0.2458 -0.1424 PTx 0 3 3 16 20200122 P1 0.0412 -0.2493 PTx 0 4 4 22 20200122 P1 -0.1194 -0.1980 PTx 0 5 5 23 20200122 P1 0.1838 -0.1540 PTx 0 6 6 25 20200122 P1 0.3291 -0.7078 PTx 0 To change the column ‘Experiment’ from text into a number that represents the replicate, we can convert it using as.numeric(). Since we need these numbers as qualitative data (a label), we convert the numbers to a factor with as.factor(): df_DRC &lt;- df_DRC %&gt;% mutate(Replicate=as.factor(as.integer(Experiment))) head(df_DRC) X Unique_Unique_Object Experiment Aktnac Response Condition Concentration 1 1 12 20200122 P1 0.0558 -0.2106 PTx 0 2 2 15 20200122 P1 0.2458 -0.1424 PTx 0 3 3 16 20200122 P1 0.0412 -0.2493 PTx 0 4 4 22 20200122 P1 -0.1194 -0.1980 PTx 0 5 5 23 20200122 P1 0.1838 -0.1540 PTx 0 6 6 25 20200122 P1 0.3291 -0.7078 PTx 0 Replicate 1 1 2 1 3 1 4 1 5 1 6 1 The range of concentrations at which the compound is examined spans a few orders of magnitude and therefore a log scale is used to display the concentrations. The minimal response is usually measured at a concentration of 0, but the logarithm of 0 is undefined. Therefore, plotting 0 on a logscale will give an error. The logarithm of 0 can be approached by minus infinity. Therefore, we convert the concentration of 0 to a low value, in this case 0.01: df_DRC &lt;- df_DRC %&gt;% mutate(Concentration = ifelse((Concentration == 0), yes = 0.01, no = Concentration) ) Next we take the ‘Response’ and calculate the average per concentration and for each biological replicate and store that information in a new dataframe: df_summary &lt;- df_DRC %&gt;% group_by(Concentration, Replicate) %&gt;% summarise(mean_Response=mean(Response)) `summarise()` has grouped output by &#39;Concentration&#39;. You can override using the `.groups` argument. We can define a plot that shows the data with geom_jitter() and the average with a large dot with geom_point(). Each replicate has its own color and the data is plotted on a log scale with scale_x_log10(): p &lt;- ggplot(data = df_DRC, aes(x = Concentration, y = Response)) + geom_jitter(aes(x = Concentration, y = Response, color=Replicate), width=0.2, size=2, shape=16, alpha=0.2) + geom_point(data=df_summary, aes(x = Concentration, y = mean_Response, fill=Replicate), size=8, shape=21, alpha=0.8) + scale_x_log10() p Adjusting the theme and the y-axis scale improves the plot: p &lt;- p + ylim(-1,6) + theme_light(16) p Add labels: p &lt;- p + labs( title = &quot;Responses of individual cells to Histamine&quot;, x = &quot;Histamine concentration [µM]&quot;, y = &quot;Response [arbitrary units]&quot;, caption = &quot;@joachimgoedhart\\nbased on data from Chavez-Abiega et al., DOI: 10.1242/jcs.259685&quot;, tag = &quot;Protocol 9&quot; ) + theme(plot.caption = element_text(color = &quot;grey80&quot;)) And to label the different replicates with a colorblind friendly palette, we define a set of colors that were proposed by Paul Tol: https://personal.sron.nl/~pault/ Tol_bright &lt;- c(&#39;66CCEE&#39;, &#39;#BBBBBB&#39;, &#39;#CCBB44&#39;,&#39;#AA3377&#39;,&#39;#228833&#39;, &#39;#4477AA&#39;) To use these colors we define manual color scales for both ‘fill’ (used for geom_point()) and ‘color’ (used for geom_jitter()): p &lt;- p + scale_fill_manual(values = Tol_bright) + scale_color_manual(values = Tol_bright) p The result is a dose response curve in which the replicates are clearly identified with colorblind friendly colors. The EC50 can be determined from fitting a curve. To this end, we use the function nls(), which needs an equation, the data and estimates of the values. It will perform a ‘nonlinear least squares’ optimization to find parameters for the equation that best fit with the data of ‘df_summary’. The parameters are ‘bot’, ‘top’, ‘EC50’ and ‘slope’: fit &lt;- nls(mean_Response ~ top+(bot-top)/(1+(Concentration/EC50)^slope), data = df_summary, start=list(bot=-2, top=8, EC50=1, slope=1)) The result is stored in the object ‘fit’ and the parameters can be listed: coef(fit) bot top EC50 slope -0.1990749 3.2915666 0.4968077 0.7141987 To plot the fitted data, we use augment() function that is part of the {broom} package, which we need to load: library(broom) p + geom_line(data = augment(fit), aes(x=Concentration, y=.fitted), color=&quot;black&quot;, size=1) Instead of geom_lin() for plotting the fit, we can also use geom_smooth(): p &lt;- p + geom_smooth(data = augment(fit), aes(x=Concentration, y=.fitted), color=&quot;black&quot;, size=1) p `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; An alternative way to do the curve fit is provided through the {drc} package. To demonstrate this, we load the package and add the curve fit to the data: library(drc) Loading required package: MASS Attaching package: &#39;MASS&#39; The following object is masked from &#39;package:dplyr&#39;: select &#39;drc&#39; has been loaded. Please cite R and &#39;drc&#39; if used for a publication, for references type &#39;citation()&#39; and &#39;citation(&#39;drc&#39;)&#39;. Attaching package: &#39;drc&#39; The following objects are masked from &#39;package:stats&#39;: gaussian, getInitial p + geom_smooth(data=df_summary, aes(x = Concentration, y = mean_Response), color=&#39;blue&#39;, method = drm, method.args = list(fct = L.4()), se = FALSE) `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; `geom_smooth()` using formula = &#39;y ~ x&#39; The {drc} package was used to fit the data in the paper by Chavez-Abiega et al. (2022). But the approach followed above with nls() is easier to adapt for data that requires other models for the fit. Finally, we can save the plot: png(file=paste0(&quot;Protocol_09.png&quot;), width = 4000, height = 3000, units = &quot;px&quot;, res = 400) p `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; dev.off() quartz_off_screen 2 4.10 Protocol 10 - Plotting data that was harvested with a Google form The goal of this script is to read data that was generated by a Google form, transform it into a tidy format and plot the data. The form contains data that is submitted by students after doing a course on cell biology. The data that is submitted are length measurements on cells and in the end, we will plot these date. First, we need the {tidyverse} package: library(tidyverse) The data from the Google form are stored in a Google sheet that was published on the web as a CSV. This was achieved by ‘File &gt; Share &gt; Publish to web’ and selecting to ‘Publish’ as ‘Comma-separated values’. After clicking on ‘Publish’ a link is generated that can be read as a csv, and any empty cells are converted to NA. Below is the outcommented code that loads the online google sheet. To make sure that this script works even when the google sheet is no longer active and to ensure a reproducible data visualization, I downloaded a snapshot (made on 21st of April, 2022) of the google sheet as a csv file and I’ll load that insetad: # df_sheet &lt;- read.csv(&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vSc-nI1-s_u-XkNXEn_u2l6wkBafxJMHQ_Cd3kStrnToh7kawqjQU3y2l_1riLigKRkIqlNOqPrgkdW/pub?output=csv&quot;, na.strings = &quot;&quot;) df_sheet &lt;- read.csv(&quot;data/20220421_Resultaat-metingen.csv&quot;, na.strings = &quot;&quot;) head(df_sheet) Tijdstempel Groep 1 &lt;NA&gt; &lt;NA&gt; 2 &lt;NA&gt; &lt;NA&gt; 3 &lt;NA&gt; &lt;NA&gt; 4 &lt;NA&gt; &lt;NA&gt; 5 &lt;NA&gt; &lt;NA&gt; 6 01/03/2021 15:42:23 B Resultaten.van.de.10.metingen..in.µm...van.de.celkleuring..methyleenblauw. 1 &lt;NA&gt; 2 &lt;NA&gt; 3 &lt;NA&gt; 4 &lt;NA&gt; 5 &lt;NA&gt; 6 64.6, 52.5, 79.4, 48.4, 66.0, 64.4, 69.8, 71.2, 50.7, 51.0 Resultaten.van.de.10.metingen..in.µm...van.de.kernkleuring..DAPI. 1 &lt;NA&gt; 2 &lt;NA&gt; 3 &lt;NA&gt; 4 &lt;NA&gt; 5 &lt;NA&gt; 6 11.8, 7.8, 7.0, 12.4, 12.7, 14.0, 11.9, 12.9, 11.7, 13.2 There are a number of issues with this data. The first 5 rows are empty. I deleted the values in these rows, since they were dummy values from testing the form. Let’s remove rows without data: df_sheet &lt;- df_sheet %&gt;% na.omit() The header names are still in the original language (Dutch) and the third and fourth header names with the relevant data are very long. Let’s change the names: colnames(df_sheet) &lt;- c(&quot;Timestamp&quot;, &quot;Group&quot;, &quot;Cell&quot;, &quot;Nucleus&quot;) All the length measurements are distributed over two columns, let’s make it tidy and move it to one column: df_tidy &lt;- pivot_longer( df_sheet, cols = -c(&quot;Timestamp&quot;, &quot;Group&quot;), names_to = &quot;Sample&quot;, values_to = &quot;Size&quot; ) head(df_tidy) # A tibble: 6 × 4 Timestamp Group Sample Size &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 01/03/2021 15:42:23 B Cell 64.6, 52.5, 79.4, 48.4, 66.0, 64.4, 69.8, 7… 2 01/03/2021 15:42:23 B Nucleus 11.8, 7.8, 7.0, 12.4, 12.7, 14.0, 11.9, 12.… 3 01/03/2021 23:50:53 A Cell 57.7, 64.9, 44.8, 60.8, 46.8, 70.1, 67.8, 7… 4 01/03/2021 23:50:53 A Nucleus 9.6, 7.1, 8.7, 9.5, 8.1, 6, 8.3, 5.5, 7.8, … 5 02/03/2021 17:09:52 B Cell 14.839,17.578,9.054,16.218,14.724,12.410,14… 6 02/03/2021 17:09:52 B Nucleus 2.792,1.819,2.275,2.426,2.263,2.159,3.056,2… The column with measurements contains up to 10 length measurements. The values are separated by a comma and often a space. First, the space is removed and then each row is split into multiple rows, where each row has a single measurement. The comma is used to separate the values: df_tidy &lt;- df_tidy %&gt;% mutate(Size = gsub(&quot; &quot;, &quot;&quot;, Size)) %&gt;% separate_rows(Size, sep=&quot;,&quot;) The ‘Size’ column values are converted to the class numeric. The advantage is that anything that is not a number will be converted to NA. These non-numeric data can stem from incorrectly enetered data. Examples of these incorrect data are numbers with units or with wrong separators. df_tidy &lt;- df_tidy %&gt;% mutate(Size = as.numeric(Size)) Finally, we can filter any values that fall outside the range of expected values (and this will also remove the cells with NA): df_size &lt;- df_tidy %&gt;% filter(Size&gt;0 &amp; Size&lt;1000) head(df_size) # A tibble: 6 × 4 Timestamp Group Sample Size &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 01/03/2021 15:42:23 B Cell 64.6 2 01/03/2021 15:42:23 B Cell 52.5 3 01/03/2021 15:42:23 B Cell 79.4 4 01/03/2021 15:42:23 B Cell 48.4 5 01/03/2021 15:42:23 B Cell 66 6 01/03/2021 15:42:23 B Cell 64.4 The dataframe is cleaned and tidy and so it is ready for visualization. The primary information is the distribution of size measurements for the measurements of cells and cell nuclei. This can be achieved by using geom_density(). Given the range of sizes, it makes sense to use a log10 scale for the x-axis: ggplot(df_size, aes(x=Size, fill=Sample))+geom_density(alpha=.8) + scale_x_log10() The geom_density() produces a smooth distribution of the data. I actually prefer to see the real data and that’s why we turn to geom_histogram. I also will split the data according to the sample and groups and this can be achieved with facet_grid(). Here, we use the labeller() function to combine the column name ‘Group’ with the the factor it presents, as this makes it easier to understand what is plotted: ggplot(df_size, aes(x=Size, fill=Sample))+geom_histogram(alpha=.8) + facet_grid(Sample~Group, labeller = labeller(Group=label_both)) + scale_x_log10() `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The use of color is redundant, since the data is split in Cell and Nucleus anyway, so we remove that. The difference between groups is not large and for the final data visualization I chose to combine the data for all the groups: p &lt;- ggplot(df_size, aes(x=Size)) + geom_histogram(alpha=.5, bins=50) + facet_grid(~Sample) + annotation_logticks(sides=&quot;b&quot;, outside = TRUE) + scale_x_log10() p The data is presented in a nice way now, and we can improve the axis labels and add a title and caption: p &lt;- p + labs( title = &quot;Distribution of size measurements&quot;, subtitle = &quot;Sizes of human cells and nuclei of cells&quot;, x = &quot;Size [µm] - log scale&quot;, y = &quot;Count&quot;, caption = &quot;@joachimgoedhart | data submitted by 4 groups of students&quot;, tag = &quot;Protocol 10&quot; ) Optimizing the layout: p &lt;- #Set text size p + theme_light(base_size = 16) + # Change the color and position of the caption theme( plot.caption = element_text( color = &quot;grey80&quot;, hjust = 1 ) ) p This looks very decent. But we can edit change the x-axis to display logarithmic space ticks. In addition, I’d like the bars of the histogram to start from the axis and not ‘float’ as in the graph above: p &lt;- p + #Force the y-axis to start at zero scale_y_continuous(expand = c(0, NA), limits = c(0,200)) + #Apply a logarithmic scale to the x-axis and set the numbers for the scale scale_x_log10(breaks = c(1,10,100), limits = c(.5,200)) + #Remove minor gridlines theme(panel.grid.minor = element_blank()) + #Add ticks to the bottom, outside annotation_logticks(sides=&quot;b&quot;, outside = TRUE) + #Give a little more space to the log-ticks by adding margin to the top of the x-axis text theme(axis.text.x = element_text(margin = margin(t=8))) + #Needed to see the tcks outside the plot panel coord_cartesian(clip = &quot;off&quot;) Scale for x is already present. Adding another scale for x, which will replace the existing scale. p To save the plot as a PNG file: png(file=paste0(&quot;Protocol_10.png&quot;), width = 3000, height = 2000, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 4.11 Protocol 11 - Plotting time data from a Google form The goal of this script is to read data that was generated by a Google form and visualize the distribution of the hours at which the form was submitted. We need the tidyverse package: library(tidyverse) The data from the Google form are stored in a sheet that was published on the web as a CSV. This was achieved by ‘File &gt; Share &gt; Publish to web’ and selecting to ‘Publish’ as ‘Comma-separated values’. After clicking on ‘Publish’ a link is generated that can be read as a csv. Any empty cells are converted to NA. As explained in protocol 10, we will load a snapshot of the sheet (the code to read the actual sheet is outcommented here): # df_sheet &lt;- read.csv(&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vSc-nI1-s_u-XkNXEn_u2l6wkBafxJMHQ_Cd3kStrnToh7kawqjQU3y2l_1riLigKRkIqlNOqPrgkdW/pub?output=csv&quot;, na.strings = &quot;&quot;) df_sheet &lt;- read.csv(&quot;data/20220421_Resultaat-metingen.csv&quot;, na.strings = &quot;&quot;) head(df_sheet) Tijdstempel Groep 1 &lt;NA&gt; &lt;NA&gt; 2 &lt;NA&gt; &lt;NA&gt; 3 &lt;NA&gt; &lt;NA&gt; 4 &lt;NA&gt; &lt;NA&gt; 5 &lt;NA&gt; &lt;NA&gt; 6 01/03/2021 15:42:23 B Resultaten.van.de.10.metingen..in.µm...van.de.celkleuring..methyleenblauw. 1 &lt;NA&gt; 2 &lt;NA&gt; 3 &lt;NA&gt; 4 &lt;NA&gt; 5 &lt;NA&gt; 6 64.6, 52.5, 79.4, 48.4, 66.0, 64.4, 69.8, 71.2, 50.7, 51.0 Resultaten.van.de.10.metingen..in.µm...van.de.kernkleuring..DAPI. 1 &lt;NA&gt; 2 &lt;NA&gt; 3 &lt;NA&gt; 4 &lt;NA&gt; 5 &lt;NA&gt; 6 11.8, 7.8, 7.0, 12.4, 12.7, 14.0, 11.9, 12.9, 11.7, 13.2 Here, we are only interested in the first column with the time data. So we select that column and replace the dutch column name by ‘Timestamp’: df_sheet &lt;- df_sheet %&gt;% dplyr::select(&quot;Timestamp&quot;=1) %&gt;% na.omit() We can seperate the Timestamp column in date and time: df_tidy &lt;- df_sheet %&gt;% separate(&#39;Timestamp&#39;, c(&quot;Date&quot;, &quot;Time&quot;), sep=&quot; &quot;) Correct handling of date and time data is quite challenging and a topic on its own. The code to format the ‘Date’ column as date (note that we need a capital Y as the year is presented in 4 digits) would be: df_tidy %&gt;% mutate(Date=as.Date(Date, &quot;%d/%m/%Y&quot;)) %&gt;% head() Date Time 6 2021-03-01 15:42:23 7 2021-03-01 23:50:53 8 2021-03-02 17:09:52 9 2021-03-03 09:30:43 10 2021-03-03 16:21:32 11 2021-03-03 21:00:23 Here, we avoid the Date format and use the individual numbers. The ‘Date’ column is seperated into three columns for year, month and day. The ‘Time’ column can be split into hour, minute and second: df_tidy &lt;- df_tidy %&gt;% separate(&#39;Date&#39;, c(&quot;day&quot;, &quot;month&quot;, &quot;year&quot;), sep=&quot;/&quot;, convert = TRUE) %&gt;% separate(&#39;Time&#39;, c(&quot;hour&quot;, &quot;min&quot;, &quot;sec&quot;), sep=&quot;:&quot;, convert = TRUE) head(df_tidy) day month year hour min sec 6 1 3 2021 15 42 23 7 1 3 2021 23 50 53 8 2 3 2021 17 9 52 9 3 3 2021 9 30 43 10 3 3 2021 16 21 32 11 3 3 2021 21 0 23 Note that we use convert = TRUE to ensure that the data is treated as numbers (not as characters). We can use the data to check at what time the data was submitted: ggplot(df_tidy, aes(x=hour))+geom_histogram(alpha=.8) `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The default number of ‘bins’ is 30, but it makes more sense to use 24 here as there are 24 hours in a day: ggplot(df_tidy, aes(x=hour))+geom_histogram(bins = 24, alpha=.8) Since ‘hour’ is a number that cycles, with 0 followed by 23, it is nice to connect the data from 23 with 0 and we can do that with coord_polar(): ggplot(df_tidy, aes(x=hour)) + geom_histogram(bins = 24, alpha=.8) + coord_polar() + scale_x_continuous(breaks=seq(0, 24, by=2)) To center the plot at 0, we need to rotate it by -7.5 degrees (1 hour equal 15 degrees). We can use the start option within coord_polar(), but it only accepts ‘radians’. To convert degrees to radians the value is multiplied by pi/180: ggplot(df_tidy, aes(x=hour)) + geom_histogram(bins = 24, alpha=.8) + coord_polar(start = (-7.5*pi/180)) + scale_x_continuous(breaks=seq(0, 24, by=3)) The plot can be rotated to center it around 15h as it reflects the middle of the afternoon, when most activity takes place. To achieve this, we need to shift the start another 15*15 degrees: p &lt;- ggplot(df_tidy, aes(x=hour)) + geom_histogram(bins = 24, alpha=.8, color=&#39;black&#39;) + coord_polar(start = (-7.5 - 15*15)*pi/180) + scale_x_continuous(breaks=seq(0, 24, by=3)) p This looks good, let’s first improve on the layout by changing the theme. I do not like the grey background, so any theme with a white background is an improvement, for instance theme_minimal(): p + theme_minimal() I do not like the labels that show the count. Since it is clear that the length of the bar indicates an amount and since the absolute amount is not so interesting, these labels can be removed. The labels for the hours are helpful and their sice needs to be increased. Finally, I do not like the grid running through the numbers. I did not find a way in the theme setting to get rid of this, so we define the grid ourselves by using repetitive horizontal and vertical lines: p &lt;- p + geom_hline(yintercept = c(2,4,6,8), colour = &quot;grey90&quot;, size = 0.5) + geom_vline(xintercept = seq(0, 21, by=3), colour = &quot;grey90&quot;, size = 0.5) + theme_minimal(base_size = 16) + theme(panel.grid.major.x = element_blank(), panel.grid.major.y = element_blank(), panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(), axis.text.y = element_blank() ) p The grid looks good, but the bars have to be on top of this layer, so we redefine the plot in the correct order: p &lt;- ggplot(df_tidy, aes(x=hour)) + geom_hline(yintercept = c(2,4,6,8), colour = &quot;grey90&quot;, size = 0.5) + geom_vline(xintercept = seq(0, 21, by=3), colour = &quot;grey90&quot;, size = 0.5) + geom_histogram(bins = 24, alpha=.8, color=&#39;black&#39;) + coord_polar(start = (-7.5 - 15*15)*pi/180) + scale_x_continuous(breaks=seq(0, 24, by=3)) + theme_minimal(base_size = 16) + theme(panel.grid.major.x = element_blank(), panel.grid.major.y = element_blank(), panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(), axis.text.y = element_blank() ) + labs( title = &quot;Counting the hours...&quot;, subtitle = &quot;that a Google form was submitted&quot;, x = &quot;&quot;, y = &quot;&quot;, caption = &quot;@joachimgoedhart | data submitted by students&quot;, tag = &quot;Protocol 11&quot; ) + theme(plot.caption = element_text(color = &quot;grey80&quot;, hjust = 1)) p A centered title and subtitle looks nice on this plot: p &lt;- p + theme( plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5) ) p To save the plot as a PNG file: png(file=paste0(&quot;Protocol_11.png&quot;), width = 4000, height = 3000, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 4.12 Protocol 12 - Plotting grouped data This protocol explains how data can be plotted side-by-side. It is a remake of a figure that was published by de Man et al. (2021). The source data is published with the paper and can be downloaded. Next to the tidyverse package we need readxl to import the excel file: library(tidyverse) library(readr) Next, we download the xls file from the url, store it as a (temporary) file, and use read_excel() to import the data (note that unlike the read.csv() we cannot directly download the data from a url): url_xls &lt;- &quot;https://cdn.elifesciences.org/articles/66440/elife-66440-fig5-data1-v2.xlsx&quot; # file_xls &lt;- basename(url_xls) download.file(url = url_xls, destfile = &#39;temp_file&#39;) df_download &lt;- readxl::read_excel(&#39;temp_file&#39;) New names: • `` -&gt; `...2` • `` -&gt; `...3` • `` -&gt; `...4` • `` -&gt; `...5` • `` -&gt; `...6` • `` -&gt; `...7` • `` -&gt; `...8` • `` -&gt; `...9` • `` -&gt; `...10` • `` -&gt; `...11` Inspection of the dataframe shows that we can remove the first 11 rows and that the name of the columns is in the 12th row. First we fix the column names: colnames(df_download) &lt;- df_download %&gt;% slice(12) Then, we can remove the first 12 frames and select only the first 3 columns (I use dplyr:: here to make explicit that the select() function is taken from the {dplyr} package. This may not be necessary when you run your own code): df_tidy &lt;- df_download %&gt;% slice(-(1:12)) %&gt;% dplyr::select(1:3) head(df_tidy) # A tibble: 6 × 3 Sample `Measurement date` `Total SGFP2-CTNNB1 concentration` &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 B10 WNT3A c1 cyto 180926 250.188126820392 2 B10 WNT3A c2 cyto 180926 243.40459000116601 3 B10 WNT3A c3 cyto 180926 233.140028430596 4 B10 WNT3A c3 nucleus 180926 377.33830720940603 5 B10 WNT3A c4 cyto 180926 256.42702997341098 6 B10 WNT3A c4 nucleus 180926 379.44318769572499 The third column, containing the measurement data has the class &lt;chr&gt;, but the correct class is numeric. Here, we mutate the column and rename it. To keep the data that is untouched, we add .keep = \"unused\": df_tidy &lt;- df_tidy %&gt;% mutate(`SGFP2-CTNNB1` = as.numeric(`Total SGFP2-CTNNB1 concentration`), .keep = &quot;unused&quot;) Finally, we need to separate the first column, since it contains the relevant data on the conditions and the grouping of the data: df_tidy &lt;- df_tidy %&gt;% separate(`Sample`, c(&quot;Celltype&quot;, &quot;ligand&quot;, &quot;number&quot;, &quot;compartment&quot;), sep=&quot; &quot;) head(df_tidy) # A tibble: 6 × 6 Celltype ligand number compartment `Measurement date` `SGFP2-CTNNB1` &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 B10 WNT3A c1 cyto 180926 250. 2 B10 WNT3A c2 cyto 180926 243. 3 B10 WNT3A c3 cyto 180926 233. 4 B10 WNT3A c3 nucleus 180926 377. 5 B10 WNT3A c4 cyto 180926 256. 6 B10 WNT3A c4 nucleus 180926 379. There are two compartments (nucleus and cytoplasm) and two ligands (BSA as a control and WNT3A). To avoid abbreviated labels in the plot, we will rename the ‘cyto’ condition: df_tidy &lt;- df_tidy %&gt;% mutate(compartment = case_when(compartment == &quot;cyto&quot; ~ &quot;cytoplasm&quot;, TRUE ~ compartment) ) head(df_tidy) # A tibble: 6 × 6 Celltype ligand number compartment `Measurement date` `SGFP2-CTNNB1` &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 B10 WNT3A c1 cytoplasm 180926 250. 2 B10 WNT3A c2 cytoplasm 180926 243. 3 B10 WNT3A c3 cytoplasm 180926 233. 4 B10 WNT3A c3 nucleus 180926 377. 5 B10 WNT3A c4 cytoplasm 180926 256. 6 B10 WNT3A c4 nucleus 180926 379. This data can be plotted in a number of different ways. The first way is by comparing the different compartments for each ligand: ggplot(df_tidy, aes(x=ligand, y=`SGFP2-CTNNB1`, fill = compartment)) + geom_boxplot(outlier.color = NA) + geom_jitter() However, in the original figure, the data is split between the two compartments and the two ligands are plotted next to eachother: ggplot(df_tidy, aes(x=ligand, y=`SGFP2-CTNNB1`, fill = compartment)) + geom_boxplot(outlier.color = NA) + geom_jitter() + facet_wrap(~compartment) This is close to the original figure, but the colors in that figure reflect the ligand, not the compartment: p &lt;- ggplot(df_tidy, aes(x=ligand, y=`SGFP2-CTNNB1`, fill = ligand)) + geom_boxplot(outlier.color = NA) + geom_jitter() + facet_wrap(~compartment) p This plot looks good. Now let’s improve the theme. p &lt;- p + theme_linedraw(base_size = 16) + theme(panel.spacing = unit(0, &quot;points&quot;)) p The theme is pretty nice, but the black strips are very distracting. Let’s change the fill and color (border) of the strips and change the text to black. At the same time, we remove the grid and the legend: p &lt;- p + theme(strip.background = element_rect(fill=NA, color=&quot;black&quot;, size = .5), strip.text = element_text(color = &#39;black&#39;), panel.grid = element_blank(), legend.position = &quot;none&quot;, plot.caption = element_text(color=&#39;grey80&#39;) ) p Finally, we fix the labels: p &lt;- p + labs(x=&quot;&quot;, y=&quot;Concentration [nM]&quot;, title = &quot;Total SGFP2-CTNNB1&quot;, caption = &quot;@joachimgoedhart | based on data from de Man et al., DOI: 10.7554/eLife.66440&quot;, tag = &quot;Protocol 12&quot; ) p To save the plot as a PNG file: png(file=paste0(&quot;Protocol_12.png&quot;), width = 4000, height = 3000, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 4.13 Protocol 13 - Plotting multiple plots side-by-side This protocol is a little different, in that it plots the same data in three different ways. I use this figure to illustrate that barplots are a poor choice for plotting multiple measurements per condition as the actual data is concealed. The alternatives that show all, i.e. jittered dotplots and beeswarmplots (or sinaplots) are shown next to the barplot. In addition to the {tidyverse} package, we need two additional packages. The {ggbeeswarm} package to plot the beeswarm plot and {patchwork} to stitch the plots together: library(tidyverse) library(patchwork) Attaching package: &#39;patchwork&#39; The following object is masked from &#39;package:MASS&#39;: area library(ggbeeswarm) Next, the wide data is loaded and converted into a tidy format: df_wide &lt;- read.csv(&quot;data/Area_in_um-GEFs.csv&quot;) df_tidy &lt;- pivot_longer(df_wide, cols = everything(), names_to = &quot;Condition&quot;, values_to = &quot;Area&quot;) head(df_tidy) # A tibble: 6 × 2 Condition Area &lt;chr&gt; &lt;dbl&gt; 1 LARG 1064. 2 wt 1571. 3 TIAM 3058. 4 LARG 1374. 5 wt 1946. 6 TIAM 1767. Next, we define the order of factors in the column with Conditions: df_tidy &lt;- df_tidy %&gt;% mutate(Condition = fct_relevel(Condition, c(&quot;LARG&quot;, &quot;wt&quot;, &quot;TIAM&quot;))) To plot the barplot, we calculate the summary statistics and store them in a separate dataframe: df_summary &lt;- df_tidy %&gt;% na.omit() %&gt;% group_by(Condition) %&gt;% summarise(n=n(), mean=mean(Area), sd=sd(Area) ) %&gt;% mutate(sem=sd/sqrt(n-1)) Now, we do things a bit differently than we’ve done in other protocols. First, we define a plotting canvas and its theme settings. We use this object p as the basis for the three plots that differ in the data visualization and not in the layout. p &lt;- ggplot(df_tidy, (aes(x=Condition))) + theme_bw(base_size = 12) + theme(panel.grid = element_blank()) + scale_y_continuous(expand = c(0, 0), limits = c(0, 4200)) + labs(y=NULL) p The first plot is a barplot (the length of the bar reflects the mean value) with the standard error of the mean as errorbar: p1 &lt;- p + geom_errorbar(data=df_summary, aes(x=Condition,ymin=mean-sem, ymax=(mean+sem)), width=0.3, size=1, alpha=0.7) + geom_bar(data=df_summary, aes(y=mean), stat = &#39;identity&#39;, width = 0.5) + labs(y=expression(&quot;Area [µm&quot;^2*&quot;]&quot;), title=&quot;Clean | Data hidden&quot;) The second plot shows the data as jittered dots: p2 &lt;- p + geom_jitter(data = df_tidy, aes(x=Condition, y=Area), position=position_jitter(0.3), cex=2, alpha=0.4) + labs(title=&quot;Messy | Data shown&quot;) The third plot is a beeswarm-type plot that organizes the dots according to the data distribution: p3 &lt;- p + geom_quasirandom(data = df_tidy, aes(x=Condition, y=Area), varwidth = TRUE, cex=2, alpha=0.4)+ labs(title=&quot;Clean | Data shown&quot;) The patchwork package has a simple syntax for combining the plots, for examples check the package website. A vertical composition is made by using p1/p2/p3. But this won’t look good, so combine the plots horizontally using p1+p2+p3: p &lt;- p1+p2+p3 p The result is a set of plots that are nicely aligned, side-by-side. We have titles for individual plots and it would be nice to have a title and caption for the entire panel. We can use the plot_annotation() function from the {patchwork} package. The ‘theme’ specified in this function is only applied to the main title and caption: p &lt;- p + plot_annotation(title=&quot;Protocol 13&quot;, caption=&quot;@joachimgoedhart&quot;, theme = theme(plot.caption = element_text(size = 12, color=&quot;grey80&quot;), plot.title = element_text(size = 18)) ) p To save the composite plot: png(file=paste0(&quot;protocol_13.png&quot;), width = 4000, height = 3000, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 4.14 Protocol 14 - Volcano plot The results of comparative omics experiments are (very) large datasets. The datasets contain information on the fold change and level of significance for every gene or protein in a genome. The volcano plot is a scatterplot that visualizes both the (\\(log_2\\)) fold change and level of significance (usually \\(-log_{10}[p-value]\\)). This data visualization enables the display of a large amount of data in a concise way. Typically, only a handful of datapoints in the volcano plot are of interest, also known as hits. The hits are datapoints that surpass a threshold that is defined by the user for both the significance and fold change. Annotation of hits (with names) is used to draw attention to these most relevant or interesting datapoints. This protocol showcases the construction of a volcano plot, including the filtering of significant datapoints and the annotation of top candidates. The code that is used here is similar to what is used in the web app VolcaNoseR which is dedicated to making volcano plots. In fact, the data visualization shown here is very close to the standard output of VolcaNoseR and uses the same example data. We start by loading the {tidyverse} package for plotting: library(tidyverse) The measured data is read from a CSV file. These data are originally from Becares et al, DOI: 10.1016/j.celrep.2018.12.094: df_tidy &lt;- read_csv(&quot;data/Becares-diffgenes_HFHC.csv&quot;) Rows: 3001 Columns: 9 ── Column specification ────────────────────────────────────────────────────────────────────────── Delimiter: &quot;,&quot; chr (1): Gene dbl (8): baseMean, log2_FoldChange, FC, lfcSE, stat, pvalue, padj, minus_log... ℹ Use `spec()` to retrieve the full column specification for this data. ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(df_tidy) # A tibble: 6 × 9 Gene baseMean log2_FoldChange FC lfcSE stat pvalue padj &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 A1bg 39.2 1.25 2.38 0.299 4.19 0.0000278 0.00170 2 A1cf 1183. 0.745 1.68 0.192 3.88 0.000102 0.00456 3 Aadac 3196. 0.688 1.61 0.157 4.39 0.0000111 0.000817 4 Aadat 1481. 0.603 1.52 0.181 3.33 0.000878 0.0232 5 Aaed1 432. 0.422 1.34 0.193 2.19 0.0288 0.235 6 Aamdc 165. 0.332 1.26 0.191 1.74 0.0827 0.419 # ℹ 1 more variable: minus_log10_pvalue &lt;dbl&gt; The basic volcano plot is a scatterplot that displays minus_log10_pvalue versus log2_FoldChange: ggplot(data = df_tidy, aes(x=log2_FoldChange, y=minus_log10_pvalue)) + geom_point(alpha=0.5) To improve the visualization, we will 1) set thresholds, 2) display the thresholds, 3) filter interesting candidates based on the thresholds and 4) annotate the top-10 candidates. 4.14.1 Step 1 - Define thresholds To identify data that is of interest, thresholds are defined for both the effect size and the significance. We define the thresholds, &lt;-1.5 and &gt;1.5 for the effect size and &gt;5 for the significance. When both thresholds are exceeded the data is considered as significant (worthy of a closer look). 4.14.2 Step 2 - Display thresholds To plot the thresholds, we use geom_hline() and geom_vline(). Note that we can specify two intercepts at once, which is used below for geom_vline(): ggplot(data = df_tidy, aes(x=log2_FoldChange, y=minus_log10_pvalue)) + geom_point() + geom_vline(xintercept = c(-1.5,1.5), linetype=&quot;dashed&quot;) + geom_hline(yintercept = 5, linetype=&quot;dashed&quot;) 4.14.3 Step 3 - Filtering with thresholds In this step, we add another column to the data, which defines, for each row, whether a gene is significantly lower, higher or unchanged. This is based on the thresholds: df_tidy &lt;- df_tidy %&gt;% mutate( Change = case_when( `log2_FoldChange` &gt; 1.5 &amp; `minus_log10_pvalue` &gt; 5 ~ &quot;Increased&quot;, `log2_FoldChange` &lt; -1.5 &amp; `minus_log10_pvalue` &gt; 5 ~ &quot;Decreased&quot;, TRUE ~ &quot;Unchanged&quot; ) ) head(df_tidy) # A tibble: 6 × 10 Gene baseMean log2_FoldChange FC lfcSE stat pvalue padj &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 A1bg 39.2 1.25 2.38 0.299 4.19 0.0000278 0.00170 2 A1cf 1183. 0.745 1.68 0.192 3.88 0.000102 0.00456 3 Aadac 3196. 0.688 1.61 0.157 4.39 0.0000111 0.000817 4 Aadat 1481. 0.603 1.52 0.181 3.33 0.000878 0.0232 5 Aaed1 432. 0.422 1.34 0.193 2.19 0.0288 0.235 6 Aamdc 165. 0.332 1.26 0.191 1.74 0.0827 0.419 # ℹ 2 more variables: minus_log10_pvalue &lt;dbl&gt;, Change &lt;chr&gt; We can now use the column ‘Change’ to color code the data: p &lt;- ggplot(data = df_tidy, aes(x=log2_FoldChange, y=minus_log10_pvalue)) + geom_vline(xintercept = c(-1.5,1.5), linetype=&quot;dashed&quot;) + geom_hline(yintercept = 5, linetype=&quot;dashed&quot;) p &lt;- p + geom_point(aes(color = Change)) p 4.14.4 Step 4 - Annotations The ‘top hits’ are the datapoints that are the furthest from the origin. Here, we only consider genes that are either ‘Decreased’ or ‘Increased’, so we remove the ‘Unchanged’. To determine the distance from the origin for each datapoint, we sum the x- and y- position. Note that this simple summation is known as the ‘Manhattan distance’. After calculation, the top 10 is selected and stored in another dataframe, df_top. : df_top &lt;- df_tidy %&gt;% filter(Change != &#39;Unchanged&#39;) %&gt;% mutate(distance = minus_log10_pvalue + abs(log2_FoldChange)) %&gt;% top_n(10,distance) The new dataframe can be used for labeling the top-10 hits: p + geom_text( data = df_top, aes(label = Gene) ) Wwe see that the labels overlap and this is often the case when a lot of data is displayed. There is a very handy packgae {ggrepel} that will reduce the overlap between labels: library(ggrepel) p &lt;- p + geom_text_repel( data = df_top, aes(label = Gene), size=5, # increase font size min.segment.length = 0, # draw all line segments box.padding = 0.5# give some space for drawing the lines ) p Now that we have implemeted the features of a volcano plot, we can focus on improving the data visualization. We can emphasize the hits by adding a black outline to these points. The default shape for ‘geom_point()’ is a filled circle. Below we use shape 21, which accepts both a color for the outline and filling. The black outline is defined by ‘color’: p + geom_point(data = df_top, aes(x=log2_FoldChange, y=minus_log10_pvalue), shape=21, color=&#39;black&#39;) The line that connects the dot with the label is a bit too long and is visible within the dot, which is a bit ugly. To solve this we can fill these dots which will hide the line. We can also make the dots a bit bigger to emphasize the hits: p &lt;- p + geom_point(data = df_top, aes(x=log2_FoldChange, y=minus_log10_pvalue, fill=Change), shape=21, color=&#39;black&#39;, size=2) p Let’s change the colors to make the uninteresting datapoints less pronounced: newColors &lt;- c(&quot;dodgerblue&quot;, &quot;orange&quot;, &quot;grey&quot;) p &lt;- p + scale_color_manual(values = newColors) + scale_fill_manual(values = newColors) p Finally, we can change the styling by adjusting the labels and the theme. The labels for x- and y-axis use the function expression() to add subscript text to the labels: p &lt;- p + labs( title = &quot;Differentially expressed genes&quot;, x = expression(&#39;Fold Change (&#39;*Log[2]*&#39;)&#39;), y = expression(&#39;Significance (&#39;*-Log[10]*&#39;)&#39;), caption = &quot;@joachimgoedhart\\n(based on data from Becares et al, DOI: 10.1016/j.celrep.2018.12.094)&quot;, tag = &quot;Protocol 14&quot; ) Modify the layout by adjusting the theme: p &lt;- p + theme_light(base_size = 16) + theme(plot.caption = element_text(color = &quot;grey80&quot;, hjust = 1)) + theme(panel.grid = element_blank()) + theme(legend.position=&quot;none&quot;) Change the x-axis scale the give the labels a bit more room and display the result: p &lt;- p + xlim(-3,3) p To save the plot as a png file: png(file=paste0(&quot;Protocol_14.png&quot;), width = 4000, height = 3000, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 4.15 Protocol 15 - Timeline In this protocol we will make a timeline. The timeline organizes events along a line that consists of dates, in this case years. The years are treated as continuous data and the events are text and hence qualitative data. We start by loading the {tidyverse} package, mainly for the use of ggplot2: library(tidyverse) The data for the timeline is stored in a text file that has tabs as delimiters: df_timeline &lt;- read.delim(&quot;data/GFP-timeline.txt&quot;, sep = &#39;\\t&#39; ) head(df_timeline) Year DOI 1 1962 10.1002/jcp.1030590302 2 1971 10.1002/jcp.1040770305 3 1974 10.1021/bi00709a028 4 1979 10.1016/0014-5793(79)80818-2 5 1992 10.1016/0378-1119(92)90691-h 6 1994 10.1126/science.8303295 Event 1 Osamu Shimomura isolates a green protein from jellyfish 2 First appearance of the name Green Fluorescent Protein 3 Crystallization of GFP, QY is 0.72 4 Structure of the GFP chromophore is solved 5 Douglas Prasher clones the AvGFP gene 6 Martin Chalfie expresses AvGFP in bacteria and worms Let’s first make a basic plot to visualize the data in the column ‘Year’: ggplot(df_timeline, aes(x=1,y=Year)) + geom_point() Each point reflects a year in which some event happened. The events can be plotted as text: ggplot(df_timeline, aes(x=1,y=Year)) + geom_point() + geom_text(aes(label = Event)) We need to do a couple of things. First, make a nice timeline that shows each year as a dot and the years with events as a more pronounced dot. Second, we need to align the text and reduce the overlap. 4.15.1 Improve the timeline To make a nice timeline, I’d like to show every year as dot and therefore I define a new dataframe: yrs &lt;- data.frame(Year=min(df_timeline$Year):max(df_timeline$Year)) ggplot(df_timeline, aes(x=1,y=Year)) + geom_point(shape=21, size=3, fill=&quot;white&quot;) + geom_point(data =yrs, aes(x=1), size=0.5, color=&quot;grey20&quot;) Let’s add a line to make it a timeline and do some formatting of the theme to get rid of the legend: ggplot(df_timeline, aes(x=1, y=Year)) + geom_vline(xintercept = 1, size=0.1) + geom_point(shape=21, size=3, fill=&quot;white&quot;) + geom_point(data=yrs, aes(x=1), size=0.5, color=&quot;grey20&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;) We will remove the grid and text for the x-axis. We add the ticks for the y-axis and we specify the labels for the years using breaks in scale_y_continuous(): p &lt;- ggplot(df_timeline, aes(x=1, y=Year)) + geom_vline(xintercept = 1, size=0.1) + geom_point(shape=21, size=3, fill=&quot;white&quot;) + geom_point(data=yrs, aes(x=1), size=0.5, color=&quot;grey20&quot;) + theme_minimal(base_size = 16) + theme(legend.position = &quot;none&quot;) + theme(axis.ticks.y = element_line(color=&quot;grey&quot;, linetype = 1, size = 1), panel.grid = element_blank(), axis.title = element_blank(), axis.text.x = element_blank(), plot.caption = element_text(color = &quot;grey80&quot;, hjust = 0) ) + scale_y_continuous(limits = c(1962,2025), breaks = c(seq(1965, 2024, by=5))) p Allright, things start to look better now. The next step is to add text. We will use labels and the geom_label_repel() function from the {ggrepel} package to avoid overlap. First we load the package: library(ggrepel) 4.15.2 Add and format text Now, we can add labels to the plotting object: p + geom_label_repel(aes(label=Event)) Let’s change the x-axis scale, to make room for the text on the right side of the line. Also, we can left-align the text vertically and add some space between the points and the text. This is achieved by optimizing the ‘nudge_x’, ‘hjust’ and ‘direction’: p + geom_label_repel(aes(label=Event), nudge_x = 0.05, hjust = 0, direction = &quot;y&quot; ) + scale_x_continuous(expand = c(0.01, 0), limits = c(1.0,1.5)) Finally, we can adjust the font size for the label. We also add colors to the different years. I like to use the ‘viridis’ color scale here. because it runs from dark to bright. However, if we would use the full range the final color is bright yellow which has a poor contrast on white. To solve this, we define the ‘end’ of the viridis scale at 80%: p &lt;- p + aes(color=Year) + geom_label_repel(aes(label=Event), nudge_x = 0.05, hjust = 0, direction = &quot;y&quot;, size = 3 ) + scale_x_continuous(expand = c(0.01, 0), limits = c(1.0,1.5)) + scale_color_viridis_c(end = 0.8)+ labs(title = &quot;60 years of GFP discovery and engineering&quot;, tag = &quot;Protocol 15&quot;, caption = &quot;@joachimgoedhart&quot;) + NULL p To save the plot as a png file: png(file=paste0(&quot;Protocol_15.png&quot;), width = 4000, height = 3000, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 4.16 Protocol 16 - Scatterplots and correlation In this protocol we will look at the data from a co-expression experiment. Images of cells co-expressing two fluorescent proteins were acquired with a fluorescence microscope. The relative intensity of each fluorescent protein was measured. The data can be plotted in a scatter plot to visualize the correlation between the two expressed proteins. To quantify the variation in co-expression, a correlation coefficient can be determined. More experimental details can be found in the original publication Goedhart et al. (2011). We start by loading the {tidyverse} package: library(tidyverse) Read the data: df_co &lt;- read.csv(&quot;data/coexpression.csv&quot;, stringsAsFactors = TRUE) head(df_co) BCY_YFP BCY_CFP BYC_YFP BYC_CFP TCY_YFP TCY_CFP TYC_YFP TYC_CFP IYC_YFP 1 88.841 113.92 88.841 113.92 107.36 124.57 148.75 165.71 290.32 2 95.053 118.53 95.053 118.53 129.05 158.97 166.32 192.10 362.47 3 99.563 208.09 99.563 208.09 132.55 160.52 170.43 215.41 395.74 4 105.630 275.01 105.630 275.01 140.84 166.70 191.14 225.23 420.21 5 124.370 162.51 124.370 162.51 144.45 166.72 198.11 255.85 476.79 6 142.130 225.24 142.130 225.24 152.47 146.71 213.19 230.92 495.70 IYC_CFP ICY_YFP ICY_CFP 1 122.01 60.505 196.97 2 151.95 124.820 625.77 3 159.96 145.510 672.24 4 194.17 143.980 703.17 5 161.08 143.450 717.70 6 225.22 150.590 775.40 The data was stored in a spreadsheet, and is in a wide format. Pairs of columns belong to the same measurement. The two fluorescent proteins were CFP and YFP and this indicated in the column name after the underscore. Several co-expression systems were analyzed and this is indicated in the string before the underscore. We can plot one condition to see what the data looks like in a scatter plot: ggplot(df_co, aes(x=BCY_YFP, y=BCY_CFP)) + geom_point() 4.16.1 Linear fit A linear fit can be added with the geom_smooth function. The method is speficied as lm which means linear model: ggplot(df_co, aes(x=BCY_YFP, y=BCY_CFP)) + geom_point() + geom_smooth(method = &#39;lm&#39;) `geom_smooth()` using formula = &#39;y ~ x&#39; The linear model has an intercept, but suppose that I want to fit the data without offset. To this end, a custom formula can added to overrule the default: ggplot(df_co, aes(x=BCY_YFP, y=BCY_CFP)) + geom_point() + geom_smooth(method = &#39;lm&#39;, formula = y~0+x) To get to the coefficients of the fit we can use the lm() function fits data to a linear model: lm(BCY_CFP ~ BCY_YFP + 0, data = df_co) Call: lm(formula = BCY_CFP ~ BCY_YFP + 0, data = df_co) Coefficients: BCY_YFP 1.14 To get a summary of the fit we use the summary() function and assign the results to ‘fit_summary’: fit_summary &lt;- summary(lm(BCY_CFP ~ BCY_YFP + 0, data = df_co)) fit_summary Call: lm(formula = BCY_CFP ~ BCY_YFP + 0, data = df_co) Residuals: Min 1Q Median 3Q Max -3622.9 -140.6 45.6 284.5 4270.0 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) BCY_YFP 1.13983 0.02218 51.39 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 572 on 733 degrees of freedom (106 observations deleted due to missingness) Multiple R-squared: 0.7828, Adjusted R-squared: 0.7825 F-statistic: 2641 on 1 and 733 DF, p-value: &lt; 2.2e-16 The object ‘fit_summary’ is a list and we can extract the R-squared value from this list: fit_summary$r.squared [1] 0.7827556 4.16.2 Multiple scatterplots So far so good; we have plotted the data from a single condition and the relevant parameters can be displayed. Now, let’s see if we can do the same for the entire dataset. First, we will need to restructure the data into a long format: df_longer_co &lt;- df_co %&gt;% pivot_longer(cols = everything(), names_to = &quot;Condition&quot;, values_to = &quot;Intensity&quot;) head(df_longer_co) # A tibble: 6 × 2 Condition Intensity &lt;chr&gt; &lt;dbl&gt; 1 BCY_YFP 88.8 2 BCY_CFP 114. 3 BYC_YFP 88.8 4 BYC_CFP 114. 5 TCY_YFP 107. 6 TCY_CFP 125. This is a ‘true’ long format, but we will need to have the CFP and YFP data side-by-side to generate the scatterplot. Let’s first split the column with conditions in the Condition and the fluorescent protein (CFP or YFP): df_longer_co &lt;- df_longer_co %&gt;% separate(Condition, into = c(&quot;Plasmid&quot;, &quot;Protein&quot;), sep = &quot;_&quot;) head(df_longer_co) # A tibble: 6 × 3 Plasmid Protein Intensity &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 BCY YFP 88.8 2 BCY CFP 114. 3 BYC YFP 88.8 4 BYC CFP 114. 5 TCY YFP 107. 6 TCY CFP 125. Now we can make the data ‘wider’ with the pivot_wider() function: df_wider_co &lt;- df_longer_co %&gt;% pivot_wider(names_from = Protein, values_from = Intensity) head(df_wider_co) # A tibble: 6 × 3 Plasmid YFP CFP &lt;chr&gt; &lt;list&gt; &lt;list&gt; 1 BCY &lt;dbl [840]&gt; &lt;dbl [840]&gt; 2 BYC &lt;dbl [840]&gt; &lt;dbl [840]&gt; 3 TCY &lt;dbl [840]&gt; &lt;dbl [840]&gt; 4 TYC &lt;dbl [840]&gt; &lt;dbl [840]&gt; 5 IYC &lt;dbl [840]&gt; &lt;dbl [840]&gt; 6 ICY &lt;dbl [840]&gt; &lt;dbl [840]&gt; This will throw a warning message, since some rows have duplicated values. This is not an issue here, but the result is a ‘nested’ dataframe with the data in lists instead of a single value per cell. To fix this, we use unnest(): df_co_all &lt;- df_wider_co %&gt;% unnest(cols = c(YFP, CFP)) head(df_co_all) # A tibble: 6 × 3 Plasmid YFP CFP &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 BCY 88.8 114. 2 BCY 95.1 119. 3 BCY 99.6 208. 4 BCY 106. 275. 5 BCY 124. 163. 6 BCY 142. 225. Now, we have the data in the right shape and we can make scatterplots for each ‘Plasmid’ with facet_wrap(): ggplot(df_co_all, aes(x=YFP, y=CFP)) + geom_point() + facet_wrap(~Plasmid, nrow = 2 ) This looks great, but the names for the column ‘Plasmids’ are not informative so let’s fix that in the dataframe. The last two letter, either YC or CY indicate the order of the fluorescent proteins. The first letter indicates the system that was used for the co-expressing, with B = dual promotor, I = Ires and T = 2A. First, we split the column ‘Plasmid’: df_co_all %&gt;% separate(Plasmid, into = c(&#39;System&#39;, &#39;Order&#39;), sep = &quot;&quot;) %&gt;% head() # A tibble: 6 × 4 System Order YFP CFP &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 &quot;&quot; B 88.8 114. 2 &quot;&quot; B 95.1 119. 3 &quot;&quot; B 99.6 208. 4 &quot;&quot; B 106. 275. 5 &quot;&quot; B 124. 163. 6 &quot;&quot; B 142. 225. This doesn’t look right, since the first column is empty, so let’s try again with: df_co_all_split &lt;- df_co_all %&gt;% separate(Plasmid, into = c(NA, &#39;System&#39;, &#39;Order&#39;), sep = &quot;&quot;) head(df_co_all_split) # A tibble: 6 × 4 System Order YFP CFP &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 B C 88.8 114. 2 B C 95.1 119. 3 B C 99.6 208. 4 B C 106. 275. 5 B C 124. 163. 6 B C 142. 225. This looks good, and so we can replace the characters with names: df &lt;- df_co_all_split %&gt;% mutate(System = case_when(System == &#39;B&#39; ~ &quot; Dual-promotor&quot;, System == &#39;I&#39; ~ &quot; IRES&quot;, System == &#39;T&#39; ~ &#39;2A-peptide&#39;)) %&gt;% mutate(Order = case_when(Order == &#39;C&#39; ~ &quot;CFP first&quot;, Order == &#39;Y&#39; ~ &quot;YFP first&quot;)) head(df) # A tibble: 6 × 4 System Order YFP CFP &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 &quot; Dual-promotor&quot; CFP first 88.8 114. 2 &quot; Dual-promotor&quot; CFP first 95.1 119. 3 &quot; Dual-promotor&quot; CFP first 99.6 208. 4 &quot; Dual-promotor&quot; CFP first 106. 275. 5 &quot; Dual-promotor&quot; CFP first 124. 163. 6 &quot; Dual-promotor&quot; CFP first 142. 225. ggplot(df, aes(x=YFP, y=CFP)) + geom_point(size = 0.5, alpha=0.5) + facet_grid(Order~System) This looks a lot like the original figure in the paper (figure 1: https://doi.org/10.1371/journal.pone.0027321), but we still need to add the fits and optimize the layout. So let’s first assign the plot to an object and optimize the theme: p &lt;- ggplot(df, aes(x=YFP, y=CFP)) + geom_point(size = 0.5, alpha=0.5) + facet_grid(Order~System) p &lt;- p + geom_smooth(method = &quot;lm&quot;, formula = y~x+0) p &lt;- p + theme_minimal(14) p Again, I’m not the biggest fan of grids, so I renmove the grids here. To indicate the area of the individual plots, I add a light blueish background with a thin grey border and the ‘strips’ with labels for the facets are a bit darker shade of blue: p &lt;- p + theme(axis.text = element_blank(), panel.grid = element_blank(), strip.background = element_rect(fill=&#39;#E0E0FF&#39;, color=&quot;grey90&quot;, size = .5), panel.background = element_rect(fill=&#39;#F4F4FF&#39;, color=&quot;grey90&quot;), plot.caption = element_text(color=&#39;grey80&#39;, hjust=1), NULL) p I’m happy with the layout, so it’s time to adjust the labels. The units for the intensity are arbitrary, so we indicate this with a.u. for arbitrary units: p &lt;- p + labs(x=&quot;yellow fluorescence [a.u.]&quot;, y=&quot;cyan fluorescence [a.u.]&quot;, title = &quot;Correlations for co-expression systems&quot;, caption = &quot;\\n@joachimgoedhart\\nbased on data from Goedhart et al., DOI: 10.1371/journal.pone.0027321&quot;, tag = &quot;Protocol 16&quot; ) p 4.16.3 Fit parameters This looks great, but we do not have the numbers for the correlations yet. So let’s calculate those and add them to a new dataframe ‘df_cor’: df_cor &lt;- df %&gt;% group_by(System, Order) %&gt;% drop_na() %&gt;% summarize(n=n(), R_squared = cor(CFP, YFP, method = &quot;pearson&quot;)^2) `summarise()` has grouped output by &#39;System&#39;. You can override using the `.groups` argument. df_cor # A tibble: 6 × 4 # Groups: System [3] System Order n R_squared &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 &quot; Dual-promotor&quot; CFP first 734 0.483 2 &quot; Dual-promotor&quot; YFP first 338 0.613 3 &quot; IRES&quot; CFP first 294 0.873 4 &quot; IRES&quot; YFP first 497 0.785 5 &quot;2A-peptide&quot; CFP first 629 0.944 6 &quot;2A-peptide&quot; YFP first 840 0.968 We can use the dataframe with correlation values to add these to the plot. We use geom_text() with the ‘df_cor’ dataframe. The main challenges are (i) to position the text at the correct place and (ii) to add a nice label to indicate that the value is R squared. Positioning of the label is defined by the x- and y-coordinate in combination with hjust and vjust. My approach to get the label at the correct place is usually trial and error. We can use ‘x=Inf’, to assign an infinite value to x, implying that it will be located at the extreme right of the plot. To position the label on the left side of the x-coordinate, we can use ‘hjust=1’. But to give it a bit more space, I prefer here ‘hjust=1.1’. Similarly, we use 0 for y-coordinate and ‘vjust=0’ to define that the middle position of the label is at this coordinate (‘vjust=1’ moves the label down and ‘vjust=-1’ moves the label up.). To add a styled R-squared to the value we use the paste() function to add text preceding the actual value. We will round down the value to two digits using round(). Finally we can set ‘parse = TRUE’ to make sure that the label is styled according to the plotmath() convention. Here that means that the caret produces superscript and that the double equal sign is reduced to a single equal sign: # Code to add r-squared to the plot p &lt;- p + geom_text(data=df_cor, x=Inf, y=0, hjust=1.1, vjust=0, aes(label=paste0(&#39;R^2 == &#39;,round(R_squared,2))), parse = TRUE, color=&#39;darkslateblue&#39; ) p Finally, we can save the plot: png(file=paste0(&quot;Protocol_16.png&quot;), width = 4000, height = 3000, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 4.17 Protocol 17 - Animated plots of data from optogenetics experiments To study dynamic processes, data is measured at multiple points in time. This type of data is often presented in lineplots (see for instance protocols 3 &amp; 7). To better visualize the dynamics, animated versions of lineplots can be made. These are nice for presentations or websites. This protocol explains how to generate an animated lineplot from timeseries data. The data is originally published by Mahlandt et al (2022). Load the required packages: library(tidyverse) library(gganimate) First, we load the data from an excel file: df &lt;- readxl::read_excel(&#39;data/210222_OptoG08_02_Sheldon_TIAM_tidy_02.xlsx&#39;) head(df) # A tibble: 6 × 6 unique_id id Time Sample Value type &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 Sheldon_A1_R Sheldon 0.000355 A1_R 836. Control 2 Sheldon_A1_R Sheldon 0.00761 A1_R 836. Control 3 Sheldon_A1_R Sheldon 0.0104 A1_R 835. Control 4 Sheldon_A1_R Sheldon 0.0132 A1_R 834. Control 5 Sheldon_A1_R Sheldon 0.0159 A1_R 834. Control 6 Sheldon_A1_R Sheldon 0.0187 A1_R 833. Control The entire dataset is quite large and to simplify the presentation, we select a timerange between 17.75 and 18.8378 (this is the time in hours). In this timerange the dynamics can be nicely observed: df_sub &lt;- df %&gt;% filter(Time &gt;= 17.75 &amp; Time &lt;= 18.8378) There are two factors in the column ‘type’, namely Control and OptoTiam. We calculate the summary of the measurements and labele the new column ‘Value’: df_sub_summary &lt;- df_sub %&gt;% group_by(type, Time) %&gt;% summarize(mean=mean(Value)) `summarise()` has grouped output by &#39;type&#39;. You can override using the `.groups` argument. Let’s make a first plot to visualize the data: ggplot(df_sub_summary, aes(x=Time, y=mean, color = type)) + geom_line(linewidth=2) This looks good, so let’s define an object that contains the plot: p &lt;- ggplot(df_sub_summary, aes(x=Time, y=mean, color = type)) + geom_line(linewidth=2) + geom_point(aes(fill=type), color=&quot;black&quot;, size=4, shape=21) Note that we add a dot as well, by ‘geom_point()’. This will looks messy in the static plot, but it is a nice addition for the dynamic plot. p &lt;- p + theme_light(base_size=14) + theme(panel.grid = element_blank()) + theme(legend.position=&quot;none&quot;) p In this experiment, cells are exposed to blue light (optogenetics) and this triggers a change in the measured value. So below, we define two lightblue rectangles that indicate the illumination with blue light: p &lt;- p + annotate(&quot;rect&quot;, xmin = 17.8333, xmax = 18.0833 , ymin = -Inf, ymax = Inf, fill=&quot;blue&quot;, alpha=0.1) + annotate(&quot;rect&quot;, xmin = 18.3333, xmax = 18.5833 , ymin = -Inf, ymax = Inf, fill=&quot;blue&quot;, alpha=0.1) p Now, we change the colors of the two lines and dots: p &lt;- p + scale_color_manual(values = c(&quot;#BEBEBE&quot;, &quot;darkorchid&quot;)) + scale_fill_manual(values = c(&quot;#BEBEBE&quot;, &quot;darkorchid&quot;)) p Next, we can add labels and titles: p &lt;- p + labs( title = &quot;Reversible increase of resistance in endothelial cells by optogenetics&quot;, x = &quot;Time [h]&quot;, y = &quot;Resistance [Ω]&quot;, caption = &quot;@joachimgoedhart\\n(based on data from Mahlandt et al, DOI: https://doi.org/10.1101/2022.10.17.512253)&quot;, tag = &quot;Protocol 17&quot; ) + theme(plot.caption = element_text(color=&#39;grey80&#39;, hjust=1)) This is the version that I save as a preview of the animation: png(file=paste0(&quot;Protocol_17.png&quot;), width = 4000, height = 3000, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 For the animated version, we can add a label for each of the two conditions. Note that (similar to the dots) it is plotted for each datapoint. But, in the animated version, only the leading (newly drawn) datapoint and label is shown. p &lt;- p + geom_label(aes(x = Time, y=mean, label=type), nudge_x = 0.02, size=4, hjust=0) Finally, we can turn this into an animated object with the function transition_reveal() from the {gganimate} package: a &lt;- p + transition_reveal(Time) To generate an animated plot, we used animate() and define the number of frames: animated_plot &lt;- animate(a, nframes=100, width = 1000, height = 750, units = &quot;px&quot;, res = 100) The result can be displayed by typing animated_plot in the command line: To save the animated object a to disk, use: animate(a, 70, renderer = gifski_renderer(&quot;Protocol_17.gif&quot;), width = 1000, height = 750, units = &quot;px&quot;, res = 100) 4.18 Protocol 18 - Combining animated plots with a movie In protocol 17 an animated plot is generated from time series data. Here, we will see how an animated plot can be combined with a movie from imaging data. The data is originally published by Mahlandt et al (2022). We will plot the cell area over time and the movie that shows the area of individual cells. Load the required packages: library(tidyverse) library(gganimate) Read the excel file with the data: df &lt;- readxl::read_excel(&#39;data/Summary_GlobalActivation_TimeTrace_TIAM_01.xlsx&#39;) head(df) # A tibble: 6 × 45 Experiment Construct Cell_Experiment Cell Time Frame AreaShape_Area &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 210429_OptoG011_03… OptoTIAM Cell_01 Cell… 15 1 32774 2 210429_OptoG011_03… OptoTIAM Cell_01 Cell… 30 2 32514 3 210429_OptoG011_03… OptoTIAM Cell_01 Cell… 45 3 32371 4 210429_OptoG011_03… OptoTIAM Cell_01 Cell… 60 4 32082 5 210429_OptoG011_03… OptoTIAM Cell_01 Cell… 75 5 31799 6 210429_OptoG011_03… OptoTIAM Cell_01 Cell… 90 6 31296 # ℹ 38 more variables: AreaShape_BoundingBoxArea &lt;dbl&gt;, # AreaShape_BoundingBoxMaximum_X &lt;dbl&gt;, AreaShape_BoundingBoxMaximum_Y &lt;dbl&gt;, # AreaShape_BoundingBoxMinimum_X &lt;dbl&gt;, AreaShape_BoundingBoxMinimum_Y &lt;dbl&gt;, # AreaShape_Center_X &lt;dbl&gt;, AreaShape_Center_Y &lt;dbl&gt;, # AreaShape_Compactness &lt;dbl&gt;, AreaShape_Eccentricity &lt;dbl&gt;, # AreaShape_EquivalentDiameter &lt;dbl&gt;, AreaShape_EulerNumber &lt;dbl&gt;, # AreaShape_Extent &lt;dbl&gt;, AreaShape_FormFactor &lt;dbl&gt;, … The first timepoint is 15 and to set that to zero: df &lt;- df %&gt;% mutate(Time=Time-15) The dataframe ‘df’ has data for a number of cells, time and the area with the column names ‘Cell’, ‘Time’ and ‘AreaShape_Area’. To compare the area of the cells at the start of the experiment, we normalize the ‘AreaShape_Area’ to the initial value dividing all values of a single cell trace by the value of the first timepoint: df &lt;- df %&gt;% group_by(Cell) %&gt;% arrange(Time) %&gt;% mutate(Area_norm=AreaShape_Area/AreaShape_Area[1]) %&gt;% ungroup() Next, we calculate the average response and store this in a new dataframe: df_summary &lt;- df %&gt;% group_by(Construct, Time) %&gt;% summarize(mean=mean(Area_norm)) `summarise()` has grouped output by &#39;Construct&#39;. You can override using the `.groups` argument. Let’s make a plot of the average: ggplot(df_summary, aes(x=Time, y=mean)) + geom_line(size=2, color=&#39;darkorchid4&#39;) This is an optogenetic experiment, where the light is turned on between timepoints 135 and 720, and we can define a new column ‘light’ that stores this information: df_summary &lt;- df_summary %&gt;% mutate(light = case_when(Time &gt;= 135 &amp; Time &lt;= 720 ~ &quot;ON&quot;, TRUE ~ &quot;OFF&quot; ) ) This information on the light condition needs to be plotted ‘behind’ the data, and so we define it as the first layer: p &lt;- ggplot(df_summary, aes(x=Time, y=mean)) + geom_tile(data=df_summary, aes(x=Time, y=1, height = Inf, width = 15, fill = light, group = seq_along(Time)), alpha=1) p Note that we use here group = seq_along(Time)). This will be explained below, when the animated object is generated. The blocks in the plot are indicating the different light conditions. The colors can be changed, so that a grey color is displayed when the light is off, and a cyan rectangle shows when the light was switched on: p &lt;- p + scale_fill_manual(values = c(&quot;grey&quot;, &quot;cyan&quot;)) p Now, let’s add the data for both the mean and the individual cells: p &lt;- p + geom_line(size=2, color=&#39;darkorchid4&#39;) + geom_line(data=df, aes(x=Time, y=Area_norm, group=Cell), alpha=0.3, color=&#39;darkorchid4&#39;) p Some styling: p &lt;- p + theme_light(base_size=18) + theme(panel.grid = element_blank(), legend.position = &quot;top&quot;, legend.justification = &quot;left&quot;, legend.box.margin = margin(0,0,0,-40), plot.caption = element_text(color=&#39;grey80&#39;, hjust=1)) p To add a dot that is like the tip of a pen, drawing the line, we can use geom_point(). Here it is only added for the average curve: p &lt;- p + geom_point(color=&#39;darkorchid4&#39;, size=4) p In the plot, the dot is shown for each timepoint, but in the animation it will only be drawn for the active frame. This is unlike the line, which remains visible. More detail about this behavior can be found below. Finally, let’s change the labels for the axes and set the scale. We use expand = FALSE to use the exact limits that we have specified: p &lt;- p + labs(x=&#39;Time [s]&#39;, y=&#39;Area [µm]&#39;, caption=&#39;\\n@joachimgoedhart\\nbased on data from Mahlandt et al., DOI: 10.1101/2022.10.17.512253&#39;) + coord_cartesian(xlim = c(0,1305), ylim = c(0.8,1.45), expand = FALSE) For the animation, we need to know the number of timepoints, as we will use this number to generate the same amount of ‘frames’ in the animation: length(df_summary$Time) [1] 88 So the number of frames should be 88. We use to plot object p as the input for the animation and generate the animation with the function transition_reveal(), which will reveal the data over time. The result is stored in the object a: a &lt;- p + transition_reveal(Time) Now, we can render the animation with a defined number of frames and size. Since we will display the movie and plot side-by-side, the height of both panels should be identical. The movie has a height of 900 pixels, so we use this to set the dimension of the animated plot: animated_plot &lt;- animate(plot = a, nframes = 88, width = 900, height = 900, units = &quot;px&quot;, res = 100, renderer = magick_renderer()) `geom_line()`: Each group consists of only one observation. ℹ Do you need to adjust the group aesthetic? `geom_line()`: Each group consists of only one observation. ℹ Do you need to adjust the group aesthetic? `geom_line()`: Each group consists of only one observation. ℹ Do you need to adjust the group aesthetic? `geom_line()`: Each group consists of only one observation. ℹ Do you need to adjust the group aesthetic? The result is animated_plot which is a “magick-image”: class(animated_plot) [1] &quot;magick-image&quot; This will allow us to combine it with a GIF using the {magick} package. So it is important to define the ‘renderer’ in the animate()function. 4.18.1 Intermezzo A few words on the animated objects. Using the function transition_reveal(), the lines are revealed over time. Objects like the dot and the mean are only displayed at the location that is defined by a specific time and do not stay. A similar behaviour would be the default for the geom_tile(). However, here we want the area to be drawn, which means that the area that was drawn before should remain visible. To achieve that, we added group = seq_along(Time) to geom_tile(). The same could be done for the geom_point() when the points should remain visible for the entire trace. 4.18.2 Time for some magick Now it’s time to load the movie. We will use the {magick} package to load the GIF and to combine them: library(magick) Linking to ImageMagick 6.9.12.93 Enabled features: cairo, fontconfig, freetype, heic, lcms, pango, raw, rsvg, webp Disabled features: fftw, ghostscript, x11 movie &lt;- image_read(&quot;data/OptoTIAM_movie.gif&quot;) To test whether we can combine the movie and plot, we start by combining the first frame and verify that the resulting image looks right: combined_gif &lt;- image_append(c(movie[1], animated_plot[1])) plot(as.raster(combined_gif)) To combine all other frames and add them to the object ‘combined_gif’ we use a for loop: for (i in 2:88) { combined_panel &lt;- image_append(c(movie[i], animated_plot[i])) combined_gif &lt;- c(combined_gif, combined_panel) } To show the combined GIF: combined_gif To save a preview, we can selected one frame of the movie and save it: montage &lt;- image_montage(c(combined_gif[22],combined_gif[44],combined_gif[66],combined_gif[88]), geometry = &quot;x400+0+100&quot;, tile = &#39;2x2&#39;) image_write_gif(montage, &#39;Protocol_18.png&#39;) To write the combined GIF to a file: image_write_gif(combined_gif, &#39;Protocol_18.gif&#39;) 4.19 Protocol 19 - Plotting ratiometric FRET data In this protocol, we plot the intensity data that are quantified by ImageJ/FIJI. The image analysis is originally reported in a preprint by Mahlandt and Goedhart (2021) in section 3.2.2. The original R-script is available on Github. Here, all steps are explained in protocol style. library(tidyverse) We can load the output from FIJI data from the Github repository: df_CFP &lt;- read.csv(&quot;https://raw.githubusercontent.com/JoachimGoedhart/TimeLapse-Imaging-DataViz/main/3_2_2_RatioImaging/Results-CFP.csv&quot;) df_YFP &lt;- read.csv(&quot;https://raw.githubusercontent.com/JoachimGoedhart/TimeLapse-Imaging-DataViz/main/3_2_2_RatioImaging/Results-YFP.csv&quot;) To avoid the repetition of actions on both dataframes, we merge them: df_ratio &lt;- bind_rows(df_CFP, df_YFP) head(df_ratio) X Label Mean Slice 1 1 CFP:Cell-01:180228_YCaM3.60_His_Py-02_w1YCam-CFP_t1 12.31 1 2 2 CFP:Cell-02:180228_YCaM3.60_His_Py-02_w1YCam-CFP_t1 14.04 1 3 3 CFP:Cell-03:180228_YCaM3.60_His_Py-02_w1YCam-CFP_t1 16.08 1 4 4 CFP:Cell-04:180228_YCaM3.60_His_Py-02_w1YCam-CFP_t1 18.39 1 5 5 CFP:Cell-01:180228_YCaM3.60_His_Py-02_w1YCam-CFP_t2 12.23 2 6 6 CFP:Cell-02:180228_YCaM3.60_His_Py-02_w1YCam-CFP_t2 13.90 2 And let’s look at the last rows of this dataframe tail(df_ratio) X Label Mean Slice 963 479 YFP:Cell-03:180228_YCaM3.60_His_Py-02_w2YCam-YFP_t120 161.79 120 964 480 YFP:Cell-04:180228_YCaM3.60_His_Py-02_w2YCam-YFP_t120 191.84 120 965 481 YFP:Cell-01:180228_YCaM3.60_His_Py-02_w2YCam-YFP_t121 127.41 121 966 482 YFP:Cell-02:180228_YCaM3.60_His_Py-02_w2YCam-YFP_t121 148.50 121 967 483 YFP:Cell-03:180228_YCaM3.60_His_Py-02_w2YCam-YFP_t121 161.95 121 968 484 YFP:Cell-04:180228_YCaM3.60_His_Py-02_w2YCam-YFP_t121 191.58 121 This looks right and the format of the dataframe is in a long format. However, the first column contains multiple conditions, or IDs, separated by a colon, so we split those with separate(): df_split_ratio &lt;- df_ratio %&gt;% separate(Label,c(&quot;filename&quot;, &quot;Sample&quot;,&quot;Number&quot;),sep=&#39;:&#39;) str(df_split_ratio) &#39;data.frame&#39;: 968 obs. of 6 variables: $ X : int 1 2 3 4 5 6 7 8 9 10 ... $ filename: chr &quot;CFP&quot; &quot;CFP&quot; &quot;CFP&quot; &quot;CFP&quot; ... $ Sample : chr &quot;Cell-01&quot; &quot;Cell-02&quot; &quot;Cell-03&quot; &quot;Cell-04&quot; ... $ Number : chr &quot;180228_YCaM3.60_His_Py-02_w1YCam-CFP_t1&quot; &quot;180228_YCaM3.60_His_Py-02_w1YCam-CFP_t1&quot; &quot;180228_YCaM3.60_His_Py-02_w1YCam-CFP_t1&quot; &quot;180228_YCaM3.60_His_Py-02_w1YCam-CFP_t1&quot; ... $ Mean : num 12.3 14 16.1 18.4 12.2 ... $ Slice : int 1 1 1 1 2 2 2 2 3 3 ... We can have a first glance at the data: ggplot(df_split_ratio, aes(x=Slice, y=Mean, by=Sample)) + geom_line() + facet_wrap(~filename) To get rid of cell-to-cell variation in expression levels, we normalize the data by dividing all data by the average value of the first 5 timepoints (a.k.a. the baseline): df_split_ratio &lt;- df_split_ratio %&gt;% group_by(Sample, filename) %&gt;% mutate(Mean_norm = Mean/mean(Mean[1:5])) ggplot(df_split_ratio, aes(x=Slice, y=Mean_norm, by=Sample)) + geom_line() + facet_wrap(~filename) This looks good. To display the FRET ratio the data from the YFP channel are divided by that of the CFP channel. To achieve that, I will first make the data wider with pivot_wider() by defining two columns (this is close to what’s done in protocol 16). One with the YFP intensities and one with the CFP intensities: df_wider_ratio &lt;- df_split_ratio %&gt;% pivot_wider(names_from = filename, values_from = Mean_norm) head(df_wider_ratio) # A tibble: 6 × 7 # Groups: Sample [4] X Sample Number Mean Slice CFP YFP &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 Cell-01 180228_YCaM3.60_His_Py-02_w1YCam-CFP_t1 12.3 1 1.03 NA 2 2 Cell-02 180228_YCaM3.60_His_Py-02_w1YCam-CFP_t1 14.0 1 1.03 NA 3 3 Cell-03 180228_YCaM3.60_His_Py-02_w1YCam-CFP_t1 16.1 1 1.02 NA 4 4 Cell-04 180228_YCaM3.60_His_Py-02_w1YCam-CFP_t1 18.4 1 1.03 NA 5 5 Cell-01 180228_YCaM3.60_His_Py-02_w1YCam-CFP_t2 12.2 2 1.03 NA 6 6 Cell-02 180228_YCaM3.60_His_Py-02_w1YCam-CFP_t2 13.9 2 1.02 NA This is not what I wanted, as the YFP column shows NA . The reason is that the columns “Number” and “Mean” are not identical for the CFP and YFP data. So I will do this again, but first get rid of these columns: df_wider_ratio &lt;- df_split_ratio %&gt;% dplyr::select(-c(Number, Mean)) %&gt;% pivot_wider(names_from = filename, values_from = Mean_norm) head(df_wider_ratio) # A tibble: 6 × 5 # Groups: Sample [4] X Sample Slice CFP YFP &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 Cell-01 1 1.03 0.991 2 2 Cell-02 1 1.03 1.01 3 3 Cell-03 1 1.02 1.00 4 4 Cell-04 1 1.03 1.00 5 5 Cell-01 2 1.03 0.999 6 6 Cell-02 2 1.02 1.00 The data is ow in the right format and the calculation the ratio is straightforward: df_wider_ratio &lt;- df_wider_ratio %&gt;% mutate (Ratio = YFP/CFP) head(df_wider_ratio) # A tibble: 6 × 6 # Groups: Sample [4] X Sample Slice CFP YFP Ratio &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 Cell-01 1 1.03 0.991 0.959 2 2 Cell-02 1 1.03 1.01 0.980 3 3 Cell-03 1 1.02 1.00 0.980 4 4 Cell-04 1 1.03 1.00 0.976 5 5 Cell-01 2 1.03 0.999 0.973 6 6 Cell-02 2 1.02 1.00 0.987 From the experimental settings, we know that each slice takes two seconds, so we can add a column with ‘Time’ data: df_wider_ratio &lt;- df_wider_ratio %&gt;% mutate(Time=Slice*2) To plot the ratio data: p &lt;- ggplot(df_wider_ratio, aes(x=Time, y=Ratio, group=Sample, color=Sample)) + geom_line(size=1) + geom_point(size=2) p The data is in correct shape and we can start to work on the layout of the plot. The response that is observed is triggered by receptor activation at time point t=25 until t=175. We can display this time window by adding a rectangle with the function annotate(). We also change the theme: p &lt;- p + annotate(&quot;rect&quot;,xmin=25,xmax=175,ymin=-Inf,ymax=Inf,alpha=0.1,fill=&quot;black&quot;) + theme_light(base_size = 16) p Let’s remove the grid and change the labels of the axes: p &lt;- p + labs(x = &quot;Time [s]&quot;, y = &quot;Normalized Ratio&quot;) + labs(title = &quot;Calcium oscillations induced by histamine&quot;, tag = &quot;Protocol 19&quot;) + theme(panel.grid = element_blank()) p The heterogeneity is best display by zooming in (to a timerange of 0-200s) and by displaying the plots individually: p &lt;- p + coord_cartesian(xlim = c(0,190), ylim = c(0.8,5.9), expand = FALSE) + facet_wrap(~Sample) p The ‘Sample’ is indicatd twice in this plot and we can get rid of at least one label. However, since I’m interested in displaying the heterogeneity and not in connecting the plots to a specific sample, I’ll get rid of both labels: To remove the labels and strips for the facets: p &lt;- p + theme(strip.text.x = element_blank()) To remove the legend and add a : p &lt;- p + theme(legend.position = &quot;none&quot;) p To save the plot: png(file=paste0(&quot;Protocol_19.png&quot;), width = 4000, height = 3000, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 4.20 Protocol 20 - On- and off-kinetics This document explains how to perform an exponential fit in R. It relies heavy on this blog by Douglas Watson: https://douglas-watson.github.io/post/2018-09_exponential_curve_fitting/ We use data that was originally published in figure 1 of Mahlandt et al (2022). Load packages needed to wrangle the data and for plotting: library(tidyverse) We can also define custom colors and set the theme for the plots: #Nature themed color palette by Eike =D #rapamycin would be springgreen4, magnets are cornflowerblue and iLID is darkorchid4 newColors &lt;- c(&quot;cornflowerblue&quot;, &quot;darkorchid4&quot;, &quot;springgreen4&quot;) 4.20.1 Reading and processing the data Read the data. Since this dataset has “N.A.” that need to be converted to ‘NA’ for proper handling, we define this in ‘na.strings’ and after that the rows with ‘NA’ are removed: df &lt;- read.csv(&quot;data/OptoG003_Kinetics_Summary_timeZero.csv&quot;, na.strings = &quot;N.A.&quot;) %&gt;% na.omit() head(df) Replicate Experiment Construct Cell 1 1 211215_OptoG003_01_4500_4516_iLID_P2_Cell_01 iLID Cell_01 2 1 211215_OptoG003_01_4500_4516_iLID_P2_Cell_01 iLID Cell_01 3 1 211215_OptoG003_01_4500_4516_iLID_P2_Cell_01 iLID Cell_01 4 1 211215_OptoG003_01_4500_4516_iLID_P2_Cell_01 iLID Cell_01 5 1 211215_OptoG003_01_4500_4516_iLID_P2_Cell_01 iLID Cell_01 6 1 211215_OptoG003_01_4500_4516_iLID_P2_Cell_01 iLID Cell_01 Time_adjusted Frame Membrane Cytosol Ratio 1 -12.95 1 200.23 356.14 0.5622227 2 -10.36 2 202.58 362.64 0.5586256 3 -7.77 3 212.70 357.81 0.5944496 4 -5.18 4 218.74 353.31 0.6191164 5 -2.59 5 205.28 347.05 0.5914998 6 0.00 6 215.72 348.98 0.6181443 Two of the columns are renamed. And the order of the factors in the new column ‘System’ is defined (this will define the order in which the plots are shown): df &lt;- df %&gt;% rename(Time=Time_adjusted, System=Construct) %&gt;% mutate(`System` = forcats::fct_relevel(`System`, c(&quot;magnets&quot;, &quot;iLID&quot;, &quot;rapamycin&quot;))) Let’s have a look at the data: ggplot(df, aes(x=Time, y=Ratio))+geom_line(aes(group=Experiment), alpha=0.5)+facet_wrap(~System) To summarise the data and get the mean for each condition: df_summary &lt;- df %&gt;% group_by(Time, System) %&gt;% summarise(n=n(), mean=mean(Ratio)) `summarise()` has grouped output by &#39;Time&#39;. You can override using the `.groups` argument. head(df_summary) # A tibble: 6 × 4 # Groups: Time [2] Time System n mean &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; 1 -13.0 magnets 12 0.604 2 -13.0 iLID 17 0.553 3 -13.0 rapamycin 11 0.781 4 -10.4 magnets 12 0.581 5 -10.4 iLID 17 0.550 6 -10.4 rapamycin 11 0.788 Let’s plot the average values: ggplot(df_summary, aes(x=Time, y=mean))+geom_point(size=2, alpha=0.8)+facet_wrap(~System) The data shows an increase and decrease of signal. The increase in the signal is due to blue-light stimulation and starts at t=0. Note that the systems ‘iLID’ and ‘magnets’ are reversible and have an on-rate and an off-rate. The rapamycin system is irreversible and only has an on-rate. To determine the rates, we will filter the relevant time-window. 4.20.2 Trial fitting on a single trace First we define a dataframe that only has the iLID data from t=0 until t=100: df_ilid &lt;- df_summary %&gt;% filter(System == &quot;iLID&quot;) %&gt;% filter(Time&gt;=0 &amp; Time&lt;100) To fit the data we use the function nls(). First, let’s look at an example for a linear fit with the unknown variables ‘a’ and ‘b’: #linear fit fit &lt;- nls(mean ~ a * Time + b, data = df_ilid, start = list(a = 1, b = 0)) The coefficients of the fit can be retrieved by: coef(fit) a b 0.00201496 0.87887972 This will not be very accurate, but it illustrates how it works. Now let’s use a better model and fit an exponential: #exponential fit: a0 is starting value, a is the amplitude, k is the rate constant fit &lt;- nls(mean ~ a0 + a*(1- exp(-k * Time)), data = df_ilid, start = list(a0 = 0, a = 2, k = .1) ) The variables are ‘a0’, ‘a’ and ‘k’, which reflect the value at t=0, the amplitude and the rate. The values are: coef(fit) a0 a k 0.5505354 0.4681990 0.1349511 To plot the fit together with the data, we use the function augment() from the broom package: library(broom) ggplot(data = augment(fit), aes(x=Time, y=mean)) + geom_point(size=4, alpha=.5) + geom_line(aes(y=.fitted), color=&quot;black&quot;, size=1) This looks like a good fit. Now let’s determine the half time, which is defines as ln2/k, where ln2 is the natural logarithm of 2. By default the log() function calculates the natural logarithm, so we use log(2). The value of ‘k’ is the third variable of the list that is returned by coef(fit): var &lt;- coef(fit) paste(&quot;The halftime is&quot;,log(2)/var[3]) [1] &quot;The halftime is 5.13628563844601&quot; 4.20.3 On kinetics - alltogether Let’s try to do the fitting on all three curves at once. First we need to filter the data to get the relevant part of the trace for determining the on-kinetics and we store this is a new dataframe df_fit_on: df_fit_on &lt;- df_summary %&gt;% filter(Time&gt;=0) df_fit_on &lt;- df_fit_on %&gt;% filter(System==&quot;iLID&quot; &amp; Time&lt;100 | System==&quot;magnets&quot; &amp; Time&lt;100 | System==&quot;rapamycin&quot;) ggplot(df_fit_on, aes(x=Time, y=mean))+geom_point(size=2, alpha=0.8)+facet_wrap(~System)+xlim(0,200) This looks good. To fit the data, we use an approach that uses nest() to generate a nested dataframe. The map() function is used to apply the fit to each System. fitted &lt;- df_fit_on %&gt;% nest(-System) %&gt;% mutate( fit = map(data, ~nls(mean ~ a0 + a*(1- exp(-k * Time)), data = ., start = list(a0 = 0, a = 2, k = .1) )), tidied = map(fit, tidy), augmented = map(fit, augment), ) fitted # A tibble: 3 × 5 System data fit tidied augmented &lt;fct&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; 1 magnets &lt;gropd_df [39 × 3]&gt; &lt;nls&gt; &lt;tibble [3 × 5]&gt; &lt;tibble [39 × 4]&gt; 2 iLID &lt;gropd_df [39 × 3]&gt; &lt;nls&gt; &lt;tibble [3 × 5]&gt; &lt;tibble [39 × 4]&gt; 3 rapamycin &lt;gropd_df [89 × 3]&gt; &lt;nls&gt; &lt;tibble [3 × 5]&gt; &lt;tibble [89 × 4]&gt; The resulting dataframe fitted holds the estimated variables and these can be shown by using the function unnest() as follows: fitted %&gt;% unnest(tidied) %&gt;% dplyr::select(System, term, estimate) %&gt;% spread(term, estimate) %&gt;% rename(Amplitude=a) %&gt;% mutate(`t1/2`=log(2)/k) # A tibble: 3 × 5 System Amplitude a0 k `t1/2` &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 magnets 0.298 0.599 0.175 3.97 2 iLID 0.468 0.551 0.135 5.14 3 rapamycin 1.98 0.573 0.0161 43.1 Finally, we can extract the fit and plot it on top of the experimental data: augmented &lt;- fitted %&gt;% unnest(augmented) ggplot(data = augmented, aes(x=Time, y=mean, colour = System)) + geom_point(size=4, alpha=.5) + geom_line(aes(y=.fitted), color=&quot;black&quot;, size=1) + facet_wrap(~System) + xlim(0,100) + scale_color_manual(values = newColors) 4.20.4 Off kinetics In a similar way, we can fit the off-kinetics. First we need to select the right time window: df_fit_off &lt;- df_summary %&gt;% mutate(Time=Time-121) df_fit_off &lt;- df_fit_off %&gt;% filter(System==&quot;iLID&quot; &amp; Time&gt;=0 | System==&quot;magnets&quot; &amp; Time&gt;=0) ggplot(df_fit_off, aes(x=Time, y=mean))+geom_point(size=2, alpha=0.8)+facet_wrap(~System)+xlim(0,200) Next, we can do the fitting: fitted_off &lt;- df_fit_off %&gt;% nest(-System) %&gt;% mutate( fit = map(data, ~nls(mean ~ a0 + a*(1- exp(-k * Time)), data = ., start = list(a0 = 2, a = -2, k = .1) )), tidied = map(fit, tidy), augmented = map(fit, augment), ) To get the fit parameters: fit_results &lt;- fitted_off %&gt;% unnest(tidied) %&gt;% dplyr::select(System, term, estimate) %&gt;% spread(term, estimate) %&gt;% rename(Amplitude=a) %&gt;% mutate(`t1/2`=log(2)/k) fit_results # A tibble: 2 × 5 System Amplitude a0 k `t1/2` &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 magnets -0.264 0.810 0.0373 18.6 2 iLID -0.342 0.948 0.0342 20.3 And finally, we can inspect the fit: augmented &lt;- fitted_off %&gt;% unnest(augmented) p &lt;- ggplot(data = augmented, aes(x=Time, y=mean, colour = System)) + geom_point(size=4, alpha=.5) + geom_line(aes(y=.fitted), color=&quot;black&quot;, size=1) + facet_wrap(~System) + scale_color_manual(values = newColors) p Finally, we can adjust the layout and titles: p &lt;- p + labs( title = &quot;Off-kinetics of two optogenetic systems&quot;, x = &quot;Time [s]&quot;, y = &quot;Response [arbitrary units]&quot;, caption = &quot;@joachimgoedhart\\nbased on data from Mahlandt et al., DOI: 10.1101/2022.10.17.512253&quot;, tag = &quot;Protocol 20&quot; ) + theme(plot.caption = element_text(color = &quot;grey80&quot;), legend.position = &quot;none&quot;) p The data and fit look good, let’s add the t1/2 values as well. Protocol 16 explains how to add labels to plots that uses ‘facets’ and we use that approach here too: # Code to add r-squared to the plot p + geom_text(data=fit_results, x=Inf, y=0.9, hjust=1.1, vjust=0, aes(label=paste0(&#39;t½ = &#39;,round(`t1/2`,1), &#39;s&#39;)), size=5, color=&#39;grey40&#39; ) Let’s save the result: png(file=paste0(&quot;Protocol_20.png&quot;), width = 4000, height = 3000, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 4.21 Protocol 21 - A spiral plot In this protocol, we will plot time-series data in a spiral plot. This protocol is inspired by another application of a spiral plot to visualize data from several years. We will use the data that is supplied by addgene on requests for plasmids that we have deposited. We first load the {tidyverse} package: library(tidyverse) The data comes in a csv format, let’s load that and check its first 6 rows: df_addgene &lt;- read.csv(&quot;data/Addgene-Requests-for-Materials_MolCyto_March_2024.csv&quot;, stringsAsFactors = TRUE) head(df_addgene) ID Material Requesting.Country Date.Ordered 1 189773 pmScarlet3-Giantin_C1 UNITED STATES 03/18/2024 2 189768 pmScarlet3-alphaTubulin_C1 DENMARK 03/18/2024 3 189754 pDx_mScarlet3 UNITED STATES 03/15/2024 4 189767 pLifeAct-mScarlet3_N1 UNITED STATES 03/15/2024 5 176098 dimericTomato-2xrGBD FRANCE 03/15/2024 6 36201 pLifeAct-mTurquoise2 ISRAEL 03/14/2024 The column with dates should not be a factor as it is now, but rather be in a date format. Alternatively, and that is what we will do here, we can split this column into day/month/year. The argument convert = TRUE ensures conversion of the values into a integer: df_addgene &lt;- df_addgene %&gt;% separate(Date.Ordered, into = c(&quot;Month&quot;,&quot;Day&quot;,&quot;Year&quot;), sep = &quot;/&quot;, convert = TRUE) head(df_addgene) ID Material Requesting.Country Month Day Year 1 189773 pmScarlet3-Giantin_C1 UNITED STATES 3 18 2024 2 189768 pmScarlet3-alphaTubulin_C1 DENMARK 3 18 2024 3 189754 pDx_mScarlet3 UNITED STATES 3 15 2024 4 189767 pLifeAct-mScarlet3_N1 UNITED STATES 3 15 2024 5 176098 dimericTomato-2xrGBD FRANCE 3 15 2024 6 36201 pLifeAct-mTurquoise2 ISRAEL 3 14 2024 Let’s calculate the accumulated number of requests. In this dataframe, each row is a single request: total_requests &lt;- nrow(df_addgene) df_addgene$total &lt;- seq(total_requests, 1) Let’s plot how the number of requests evolved over months: ggplot(df_addgene, aes(Month, total, color=as.factor(Year))) + geom_line() The plot shows the accumulation of requests over the years. The zig-zag appears because there are multiple values for each month. We can simplify this dataframe to show only the maximal value for each month (per year): df_addgene_month &lt;- df_addgene %&gt;% group_by(Year, Month) %&gt;% summarise(total_max = max(total)) %&gt;% ungroup() `summarise()` has grouped output by &#39;Year&#39;. You can override using the `.groups` argument. Let’s plot these data again: ggplot(df_addgene_month, aes(Month, total_max, color=as.factor(Year))) + geom_line() This looks better, and we can now plot this in a circular fahsion with coord_polar(): ggplot(df_addgene_month, aes(Month, total_max, color=as.factor(Year))) + geom_line(linewidth=2) + coord_polar(clip = &#39;off&#39;) This looks good, but the lines show a step at the 0 degrees angle. This is because the data for the different years are connected to join all these data. There is a nice solution described here that we will use: https://stackoverflow.com/questions/41842249/join-gap-in-polar-line-ggplot-plot bridge &lt;- df_addgene_month[df_addgene_month$Month==12,] bridge$Year &lt;- bridge$Year +1 bridge$Month &lt;- 0 df &lt;- rbind(df_addgene_month, bridge) Now, plot again and change the colors: p &lt;- ggplot(df, aes(Month, total_max, color=as.factor(Year), fill=as.factor(Year))) + geom_line(linewidth=2) + coord_polar() p I do not like the girds and axis labels for these types of plots, so let’s get rid of those: p &lt;- p + theme_bw(base_size = 14) + labs(x=&quot;&quot;, y=&quot;&quot;, color=&quot;Year&quot;) + scale_y_continuous(limits = c(0,NA)) + guides(color = guide_legend(reverse = TRUE)) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.text.x = element_blank(), axis.title.y = element_text(hjust=.8), plot.caption = element_text(color=&#39;grey80&#39;, hjust=1, vjust=5)) p Adding titles: p &lt;- p + labs(title = &quot;Evolution of the total number of plasmids&quot;, subtitle = &quot;shared through addgene.org&quot;, tag = &quot;Protocol 21&quot;, caption=&quot;data provided by addgene.org - March 2024&quot;) p The legend is quite long as it lists all the years. We could list a subset by using scale_color_discrete(): p + scale_color_discrete(breaks=c(&quot;2011&quot;, &quot;2014&quot;, &quot;2017&quot;, &quot;2020&quot;, &quot;2023&quot;)) + labs(color = &quot;&quot;) The legend is less cluttered, but it is less clear how many years are shown and which line belongs to which year (especially if you are colorblind, like I am). Another way the make the legend less prominent is by reducing the size: p + theme(legend.text = element_text(size = 10), legend.key.height= unit(12, &#39;points&#39;)) Yep, that is better. Let’s see if we can also do some direct labeling. We need a separate dataframe to get there: df_label &lt;- df %&gt;% filter(Month==12) Now we take a subset, because we only want to label every nth line (every 5th in this example), to reduce the number of labels. We use slice() and since we want to include the most recent year, we first sort the order of the years with arrange(): df_label &lt;- df_label %&gt;% arrange(desc(Year)) %&gt;% slice(seq(1, nrow(df_label), 5)) df_label # A tibble: 3 × 3 Year Month total_max &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 2023 12 9881 2 2018 12 4394 3 2013 12 691 The df_label is now exactly as I wanted it. Let’s add the labels from this dataframe: p + geom_label(data=df_label, aes(x=Month, y=total_max, label=paste0(Year,&quot;: &quot;,total_max)), color=&quot;black&quot;, size=4, label.size = NA, alpha=0.8) Nice. We do not need the legend anymore. And the labels are a bit large, so let’s make them smaller and move them a bit upwards. By adding a triangle (this is defined by shape=25 within geom_point()), that is also slightly moved upwards in the plot, the labels nicely point to the correct line: p &lt;- p + theme(legend.position = &quot;none&quot;, axis.text = element_blank(), axis.ticks = element_blank()) + geom_point(data=df_label, aes(x=Month, y=total_max+600), color=&quot;black&quot;, shape=25, size=5) + geom_label(data=df_label, aes(x=Month, y=total_max+1200, label=paste0(Year,&quot;: &quot;,total_max)), color=&quot;black&quot;, size=4, # label.size = NA, alpha=1) + scale_y_continuous(limits = c(0,NA), expand = c(0,0)) Scale for y is already present. Adding another scale for y, which will replace the existing scale. p Let’s also add a label that shows the current, maximal value. First, we define the dataframe: df_label_last &lt;- df_addgene_month %&gt;% tail(1) We use this dataframe to add a label to the endpoint: p &lt;- p + geom_point(data=df_label_last, aes(x=Month, y=total_max), size=3) + geom_label(data=df_label_last, aes(x=Month, y=total_max+2000, label=paste0(total_max)), color=&quot;black&quot;, size=4, # label.size = NA, alpha=1) p Finally, we can save the plot: png(file=paste0(&quot;Protocol_21.png&quot;), width = 4000, height = 3000, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 4.22 Protocol 22 - Frequencies of discrete data In this protocol we will look at the data supplied by addgene on requests for plasmids that we have deposited. We first load the {tidyverse} package: library(tidyverse) Let’s load the data: df_addgene &lt;- read.csv(&quot;data/Addgene-Requests-for-Materials-Joachim-Goedhart-2005-2023-2023-Jan-31-05-05.csv&quot;, stringsAsFactors = TRUE) head(df_addgene) ID Material Requesting.Country Date.Ordered 1 176106 mNeonGreen-Paxillin UNITED STATES 01/25/2023 2 191450 CMVdel-dimericTomato-WASp(CRIB) FRANCE 01/25/2023 3 36205 pmTurquoise2-Golgi ISRAEL 01/25/2023 4 176102 iSH-iRFP670-sspB GERMANY 01/20/2023 5 176113 SspB-HaloTag-DHPH-ITSN1 GERMANY 01/20/2023 6 176114 SspB-HaloTag-DHPH-TIAM1 GERMANY 01/20/2023 There are several columns of data (note that the original data contains a column with names of the ‘Requesting PI’ which I have removed for privacy reasons). The Material, Requesting.Country and Date.ordered are of type factor. We can learn more about these factors with the function str() str(df_addgene) &#39;data.frame&#39;: 3740 obs. of 4 variables: $ ID : int 176106 191450 36205 176102 176113 176114 176116 176125 176130 191453 ... $ Material : Factor w/ 107 levels &quot;3xmNeonGreen-3xrGBD&quot;,..: 36 4 73 26 98 99 100 29 62 43 ... $ Requesting.Country: Factor w/ 54 levels &quot;ARGENTINA&quot;,&quot;AUSTRALIA&quot;,..: 54 16 24 17 17 17 17 17 17 17 ... $ Date.Ordered : Factor w/ 1655 levels &quot;01/01/2013&quot;,&quot;01/02/2013&quot;,..: 114 114 114 90 90 90 90 90 90 90 ... There are 3740 rows, which reflect the number of requests. There are 107 different values for Material and 54 unique countries. Now, the ID is a unique identifier of the material as described in the column Material. df_addgene &lt;- df_addgene %&gt;% mutate(ID=as.factor(ID)) head(df_addgene) ID Material Requesting.Country Date.Ordered 1 176106 mNeonGreen-Paxillin UNITED STATES 01/25/2023 2 191450 CMVdel-dimericTomato-WASp(CRIB) FRANCE 01/25/2023 3 36205 pmTurquoise2-Golgi ISRAEL 01/25/2023 4 176102 iSH-iRFP670-sspB GERMANY 01/20/2023 5 176113 SspB-HaloTag-DHPH-ITSN1 GERMANY 01/20/2023 6 176114 SspB-HaloTag-DHPH-TIAM1 GERMANY 01/20/2023 Let’s look at the number of requests for each plasmid: ggplot(df_addgene, aes(x=Material)) + geom_bar() Clearly, there are several popular plasmids and a large majority that has been requested only a few times. We generate a summary that shows has the count for each plasmid: df_count &lt;- df_addgene %&gt;% group_by(Material) %&gt;% tally(sort = TRUE) head(df_count) # A tibble: 6 × 2 Material n &lt;fct&gt; &lt;int&gt; 1 pmTurquoise2-Mito 352 2 pmTurquoise2-Golgi 340 3 pPalmitoyl-mTurquoise2 340 4 pLifeAct-mTurquoise2 330 5 pmTurquoise2-ER 268 6 pmTurquoise2-Tubulin 207 A column with the count data is generated. This can be used to sort the data: df_count &lt;- df_count %&gt;% mutate(Material = fct_reorder(Material, n)) head(df_count) # A tibble: 6 × 2 Material n &lt;fct&gt; &lt;int&gt; 1 pmTurquoise2-Mito 352 2 pmTurquoise2-Golgi 340 3 pPalmitoyl-mTurquoise2 340 4 pLifeAct-mTurquoise2 330 5 pmTurquoise2-ER 268 6 pmTurquoise2-Tubulin 207 The resulting dataframe shows the sorted count n for each Material. We can use these values in combination with geom_col() to plot these data. ggplot(df_count, aes(x=Material, y=n)) + geom_col() Ok, this looks better, but it’s probably more informative to look at a subset. Let’s say anything that has more than 10 requests. We also rotate the plot, which makes the plasmid names easier to read: ggplot(df_count %&gt;% filter(n&gt;10), aes(x=Material, y=n)) + geom_col() + coord_flip() Addgene uses different colors of flames to indicate the popularity of the plasmids. For more than 20 request a red flame, more than 50 requests is a green flame and more than 100 requests is a blue flame. We can use these categories and colors also in the bar chart, but we first need to add this to the dataframe: df_count &lt;- df_count %&gt;% mutate(flame = case_when(n&lt;20 ~ &quot;grey80&quot;, (n&gt;=20 &amp; n&lt;50) ~ &quot;#D96C6D&quot;, #using a &#39;softer red&#39; (n&gt;=50 &amp; n &lt;100) ~ &quot;#E19E37&quot;, n&gt;=100 ~ &quot;#50B0E8&quot;) ) Let’s replot these data: ggplot(df_count %&gt;% filter(n&gt;15), aes(x=Material, y=n, fill=flame)) + geom_col() + coord_flip() The colors do not match with the data in the flame column. To use the actual colors from this column, we need the function scale_fill_identity(): p &lt;- ggplot(df_count %&gt;% filter(n&gt;15), aes(x=Material, y=n, fill=flame)) + geom_col() + coord_flip() + scale_fill_identity() p Let’s tweak the layout: p &lt;- p + theme_bw(base_size = 12) + labs(x=&quot;&quot;, y=&quot;Number of requests&quot;, title=&quot;Requests for our plasmids at Addgene.org&quot;, tag = &quot;Protocol 22&quot;, caption=&quot;@joachimgoedhart | Data from addgene&quot;) + theme(legend.position = &quot;none&quot;) + scale_y_continuous(expand = c(0,0)) + theme(plot.caption = element_text(color=&#39;grey80&#39;, hjust=1), panel.border = element_blank(), panel.grid.minor = element_blank(), panel.grid.major.y = element_blank(), axis.ticks.x = element_blank(), axis.text.y = element_text(size=8)) p Note that the font size for labels on the vertical axis is rather small, to avoid overlap between the labels. The text size can only be invreased when the number of labels is reduced (or when the plot area is increased). To save the plot: png(file=paste0(&quot;Protocol_22.png&quot;), width = 4000, height = 3000, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 Note that this plot is also used on the addgene dashboard that is available here: https://amsterdamstudygroup.shinyapps.io/Addgene_dashboard/ You can upload your own addgene data, which will a plot similar to the one that is described here. 4.23 Protocol 23 - Plotting multiple conditions side-by-side In this protocol, we plot data that is gathered on ‘cellular brightness’ as described in this blog post. In this specific set, different proteins are measured before (pre) and after (post) saturating the cells with calcium. The dataset has brightness values per cell and there are quite some data-points that are gathered from two independent experiments. This data was first published here and the code below can be used to replicate figure 4 of the preprint. First the necessary packages are activated: library(tidyverse) library(ggbeeswarm) Next, the data is loaded as CSV into a dataframe: df &lt;- read.csv(&quot;data/ratio_cells_selection_for_JG_2.csv&quot;) head(df) LMC construct glass state cell GFP RFP norm_ratio day 1 4455 Tq-Ca-FLITS_T203Y-AS g1 post 1 2560.655 3397.798 0.3459451 1 2 4455 Tq-Ca-FLITS_T203Y-AS g1 post 2 5524.729 7296.616 0.3476011 1 3 4455 Tq-Ca-FLITS_T203Y-AS g1 post 3 1791.515 2255.126 0.3650223 1 4 4455 Tq-Ca-FLITS_T203Y-AS g1 post 5 2482.131 3540.182 0.3213999 1 5 4455 Tq-Ca-FLITS_T203Y-AS g1 post 6 2384.282 3364.853 0.3248852 1 6 4455 Tq-Ca-FLITS_T203Y-AS g1 post 7 1297.668 1595.668 0.3738251 1 There are more data than I want to show, so I filter the data based on the name in the column construct, and this cleaned dataset is saved: df %&gt;% filter(construct %in% c(&quot;EGFP&quot;, &quot;G-Ca-FLITS&quot;, &quot;mTq2_T203Y&quot;,&quot;GCaMP6s&quot;, &quot;jGCaMP7c&quot;, &quot;GCaMP3&quot;, &quot;G-GECO1.1&quot;)) %&gt;% write.csv(&quot;data/ratio_cells_FLITS_set.csv&quot;, row.names = F) We can load the filtered data, which is a standard CSV file: df &lt;- read.csv(&quot;data/ratio_cells_FLITS_set.csv&quot;) head(df) LMC construct glass state cell GFP RFP norm_ratio day 1 4457 G-Ca-FLITS g1 post 1 1278.1177 2162.810 0.2698795 1 2 4457 G-Ca-FLITS g1 post 3 1348.1307 2248.606 0.2738954 1 3 4457 G-Ca-FLITS g1 post 4 7603.0459 13978.270 0.2478862 1 4 4457 G-Ca-FLITS g1 post 5 2602.3306 4542.771 0.2614151 1 5 4457 G-Ca-FLITS g1 post 6 6293.4057 10836.107 0.2651233 1 6 4457 G-Ca-FLITS g1 post 8 898.5548 1376.185 0.2988616 1 The column glass identifies the replicate, so we change the name to better reflect that: df &lt;- df %&gt;% rename(replicate=glass) Let’s also clean up some of the qualitative variables: df &lt;- df %&gt;% mutate(construct = case_when(construct == &quot;mTq2_T203Y&quot; ~ &quot;mTq2-T203Y&quot;, TRUE ~ construct) ) df &lt;- df %&gt;% mutate(across(&#39;replicate&#39;, str_replace, &#39;g&#39;, &#39;&#39;)) head(df) LMC construct replicate state cell GFP RFP norm_ratio day 1 4457 G-Ca-FLITS 1 post 1 1278.1177 2162.810 0.2698795 1 2 4457 G-Ca-FLITS 1 post 3 1348.1307 2248.606 0.2738954 1 3 4457 G-Ca-FLITS 1 post 4 7603.0459 13978.270 0.2478862 1 4 4457 G-Ca-FLITS 1 post 5 2602.3306 4542.771 0.2614151 1 5 4457 G-Ca-FLITS 1 post 6 6293.4057 10836.107 0.2651233 1 6 4457 G-Ca-FLITS 1 post 8 898.5548 1376.185 0.2988616 1 The data is in tidy format. The quantitative variable that we want to plot is the norm_ratio per construct and state. One way to achieve this by specifying different fill colors for the state and plotting the distributions with geom_violin(): ggplot(df, aes(x=construct, y=norm_ratio)) + geom_violin(aes(fill=state)) Another way is to do this with facet_wrap(): ggplot(df, aes(x=state, y=norm_ratio)) + geom_violin() + facet_wrap(~construct) Here, I think the second option is much nicer, as it shows the different constructs as a label on top of the figure. Let’s work on this plot to improve it. It makes more sense that the ‘pre’ condition is shown first: df &lt;- df %&gt;% mutate(state = fct_relevel(state, c(&quot;pre&quot;, &quot;post&quot;))) And, in the same way, I will group the true GFP-based probes (EGFP,GCaMP3,GCaMP6s,jGCaMP7c) and the Turquoise-based probes (G-Ca-FLITS,mTq2_T203Y): df &lt;- df %&gt;% mutate(construct = fct_relevel(construct, c(&quot;EGFP&quot;,&quot;G-GECO1.1&quot;, &quot;GCaMP3&quot;,&quot;GCaMP6s&quot;,&quot;jGCaMP7c&quot;,&quot;G-Ca-FLITS&quot;,&quot;mTq2-T203Y&quot;))) We can also show the individual datapoints according to their distribution by using geom_quasirandom() from the {ggbeeswarm} package: ggplot(df, aes(x=state, y=norm_ratio, group = replicate)) + geom_quasirandom(alpha=0.3, size=2) + facet_wrap(~construct) Let’s compress the plot in the y-direction, showing all plots in a single row (and add some color): We also ‘compress’ the plot, taking less space: p &lt;- ggplot(df, aes(x=state, y=norm_ratio, group = replicate, fill=construct, color=construct)) + geom_quasirandom(alpha=0.3, size=2) + facet_wrap(~construct, nrow = 1) p &lt;- p + theme_bw(base_size = 16) + theme(aspect.ratio = 4) p To display the median value of the replicates we add a larger dot showing the median and add a line to connect these paired data: p &lt;- p + stat_summary(fun = median, geom=&quot;line&quot;, color=&quot;black&quot;, linewidth=1) + stat_summary(fun = median, geom=&#39;point&#39;, size=6, shape=21, color=&quot;black&quot;, alpha=0.5) p Let’s edit the plot to adjust the theme, labels and layout: p &lt;- p + scale_color_manual(values = c(&#39;darkgreen&#39;,&#39;darkgreen&#39;,&#39;darkgreen&#39;,&#39;darkgreen&#39;,&#39;darkgreen&#39;,&#39;darkseagreen4&#39;,&#39;darkseagreen4&#39;)) p &lt;- p + scale_fill_manual(values = c(&#39;darkgreen&#39;,&#39;darkgreen&#39;,&#39;darkgreen&#39;,&#39;darkgreen&#39;,&#39;darkgreen&#39;,&#39;darkseagreen4&#39;,&#39;darkseagreen4&#39;)) p &lt;- p + guides(fill = &quot;none&quot;, color = &quot;none&quot; ) p &lt;- p + labs( title = &quot;Cellular brightness of green Calcium biosensors&quot;, subtitle = &quot;pre and post ionomycin addition&quot;, tag = &quot;Protocol 23&quot;, y = &quot;normalized brightness&quot; ) + theme(panel.grid = element_blank()) + scale_y_continuous(expand = c(0, 0), limits = c(0, 1.3)) p To save the plot: png(file=paste0(&quot;Protocol_23.png&quot;), width = 4000, height = 3000, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 4.24 Protocol 24 - Colored slopes plot The purpose of this protocol is to demonstrate how paired data can be colored based on the slope of the connecting line. In this example, a negative slope will be blue and a positive slope red. Let’s load the necessary library first: library(tidyverse) We’ll create some synthetic data with three columns. There is data on the Tissue that was examined, and a measured result Value. We also have a column with information on the Replicate. The latter columns is needed to identifiy the pairs of data. We generate random values between 0 and 1 by using runif() for the ‘Healthy’ condition and repeat this for the ‘Tumor’ and add 0.4 for every sample to obtain and increased value: df &lt;- data.frame(Tissue = c(rep(&quot;Healthy&quot;,50),rep(&quot;Tumor&quot;,50)), Replicate = rep(1:50, 2), Value=c(runif(50), runif(50)+0.4)) head(df) Tissue Replicate Value 1 Healthy 1 0.1293723 2 Healthy 2 0.4781180 3 Healthy 3 0.9240745 4 Healthy 4 0.5987610 5 Healthy 5 0.9761707 6 Healthy 6 0.7317925 These data can be visualized as a slope plot, where the Value for every replicate is connected with a line: ggplot(df, aes(x=Tissue, y=Value)) + geom_point() + geom_line(aes(group=Replicate)) We can see that the average of “Tumor” is higher than the “Healthy” condition, but it is not straighforward to see how many of the slopes show an increase or decrease. To depict the different slopes with different colorsm we will calculate the difference for Value between the “Tumor” and “Healthy” condition per replicate, as this defines the trend (positive or negative): df &lt;- df %&gt;% group_by(Replicate) %&gt;% mutate(difference = (Value[Tissue==&quot;Tumor&quot;] - Value[Tissue==&quot;Healthy&quot;])) head(df) # A tibble: 6 × 4 # Groups: Replicate [6] Tissue Replicate Value difference &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Healthy 1 0.129 0.800 2 Healthy 2 0.478 -0.00284 3 Healthy 3 0.924 -0.246 4 Healthy 4 0.599 0.0139 5 Healthy 5 0.976 -0.291 6 Healthy 6 0.732 0.563 It is rather straighforward to add a color that reflects the slope: ggplot(df, aes(x=Tissue, y=Value)) + geom_line(aes(group=Replicate, color=difference), linewidth=1, alpha=0.6) + geom_point(color = &quot;black&quot;, fill=&quot;grey&quot;, shape=21, size=3, alpha=0.4) + theme_light(base_size = 16) This works, but it’s probably sufficient, and much clearer, to just distinguish between an increase and decrease. We can use a trick to color according to the slope by using color=difference&gt;0“: p &lt;- ggplot(df, aes(x=Tissue, y=Value)) + geom_line(aes(group=Replicate, color=difference&lt;0), linewidth=1, alpha=0.8) + geom_point(color = &quot;black&quot;, fill=&quot;grey&quot;, shape=21, size=3, alpha=0.8) p &lt;- p + theme_light(base_size = 16) p To define the colors manually: p + scale_color_manual(values = c(&quot;red&quot;, &quot;darkblue&quot;)) And we can also use the function scale_color_manual() to further style the legend: We can also use p &lt;- p + scale_color_manual(values = c(&quot;red&quot;, &quot;darkblue&quot;), labels = c(&quot;Positive&quot;, &quot;Negative&quot;), name = &quot;Slope Direction&quot;) p We can add a title to complete this protocol: p &lt;- p + labs( title = &quot;A colored slope plot&quot;, tag = &quot;Protocol 24&quot; ) p To save the plot as a png file: png(file=paste0(&quot;Protocol_24.png&quot;), width = 4000, height = 3000, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 4.25 Protocol 25 - Colorblind friendly colors on a dark theme In this protocol, we recreate a plot that can easily be done in PlotTwist. What I want to highlight is (i) how to use colorblind friendly colors for the lines and (i) how to apply a dark theme. I think that the colorblind friendly palette designed by Okabe and Ito works very well on a dark background, showing lines in clear, vivid colors. library(tidyverse) We load the data from a tidy dataframe: df_tidy &lt;- read.csv(&quot;data/PlotTwist_tidy_6.csv&quot;) head(df_tidy) Time id unique_id Sample Value 1 0 1 cell_01 cell_01 0.9935818 2 2 1 cell_01 cell_01 0.9972606 3 4 1 cell_01 cell_01 1.0109350 4 6 1 cell_01 cell_01 0.9959134 5 8 1 cell_01 cell_01 0.9970548 6 10 1 cell_01 cell_01 1.0038560 We can have a first glance at the data: ggplot(df_tidy, aes(x=Time, y=Value)) + geom_line() + facet_wrap(~Sample) 4.25.1 Add a colorblind safe palette The different lines are different categories and therefore we need a qualitative colorscale. This can be defined with a vector of hexadecimal color codes. For the Okabe Ito palette we define: Okabe_Ito &lt;- c(&quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#000000&quot;) To use these colors for the line we use the function scale_color_manual(). We also increase the width of the lines to improve the visibility of the colors. ggplot(df_tidy, aes(x=Time, y=Value, color=Sample)) + geom_line(linewidth=1.5) + facet_wrap(~Sample) + scale_color_manual(values=Okabe_Ito) This looks good, so we define this plot as p and make the layout less cluttered, by removing the legend and the strips on top of each plot: p &lt;- ggplot(df_tidy, aes(x=Time, y=Value, color=Sample)) + geom_line(linewidth=1.5) + facet_wrap(~Sample) + scale_color_manual(values=Okabe_Ito) + theme(legend.position = &quot;none&quot;, strip.text = element_blank()) p 4.25.2 Dark theme Let’s add a dark theme. Themes can be defined as modifications of existing themes, e.g. the default theme theme_grey(). Here, we will use a theme that is also available in PlotTwist and that can be loaded from the Github repo by using source(): source(&quot;https://raw.githubusercontent.com/JoachimGoedhart/PlotTwist/master/themes.R&quot;) The theme that is defined in this file is theme_light_dark_bg() and we can apply it to the plot like this: p + theme_light_dark_bg() Now we have a nice dark background, but the legend and strips on top of each plot are back. That’s because these modifications are overwritten by applying a new theme. So we should first add the new theme definition and then the modifications: p &lt;- ggplot(df_tidy, aes(x=Time, y=Value, color=Sample)) + geom_line(linewidth=1.5) + facet_wrap(~Sample) + scale_color_manual(values=Okabe_Ito) + theme_light_dark_bg() + theme(legend.position = &quot;none&quot;, strip.text = element_blank()) p That looks great. Now we can further modify the layout by modifying labels and tweaking the theme: p &lt;- p + labs(x = &quot;Time [s]&quot;, y = &quot;Normalized Ratio&quot;) + labs(title = &quot;G-protein activation and deactivation&quot;, tag = &quot;Protocol 25&quot;) + theme(panel.grid = element_blank()) p I do not like the boxes around each plot, and these can be removed by modifying the theme: p + theme(panel.border = element_blank()) This looks better, and if we still want to keep the axis on one side we can re-add those: p &lt;- p + theme(panel.border = element_blank()) p &lt;- p + theme(axis.line.x = element_line(colour = &quot;grey80&quot;), axis.line.y = element_line(colour = &quot;grey80&quot;)) p To prevent the collision of labels (i.e. “0” and “250”) on the x-axis, we can remove the space that is added by default: p + coord_cartesian(expand = F) Or by defining the ticks: p &lt;- p + scale_x_continuous(breaks=c(0,100,200)) p Ok, let’s (s)tick with this. To save the plot: png(file=paste0(&quot;Protocol_25.png&quot;), width = 4000, height = 3000, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 4.26 Protocol 26 - Donut charts The pie chart is used to show proportions and it is one of the ugliest charts that is around. Somehow, adding a hole to the pie chart is an incredible, aesthetic upgrade of this type of data visualization. This alternative is known as a donut chart for obvious reasons. Donut charts also work well on dashboards. So let’s see how we can generate a donut chart. Here, we start with a very simple donut chart that displays the proportion of cells in the S-phase as determined from staining of EdU incorporation. I use data harvested from a practical course that I teach Goedhart, DOI: 10.1371/journal.pcbi.1011836. library(tidyverse) Let’s load the data on the percentage of S-phase cells that I gathered over 5 years: df_S &lt;- read.csv(&quot;data/results_S-phase-5yrs.csv&quot;) %&gt;% filter(Group != &quot;D&quot;) head(df_S) X day month year Time Group Analysis S_phase 1 1 4 3 2021 17:35:44 B manual 37.29 2 2 4 3 2021 17:35:44 B automated 37.85 3 3 4 3 2021 23:03:48 A manual 44.00 4 4 4 3 2021 23:03:48 A automated 42.00 5 5 5 3 2021 16:11:07 B manual 58.80 6 6 5 3 2021 16:11:07 B automated 65.80 This data is cleaned and it has information on the year (ranging from 2021-2025), the group (A/B/C/D), whether the data was analysed by hand or automatically and the percentage of S-phase cells that was detected. I’d like to make a donut chart of the manual and automated analysis that was done over the years, so we’ll make a summary: df_avg &lt;- df_S %&gt;% group_by(Analysis) %&gt;% summarise(S = mean(S_phase), n=n()) df_avg # A tibble: 2 × 3 Analysis S n &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; 1 automated 40.9 130 2 manual 39.5 132 To improve the labels later on, I will change the content of the column ‘Analysis’ to more understandable text: df_avg &lt;- df_avg %&gt;% mutate( Analysis = paste(Analysis, &quot;analysis&quot;) ) Great, so now we have summarised data for the donut chart. However, to contruct the chart we also need the non-S-phase cells. In the donut chart, this will be the rest of the circle. To get this, we add a second column and subtract the total, 100&amp;, from the percentage of S-phase: df_avg &lt;- df_avg %&gt;% mutate(&quot;non-S&quot; = 100-S) Finally, we need to tidy up this dataframe: df_tidy &lt;- df_avg %&gt;% pivot_longer(cols = c(S, `non-S`), names_to = &quot;category&quot;, values_to = &quot;percentage&quot;) I will add an ‘index’ column that later can be used to specify colors: df_tidy &lt;- df_tidy %&gt;% mutate(ID = as.factor(row_number())) A standard method to display the proportion is to use a stacked bar graph with geom_bar(stat = \"identity\") to use the actual values in the dataframe: ggplot(df_tidy, aes(x = 1, y = percentage, fill = category)) + geom_bar(stat = &quot;identity&quot;) + facet_wrap(~Analysis) This can be turned into a pie chart by changing the coordinates to a polar system: ggplot(df_tidy, aes(x = 1, y = percentage, fill = category)) + geom_bar(stat = &quot;identity&quot;) + facet_wrap(~Analysis) + coord_polar(theta = &quot;y&quot;) Here, the y values fill up the 360 degrees of the pie chart, and the x value defines the width of the curved band. This can be turned into a donut chart by changing the x limits. We also move the labels of the facets to the bottom by defining strip.position = \"bottom\", as this looks better in the final figure: ggplot(df_tidy, aes(x = 1, y = percentage, fill = category)) + geom_bar(stat = &quot;identity&quot;) + facet_wrap(~Analysis, strip.position = &quot;bottom&quot;) + coord_polar(theta = &quot;y&quot;) + xlim(-0.5,1.5) This looks like a donut. We will define the object as ‘p’ and adjust the layout. In addition, we will use the ‘ID’ to define the color of each of the segments: p &lt;- ggplot(df_tidy, aes(x = 1, y = percentage, fill = ID)) + geom_bar(stat = &quot;identity&quot;) + facet_wrap(~Analysis, strip.position = &quot;bottom&quot;) + coord_polar(theta = &quot;y&quot;, direction = -1) + xlim(-0.5,1.5) p That looks odd. I want the proportions of S-phase in two distinct (colorblind safe) colors and I want the remainder (non-S) in grey: p &lt;- p + scale_fill_manual(values = c(&quot;darkorange2&quot;, &quot;grey80&quot;, &quot;dodgerblue3&quot;, &quot;grey80&quot;)) We do not want any of the labels for the axes, the grid and background, so we use theme_void(). In addition, we will highlight the proportion of S-phase cells in two different colors and remove the legend: p &lt;- p + theme_void() + theme(legend.position = &quot;none&quot;) p The hole in the donut is often used to display a number, in this case we can add the percentage, to state the percentage of cells in the S-phase. To this end, we can filter the dataframe to get the right labels: df_label &lt;- df_tidy %&gt;% filter(category == &quot;S&quot;) %&gt;% mutate() Then we use this to add the label: p &lt;- p + geom_text(data = df_label, aes(label = paste0(round(percentage,0),&quot;%&quot;)), x= -.5, y = 0, size=12) p And we add a title (note that I use a ‘subtitle’ here to leave a bit of space between the ‘tag’ and the title): p &lt;- p + labs( title = &quot;The percentage of HeLa cells in S-phase...&quot;, subtitle = &quot;...determined by two methods&quot;, caption = &quot;@joachimgoedhart | data submitted by students&quot;, tag = &quot;Protocol 26&quot; ) + theme(plot.caption = element_text(color = &quot;grey80&quot;, hjust = 1)) + theme(plot.title = element_text(hjust = 0.5, size= 18)) + theme(plot.subtitle = element_text(hjust = 0.5, size= 18) ) p p &lt;- p + theme(strip.text = element_text(size = 16, face = &quot;bold&quot;), strip.placement = &quot;inside&quot;) p Saving the plot: png(file=paste0(&quot;Protocol_26.png&quot;), width = 4000, height = 3000, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 4.27 Protocol 27 - Raincloud plot library(tidyverse) The donut chart that we’ve seen in protocol 26 summarizes a rich dataset in a single value. While this simplification can be a powerful way to communicate results, we may be interested in the underlying data. A comprehensive way to show all data and its distribution is the raincloud plot. This is a recent innovation that was reported by Micha Allen and colleagues. We will use the same dataset as for protocol 26 to generate a raincloud plot and this will nicely contrast with the donut chart. Let’s again load the data on the percentage of HeLa cells in the S-phase: df_S &lt;- read.csv(&quot;data/results_S-phase-5yrs.csv&quot;) %&gt;% filter(Group != &quot;D&quot;) head(df_S) X day month year Time Group Analysis S_phase 1 1 4 3 2021 17:35:44 B manual 37.29 2 2 4 3 2021 17:35:44 B automated 37.85 3 3 4 3 2021 23:03:48 A manual 44.00 4 4 4 3 2021 23:03:48 A automated 42.00 5 5 5 3 2021 16:11:07 B manual 58.80 6 6 5 3 2021 16:11:07 B automated 65.80 This data is cleaned and it has information on the year (ranging from 2021-2025), the group (A/B/C/D), whether the data was analysed by hand or automatically and the percentage of S-phase cells that was detected. There is quite some data on the manual versus automated analysis and I want to see the distribution of the data and evaluate whether these approaches give similar results. Let’s first look at the distributions: ggplot(df_S, aes(x=S_phase, fill=Analysis)) + geom_density() + facet_wrap(~Analysis) We do not see the actual data, and we can add these with geom_rug(): ggplot(df_S, aes(x=S_phase, fill=Analysis)) + geom_density() + geom_rug() + facet_wrap(~Analysis) This gives some idea of the data, but in this plot, the emphasis is on the distribution. To better visualize the distribution and the data, one may combine a jittered dotplot with a violinplot. Note that we generate a standard jitter+violin plot, and then we rotate it to make it comparable to the distributions in the plot above: ggplot(df_S, aes(x=Analysis, y=S_phase, fill=Analysis)) + geom_violin() + geom_jitter() + coord_flip() The violinplot shows the distribution, but it does so twice. To show it once, a new geom has been defined, i.e. geom_flat_violin(). This is not (yet) supported by the {ggplot2} package and therefore we need to load it separately: source(&quot;https://raw.githubusercontent.com/JoachimGoedhart/SuperPlotsOfData/refs/heads/master/geom_flat_violin.R&quot;) ------------------------------------------------------------------------------ You have loaded plyr after dplyr - this is likely to cause problems. If you need functions from both plyr and dplyr, please load plyr first, then dplyr: library(plyr); library(dplyr) ------------------------------------------------------------------------------ Attaching package: &#39;plyr&#39; The following objects are masked from &#39;package:dplyr&#39;: arrange, count, desc, failwith, id, mutate, rename, summarise, summarize The following object is masked from &#39;package:purrr&#39;: compact ggplot(df_S, aes(x=Analysis, y=S_phase, fill=Analysis)) + geom_flat_violin() + geom_jitter() +coord_flip() Now, the distributions look like a cloud, but we need to reduce the jitter, and to move the clouds up: ggplot(df_S, aes(x=Analysis, y=S_phase, fill=Analysis, color=Analysis)) + geom_flat_violin(position = position_nudge(x = .15, y = 0), width=0.8) + geom_jitter(width = 0.1) + coord_flip() The original raincloudplots include a boxplot, but that’s a bit too cluttered in my opinion: ggplot(df_S, aes(x=Analysis, y=S_phase, fill=Analysis, color=Analysis)) + geom_flat_violin(position = position_nudge(x = .15, y = 0), width=0.8) + geom_boxplot(position = position_nudge(x = 0.15, y = 0), width = 0.05, color=&quot;black&quot;, outlier.shape = NA, fill=NA) + geom_jitter(width = 0.1) + coord_flip() So let’s define a raincloudplot without the box and do some styling. We will also change the order of the the methods: p &lt;- ggplot(df_S, aes(x=fct_rev(Analysis), y=S_phase, fill=Analysis, color=Analysis)) + geom_flat_violin(position = position_nudge(x = .15, y = 0), width=0.8) + geom_jitter(width = 0.1) + coord_flip() + theme_light(base_size = 16) p Deafault colors are OK-ish, but let’s use the same colors we’ve used for the donut chart in protocol 26: p &lt;- p + scale_fill_manual(values=c(&quot;darkorange2&quot;, &quot;dodgerblue3&quot;)) + scale_color_manual(values=c(&quot;darkorange2&quot;, &quot;dodgerblue3&quot;)) p p &lt;- p + theme(legend.position = &quot;none&quot;) + theme(plot.caption = element_text(color = &quot;grey80&quot;, hjust = 1)) + labs(title = &quot;The percentage of cells in S-phase...&quot;, subtitle = &quot;...determined by two methods&quot;, x=&quot;Analysis&quot;, y=&quot;S-phase [%]&quot;, caption = &quot;@joachimgoedhart | data submitted by students&quot;, tag = &quot;Protocol 27&quot; ) p Saving the plot: png(file=paste0(&quot;Protocol_27.png&quot;), width = 4000, height = 3000, units = &quot;px&quot;, res = 400) p dev.off() quartz_off_screen 2 detach(package:plyr) #needed to get rid of this package which was required for the flat violin In progress This chapter is ‘in progress’ and I plan to add more protocols in the future. "],["questions-and-answers.html", "Chapter 5 Questions and Answers", " Chapter 5 Questions and Answers A list of questions that may arise during the use of R and their answers. When you have questions that remain unanswered, feel free to reach out! What is the difference between a data.frame and a tibble? Both are objects that hold data. The data.frame is the structure supported by basic R and the tibble is an extended variant that is supported by the tidyverse package. Dependig on the function that is used to load data, read.csv() from base R or read_csv() from the tidyverse, the data will be loaded in a data.frame or tibble, respectively. It is possible to convert a classic data.frame to a tibble (which requires the tidyverse pakcage): df &lt;- as_tibble(mtcars) class(df) [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; And to convert a tibble to a data.frame: df &lt;- as.data.frame(df) class(df) [1] &quot;data.frame&quot; What’s up with spaces in variable names and in files names? Spaces are used to separate words (as in this line). When a space would be used between two words, these would be treated as separate entities. Avoiding spaces in filenames has a more technical background. Most filesystems currently accept spaces, but some systems do not. To increase compatibility across systems it is a good habit to use_underscores_instead. What are the rules for naming dataframes or variables? Do not use spaces, stick to characters and numbers. Whenever a variable consists of multiple words, e.g. room temperature there are two options: Add an underscore as separator: room_temperature. Use the ‘camelCase’ notation: roomTemperature. Personally, I prefer an underscore and the abbreviation df for dataframe when multiple dataframes are generated, e.g. df_tidy or df_summary What is the difference between &lt;- and = for assigning variables? R prefers the &lt;- for the assignment of a value to a variable. In RStudio the shortcut is &lt;alt&gt;+&lt;-&gt;. It really is a preference since both can be used: x &lt;- 1 y = 2 x + y [1] 3 What does ‘NA’ mean in a dataframe? When no data is available, this is known as a ‘missing value’. In R this is indicated with NA for ‘Not Available’. Empty cells in a CSV file will be converted to NA when the file is loaded. Other characters that can be present in a file (especially xls files) are: “.” or “NaN” or “#N/A” or “#VALUE!”. To convert these strings to NA use: df &lt;- read.csv(&#39;FPbase_Spectra.csv&#39;, na.strings=c(&quot;.&quot;, &quot;NaN&quot;, &quot;#N/A&quot;, &quot;#VALUE!&quot;)) What is the beste way to type a ‘%&gt;%’ operator? I prefer to literally type it. The shortcut that works in Rstudio is &lt;shift&gt;+&lt;command&gt;+&lt;M&gt; or &lt;shift&gt;+&lt;control&gt;+&lt;M&gt;. Where do I find the example data? The example data that is used in Chapter 2 and 3 is located on this github page: https://github.com/JoachimGoedhart/DataViz-protocols The data that is used for the protocols in Chapter 4 is located in the subdirectory /Protocols. Is there a way to re-use or adjust the protocols? Instead of copy-pasting and running the code line-by-line, you can download the R Markdown file (.Rmd) from the protocols folder in the Github repository. The R Markdown file (together with the data) can be used to replicate the protocol and to modify it. For more info on R Markdown, see https://rmarkdown.rstudio.com Which packages are included in the {tidyverse} package? You can find this out by first loading the package and then run tidyverse_packages() What is the difference between require() and library() for loading a package? Both functions can be used to load a package. The difference is that require() returns FALSE when the package does not exist and can be used to check whether a package was loaded: if (require(&quot;nonexistant&quot;) == FALSE) (&quot;This packkage doesn&#39;t exist&quot;) Loading required package: nonexistant [1] &quot;This packkage doesn&#39;t exist&quot; Is it possible to have interactive file selection? In some cases it can be convenient to select a file by point-and-click, although this is not strictly reproducible. This example code shows how this can be achieved by using the function file.choose() inside a function for reading a csv file: df &lt;- read.csv(file.choose(), header = TRUE) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
